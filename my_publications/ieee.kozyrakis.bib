@ARTICLE{9380428,
author={Li, Qian and Li, Bin and Mercati, Pietro and Illikkal, Ramesh and Tai, Charlie and Kishinevsky, Michael and Kozyrakis, Christos},
journal={IEEE Computer Architecture Letters}, title={RAMBO: Resource Allocation for Microservices Using Bayesian Optimization},
year={2021},
volume={20},
number={1},
pages={46-49},
abstract={Microservices are becoming the defining paradigm of cloud applications, which raises urgent challenges for efficient datacenter management. Guaranteeing end-to-end Service Level Agreement (SLA) while optimizing resource allocation is critical to both cloud service providers and users. However, one application may contain hundreds of microservices, which constitute an enormous search space that is unfeasible to explore exhaustively. Thus, we propose RAMBO, an SLA-aware framework for microservices that leverages multi-objective Bayesian Optimization (BO) to allocate resources and meet performance/cost goals. Experiments conducted on a real microservice workload demonstrate that RAMBO can correctly characterize each microservice and efficiently discover Pareto-optimal solutions. We envision that the proposed methodology and results will benefit future resource planning, cluster orchestration, and job scheduling.},
keywords={Optimization;Resource management;Service level agreements;Servers;Distributed computing;Bayes methods;Distributed computing;Distributed applications;emerging technologies},
doi={10.1109/LCA.2021.3066142},
ISSN={1556-6064},
month={Jan},}

@ARTICLE{9069187,
author={Nagendra, Nayana Prasad and Ayers, Grant and August, David I. and Cho, Hyoun Kyu and Kanev, Svilen and Kozyrakis, Christos and Krishnamurthy, Trivikram and Litz, Heiner and Moseley, Tipp and Ranganathan, Parthasarathy},
journal={IEEE Micro}, title={AsmDB: Understanding and Mitigating Front-End Stalls in Warehouse-Scale Computers},
year={2020},
volume={40},
number={3},
pages={56-63},
abstract={It is well known that the datacenters hosting today's cloud services waste a significant number of cycles on front-end stalls. However, prior work has provided little insights about the source of these front-end stalls and how to address them. This work analyzes the cause of instruction cache misses at a fleet-wide scale and proposes a new compiler-driven software code prefetching strategy to reduce instruction caches misses by 90%.},
keywords={Prefetching;Optimization;Servers;Hardware;Databases;Complexity theory},
doi={10.1109/MM.2020.2986212},
ISSN={1937-4143},
month={May},}


@INPROCEEDINGS{8980365,
author={Ayers, Grant and Nagendra, Nayana Prasad and August, David I. and Cho, Hyoun Kyu and Kanev, Svilen and Kozyrakis, Christos and Krishnamurthy, Trivikram and Litz, Heiner and Moseley, Tipp and Ranganathan, Parthasarathy},
booktitle={2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)}, title={AsmDB: Understanding and Mitigating Front-End Stalls in Warehouse-Scale Computers},
year={2019},
volume={},
number={},
pages={462-473},
abstract={The large instruction working sets of private and public cloud workloads lead to frequent instruction cache misses and costs in the millions of dollars. While prior work has identified the growing importance of this problem, to date, there has been little analysis of where the misses come from, and what the opportunities are to improve them. To address this challenge, this paper makes three contributions. First, we present the design and deployment of a new, always-on, fleet-wide monitoring system, AsmDB, that tracks front-end bottlenecks. AsmDB uses hardware support to collect bursty execution traces, fleet-wide temporal and spatial sampling, and sophisticated offline post-processing to construct full-program dynamic control-flow graphs. Second, based on a longitudinal analysis of AsmDB data from real-world online services, we present two detailed insights on the sources of front-end stalls: (1) cold code that is brought in along with hot code leads to significant cache fragmentation and a corresponding large number of instruction cache misses; (2) distant branches and calls that are not amenable to traditional cache locality or next-line prefetching strategies account for a large fraction of cache misses. Third, we prototype two optimizations that target these insights. For misses caused by fragmentation, we focus on memcmp, one of the hottest functions contributing to cache misses, and show how fine-grained layout optimizations lead to significant benefits. For misses at the targets of distant jumps, we propose new hardware support for software code prefetching and prototype a new feedback-directed compiler optimization that combines static program flow analysis with dynamic miss profiles to demonstrate significant benefits for several large warehouse-scale workloads. Improving upon prior work, our proposal avoids invasive hardware modifications by prefetching via software in an efficient and scalable way. Simulation results show that such an approach can eliminate up to 96\% of instruction cache misses with negligible overheads.},
keywords={},
doi={},
ISSN={2575-713X},
month={June},}

@ARTICLE{8357989,
author={Prabhakar, Raghu and Zhang, Yaqi and Koeplinger, David and Feldman, Matt and Zhao, Tian and Hadjis, Stefan and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
journal={IEEE Micro}, title={Plasticine: A Reconfigurable Accelerator for Parallel Patterns},
year={2018},
volume={38},
number={3},
pages={20-31},
abstract={Plasticine is a new spatially reconfigurable architecture designed to efficiently execute applications composed of high-level parallel patterns. With an area footprint of 113 mm2 in a 28-nm process and a 1-GHz clock, Plasticine has a peak floating-point performance of 12.3 single-precision Tflops and a total on-chip memory capacity of 16 MB, consuming a maximum power of 49 W. Plasticine provides an improvement of up to 76.9X in performance-per-watt over a conventional FPGA over a wide range of dense and sparse applications.},
keywords={Phasor measurement units;Random access memory;Computer architecture;Reconfigurable architecture;System-on-chip;Field programmable gate arrays;hardware accelerators;dataflow architectures;FPGA;coarse-grained reconfigurable architectures;parallel patterns;reconfigurable architectures;hardware},
doi={10.1109/MM.2018.032271058},
ISSN={1937-4143},
month={May},}


@INPROCEEDINGS{8192487,
author={Prabhakar, Raghu and Zhang, Yaqi and Koeplinger, David and Feldman, Matt and Zhao, Tian and Hadjis, Stefan and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)}, title={Plasticine: A reconfigurable architecture for parallel patterns},
year={2017},
volume={},
number={},
pages={389-402},
abstract={Reconfigurable architectures have gained popularity in recent years as they allow the design of energy-efficient accelerators. Fine-grain fabrics (e.g. FPGAs) have traditionally suffered from performance and power inefficiencies due to bit-level reconfigurable abstractions. Both fine-grain and coarse-grain architectures (e.g. CGRAs) traditionally require low level programming and suffer from long compilation times. We address both challenges with Plasticine, a new spatially reconfigurable architecture designed to efficiently execute applications composed of parallel patterns. Parallel patterns have emerged from recent research on parallel programming as powerful, high-level abstractions that can elegantly capture data locality, memory access patterns, and parallelism across a wide range of dense and sparse applications. We motivate Plasticine by first observing key application characteristics captured by parallel patterns that are amenable to hardware acceleration, such as hierarchical parallelism, data locality, memory access patterns, and control flow. Based on these observations, we architect Plasticine as a collection of Pattern Compute Units and Pattern Memory Units. Pattern Compute Units are multi-stage pipelines of reconfigurable SIMD functional units that can efficiently execute nested patterns. Data locality is exploited in Pattern Memory Units using banked scratchpad memories and configurable address decoders. Multiple on-chip address generators and scatter-gather engines make efficient use of DRAM bandwidth by supporting a large number of outstanding memory requests, memory coalescing, and burst mode for dense accesses. Plasticine has an area footprint of 113 mm2 in a 28 nm process, and consumes a maximum power of 49 W at a 1 GHz clock. Using a cycle-accurate simulator, we demonstrate that Plasticine provides an improvement of up to 76.9× in performance-per-Watt over a conventional FPGA over a wide range of dense and sparse applications.},
keywords={Computer architecture;Field programmable gate arrays;Hardware;Programming;Indexes;System-on-chip;Fabrics;parallel patterns;reconfigurable architectures;hardware accelerators;CGRAs},
doi={10.1145/3079856.3080256},
ISSN={},
month={June},}
@INPROCEEDINGS{8101297,
author={Hwang, William and Sabry Aly, Mohamed M. and Malviya, Yash H. and Gao, Mingyu and Wu, Tony F. and Kozyrakis, Christos and Wong, H.-S. Philip and Mitra, Subhasish},
booktitle={2017 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)}, title={Special session paper 3D nanosystems enable embedded abundant-data computing},
year={2017},
volume={},
number={},
pages={1-2},
abstract={The world's appetite for abundant-data computing, where a massive amount of structured and unstructured data is analyzed, has increased dramatically. The computational demands of these applications, such as deep learning, far exceed the capabilities of today's systems, especially for energy-constrained embedded systems (e.g., mobile systems with limited battery capacity). These demands are unlikely to be met by isolated improvements in transistor or memory technologies, or integrated circuit (IC) architectures alone. Transformative nanosystems, which leverage the unique properties of emerging nanotechnologies to create new IC architectures, are required to deliver unprecedented functionality, performance, and energy efficiency. We show that the projected energy efficiency benefits of domain-specific 3D nanosystems is in the range of 1,000x (quantified using the product of system-level energy consumption and execution time) over today's domain-specific 2D systems with off-chip DRAM. Such a drastic improvement is key to enabling new capabilities such as deep learning in embedded systems.},
keywords={Three-dimensional displays;Energy efficiency;Two dimensional displays;Random access memory;Computer architecture;CNTFETs;Machine learning},
doi={10.1145/3125502.3125531},
ISSN={},
month={Oct},}
@ARTICLE{7948664,
author={Gao, Mingyu and Delimitrou, Christina and Niu, Dimin and Malladi, Krishna T. and Zheng, Hongzhong and Brennan, Bob and Kozyrakis, Christos},
journal={IEEE Micro}, title={DRAF: A Low-Power DRAM-Based Reconfigurable Acceleration Fabric},
year={2017},
volume={37},
number={3},
pages={70-78},
abstract={The DRAM-Based Reconfigurable Acceleration Fabric (DRAF) uses commodity DRAM technology to implement a bit-level, reconfigurable fabric that improves area density by 10 times and power consumption by more than 3 times over conventional field-programmable gate arrays. Latency overlapping and multicontext support allow DRAF to meet the performance and density requirements of demanding applications in data center and mobile environments.},
keywords={Random access memory;Field programmable gate arrays;Context modeling;DRAM chips;Acceleration;Digital signal processing;Arrays;DRAM;reconfigurable logic;field-programmable gate array;FPGA;low power},
doi={10.1109/MM.2017.50},
ISSN={1937-4143},
month={},}
@INPROCEEDINGS{7551387,
author={Koeplinger, David and Prabhakar, Raghu and Zhang, Yaqi and Delimitrou, Christina and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)}, title={Automatic Generation of Efficient Accelerators for Reconfigurable Hardware},
year={2016},
volume={},
number={},
pages={115-127},
abstract={Acceleration in the form of customized datapaths offer large performance and energy improvements over general purpose processors. Reconfigurable fabrics such as FPGAs are gaining popularity for use in implementing application-specific accelerators, thereby increasing the importance of having good high-level FPGA design tools. However, current tools for targeting FPGAs offer inadequate support for high-level programming, resource estimation, and rapid and automatic design space exploration. We describe a design framework that addresses these challenges. We introduce a new representation of hardware using parameterized templates that captures locality and parallelism information at multiple levels of nesting. This representation is designed to be automatically generated from high-level languages based on parallel patterns. We describe a hybrid area estimation technique which uses template-level models and design-level artificial neural networks to account for effects from hardware place-and-route tools, including routing overheads, register and block RAM duplication, and LUT packing. Our runtime estimation accounts for off-chip memory accesses. We use our estimation capabilities to rapidly explore a large space of designs across tile sizes, parallelization factors, and optional coarse-grained pipelining, all at multiple loop levels. We show that estimates average 4.8% error for logic resources, 6.1% error for runtimes, and are 279 to 6533 times faster than a commercial high-level synthesis tool. We compare the best-performing designs to optimized CPU code running on a server-grade 6 core processor and show speedups of up to 16.7×.},
keywords={Field programmable gate arrays;Hardware;Pipeline processing;Space exploration;Estimation;Design tools;hardware generation;design space exploration;FPGAs;parallel patterns;hardware definition language;reconfigurable hardware;application-specific accelerators},
doi={10.1109/ISCA.2016.20},
ISSN={1063-6897},
month={June},}
@INPROCEEDINGS{7551418,
author={Gao, Mingyu and Delimitrou, Christina and Niu, Dimin and Malladi, Krishna T. and Zheng, Hongzhong and Brennan, Bob and Kozyrakis, Christos},
booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)}, title={DRAF: A Low-Power DRAM-Based Reconfigurable Acceleration Fabric},
year={2016},
volume={},
number={},
pages={506-518},
abstract={FPGAs are a popular target for application-specific accelerators because they lead to a good balance between flexibility and energy efficiency. However, FPGA lookup tables introduce significant area and power overheads, making it difficult to use FPGA devices in environments with tight cost and power constraints. This is the case for datacenter servers, where a modestly-sized FPGA cannot accommodate the large number of diverse accelerators that datacenter applications need. This paper introduces DRAF, an architecture for bit-level reconfigurable logic that uses DRAM subarrays to implement dense lookup tables. DRAF overlaps DRAM operations like bitline precharge and charge restoration with routing within the reconfigurable routing fabric to minimize the impact of DRAM latency. It also supports multiple configuration contexts that can be used to quickly switch between different accelerators with minimal latency. Overall, DRAF trades off some of the performance of FPGAs for significant gains in area and power. DRAF improves area density by 10x over FPGAs and power consumption by more than 3x, enabling DRAF to satisfy demanding applications within strict power and cost constraints. While accelerators mapped to DRAF are 2-3x slower than those in FPGAs, they still deliver a 13x speedup and an 11x reduction in power consumption over a Xeon core for a wide range of datacenter tasks, including analytics and interactive services like speech recognition.},
keywords={Field programmable gate arrays;Random access memory;Table lookup;Fabrics;Servers;Power demand;Switches;DRAM;reconfigurable logic;FPGA;low-power},
doi={10.1109/ISCA.2016.51},
ISSN={1063-6897},
month={June},}

@ARTICLE{7167661,
author={Delimitrou, Christina and Kozyrakis, Christos},
journal={IEEE Computer Architecture Letters}, title={Security Implications of Data Mining in Cloud Scheduling},
year={2016},
volume={15},
number={2},
pages={109-112},
abstract={Cloud providers host an increasing number of popular applications, on the premise of resource flexibility and cost efficiency. Most of these systems expose virtualized resources of different types and sizes. As instances share the same physical host to increase utilization, they contend on hardware resources, e.g., last-level cache, making them vulnerable to side-channel attacks from co-scheduled applications. In this work we show that using data mining techniques can help an adversarial user of the cloud determine the nature and characteristics of co-scheduled applications and negatively impact their performance through targeted contention injections. We design Bolt, a simple runtime that extracts the sensitivity of co-scheduled applications to various types of interference and uses this signal to determine the type of these applications by applying a set of data mining techniques. We validate the accuracy of Bolt on a 39-server cluster. Bolt correctly identifies the type and characteristics of 81 percent out of 108 victim applications, and constructs specialized contention signals that degrade their performance. We also use Bolt to find the most commonly-run applications on EC2. We hope that underlining such security vulnerabilities in modern cloud facilities will encourage cloud providers to introduce stronger resource isolation primitives in their systems.},
keywords={Data mining;Interference;Cloud computing;Computer crime;Data mining;Servers;Degradation;Super (very large) computers;security and privacy protection;scheduling and task partitioning;application studies resulting in better multiple-processor systems},
doi={10.1109/LCA.2015.2461215},
ISSN={1556-6064},
month={July},}
@INPROCEEDINGS{7429299,
author={Gao, Mingyu and Ayers, Grant and Kozyrakis, Christos},
booktitle={2015 International Conference on Parallel Architecture and Compilation (PACT)}, title={Practical Near-Data Processing for In-Memory Analytics Frameworks},
year={2015},
volume={},
number={},
pages={113-124},
abstract={The end of Dennard scaling has made all systemsenergy-constrained. For data-intensive applications with limitedtemporal locality, the major energy bottleneck is data movementbetween processor chips and main memory modules. For such workloads, the best way to optimize energy is to place processing near the datain main memory. Advances in 3D integrationprovide an opportunity to implement near-data processing (NDP) withoutthe technology problems that similar efforts had in the past. This paper develops the hardware and software of an NDP architecturefor in-memory analytics frameworks, including MapReduce, graphprocessing, and deep neural networks. We develop simple but scalablehardware support for coherence, communication, and synchronization, anda runtime system that is sufficient to support analytics frameworks withcomplex data patterns while hiding all thedetails of the NDP hardware. Our NDP architecture provides up to 16x performance and energy advantageover conventional approaches, and 2.5x over recently-proposed NDP systems. We also investigate the balance between processing and memory throughput, as well as the scalability and physical and logical organization of the memory system. Finally, we show that it is critical to optimize software frameworksfor spatial locality as it leads to 2.9x efficiency improvements for NDP.},
keywords={Hardware;Computer architecture;Three-dimensional displays;Instruction sets;Random access memory;Runtime;Near-data processing;Processing in memory;Energy efficiency;In-memory analytics},
doi={10.1109/PACT.2015.22},
ISSN={1089-795X},
month={Oct},}
@ARTICLE{7368008,
author={Sabry Aly, Mohamed M. and Gao, Mingyu and Hills, Gage and Lee, Chi-Shuen and Pitner, Greg and Shulaker, Max M. and Wu, Tony F. and Asheghi, Mehdi and Bokor, Jeff and Franchetti, Franz and Goodson, Kenneth E. and Kozyrakis, Christos and Markov, Igor and Olukotun, Kunle and Pileggi, Larry and Pop, Eric and Rabaey, Jan and Ré, Christopher and Wong, H.-S. Philip and Mitra, Subhasish},
journal={Computer}, title={Energy-Efficient Abundant-Data Computing: The N3XT 1,000x},
year={2015},
volume={48},
number={12},
pages={24-33},
abstract={Next-generation information technologies will process unprecedented amounts of loosely structured data that overwhelm existing computing systems. N3XT improves the energy efficiency of abundant-data applications 1,000-fold by using new logic and memory technologies, 3D integration with fine-grained connectivity, and new architectures for computation immersed in memory.},
keywords={Three-dimensional displays;CNTFETs;Random access memory;Memory management;Logic gates;Energy efficiency;green computing;high-performance computing;big data;hardware;emerging technologies;systems architectures;integration and modeling;nanotechnology},
doi={10.1109/MC.2015.376},
ISSN={1558-0814},
month={Dec},}
@INPROCEEDINGS{7284086,
author={Lo, David and Cheng, Liqun and Govindaraju, Rama and Ranganathan, Parthasarathy and Kozyrakis, Christos},
booktitle={2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)}, title={Heracles: Improving resource efficiency at scale},
year={2015},
volume={},
number={},
pages={450-462},
abstract={User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low traffic. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy-efficiency of large-scale datacenters. With technology scaling slowing down, it becomes important to address this opportunity. We present Heracles, a feedback-based controller that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks. We evaluate Heracles using production latency-critical and batch workloads from Google and demonstrate average server utilizations of 90% without latency violations across all the load and colocation scenarios that we evaluated.},
keywords={Lead;Wireless sensor networks},
doi={10.1145/2749469.2749475},
ISSN={1063-6897},
month={June},}
@INPROCEEDINGS{6853237,
author={Lo, David and Cheng, Liqun and Govindaraju, Rama and Barroso, Luiz André and Kozyrakis, Christos},
booktitle={2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)}, title={Towards energy proportionality for large-scale latency-critical workloads},
year={2014},
volume={},
number={},
pages={301-312},
abstract={Reducing the energy footprint of warehouse-scale computer (WSC) systems is key to their affordability, yet difficult to achieve in practice. The lack of energy proportionality of typical WSC hardware and the fact that important workloads (such as search) require all servers to remain up regardless of traffic intensity renders existing power management techniques ineffective at reducing WSC energy use. We present PEGASUS, a feedback-based controller that significantly improves the energy proportionality of WSC systems, as demonstrated by a real implementation in a Google search cluster. PEGASUS uses request latency statistics to dynamically adjust server power management limits in a fine-grain manner, running each server just fast enough to meet global service-level latency objectives. In large cluster experiments, PEGASUS reduces power consumption by up to 20%. We also estimate that a distributed version of PEGASUS can nearly double these savings.},
keywords={Abstracts;Servers;Internet;Indexes},
doi={10.1109/ISCA.2014.6853237},
ISSN={1063-6897},
month={June},}

@INPROCEEDINGS{6835969,
author={Lo, David and Kozyrakis, Christos},
booktitle={2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)}, title={Dynamic management of TurboMode in modern multi-core chips},
year={2014},
volume={},
number={},
pages={603-613},
abstract={Dynamic overclocking of CPUs, or TurboMode, is a feature recently introduced on all x86 multi-core chips. It leverages thermal and power headroom from idle execution resources to overclock active cores to increase performance. TurboMode can accelerate CPU-bound applications at the cost of additional power consumption. Nevertheless, naive use of TurboMode can significantly increase power consumption without increasing performance. Thus far, there is no strategy for managing TurboMode to optimize its use across all workloads and efficiency metrics. This paper analyzes the impact of TurboMode on a wide range of efficiency metrics (performance, power, cost, and combined metrics such as QPS/W and ED2) for representative server workloads on various hardware configurations. We determine that TurboMode is generally beneficial for performance (up to +24%), cost efficiency (QPS/$ up to +8%), energy-delay product (ED, up to +47%), and energy-delay-squared product (ED2, up to +68%). However, TurboMode is inefficient for workloads that exhibit interference for shared resources. We use this information to build and validate a model that predicts the optimal TurboMode setting for each efficiency metric. We then implement autoturbo, a background daemon that dynamically manages TurboMode in real time without any hardware changes. We demonstrate that autoturbo improves QPS/$, ED, and ED2 by 8%, 47%, and 68% respectively over not using TurboMode. At the same time, autoturbo virtually eliminates all the large drops in those same metrics (-12%, -25%, -25% for QPS/$, ED, and ED2) that occur when TurboMode is used naively (always on).},
keywords={Hardware;Servers;Interference;Bridges;Clocks;Quality of service},
doi={10.1109/HPCA.2014.6835969},
ISSN={2378-203X},
month={Feb},}

@ARTICLE{6756704,
author={Delimitrou, Christina and Kozyrakis, Christos},
journal={IEEE Micro}, title={Quality-of-Service-Aware Scheduling in Heterogeneous Data centers with Paragon},
year={2014},
volume={34},
number={3},
pages={17-30},
abstract={Large-scale datacenters host tens of thousands of diverse applications each day. However, performance is degraded by interference between colocated workloads and the difficulty of matching applications to one of the many hardware platforms available, violating the quality of service (QoS) guarantees that many cloud workloads require. Thus, the authors present Paragon, an online and scalable datacenter scheduler that is aware of heterogeneity and interference. Paragon is derived from robust analytical methods. Instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown, incoming workload with respect to heterogeneity and interference by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily schedule applications in a manner that minimizes interference and maximizes server utilization. Paragon scales to tens of thousands of servers with marginal scheduling overheads. The authors evaluated Paragon with many workload scenarios, on both small and large-scale systems, including 1,000 servers on Amazon Elastic Compute Cloud (Amazon EC2). For a 2,500-workload scenario, Paragon preserves performance constraints for 91 percent of applications, while significantly improving utilization. In comparison, a baseline least-loaded scheduler only provides similar guarantees for 3 percent of workloads. The differences are more striking during high load when resource efficiency is more critical.},
keywords={Quality of service;Data centers;Computer architecture;Scheduling;Quality of service;Collaboration;Quality of service;Data centers;Computer architecture;Scheduling;Quality of service;Collaboration;high performance computing;datacenter;cloud computing;heterogeneity;interference;scheduling;quality of service;QoS;EC2;virtualization;networking},
doi={10.1109/MM.2014.7},
ISSN={1937-4143},
month={May},}



@INPROCEEDINGS{6557164,
author={Kozyrakis, Christos},
booktitle={2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, title={Advancing computer systems without technology progress},
year={2013},
volume={},
number={},
pages={142-142},
abstract={Summary form only given. Computing is now an essential tool for all aspects of human endeavor, including healthcare, education, science, commerce, government, and entertainment. We expect our computers, whether those hidden away in data-centers or those in a handheld form factor, to be capable of running sophisticated algorithms that process rapidly growing volumes of data. In other words, we expect our computers to have exponentially increasing performance at constant cost (energy and chip area). For decades, CMOS technology has been our ally, providing exponential improvements in both transistor density and energy consumption, which we turned into exponential improvements in system performance. Unfortunately, we are now in a phase where transistor cost and energy consumption are barely scaling, making it necessary to rethink the way we build scalable systems. In this talk, we will consider how to advance computer systems without technology progress. There are several promising directions that combined can provide improvements equivalent to several decades of Moore's law. These directions include massive parallelism with locality awareness, specialization, removing the bloat from our infrastructure, increasing system utilization, and embracing approximate computing. We will review motivating results in these areas, establish that they require cross-layer optimizations across both hardware and software, and discuss the remaining challenges that systems researchers must address.},
keywords={Computers;Educational institutions;Parallel processing;Hardware;Software;Transistors;Energy consumption},
doi={10.1109/ISPASS.2013.6557164},
ISSN={},
month={April},}
@INPROCEEDINGS{6513724,
author={Kozyrakis, Christos},
booktitle={2013 Design, Automation Test in Europe Conference Exhibition (DATE)}, title={Resource efficient computing for warehouse-scale datacenters},
year={2013},
volume={},
number={},
pages={1351-1356},
abstract={An increasing amount of information technology services and data are now hosted in the cloud, primarily due to the cost and scalability benefits for both the end-users and the operators of the warehouse-scale datacenters (DCs) that host cloud services. Hence, it is vital to continuously improve the capabilities and efficiency of these large-scale systems. Over the past ten years, capability has improved by increasing the number of servers in a DC and the bandwidth of the network that connects them. Cost and energy efficiency have improved by eliminating the high overheads of the power delivery and cooling infrastructure. To achieve further improvements, we must now examine how well we are utilizing the servers themselves, which are the primary determinant for DC performance, cost, and energy efficiency. This is particularly important since the semiconductor chips used in servers are now energy limited and their efficiency does not scale as fast as in the past. This paper motivates the need for resource efficient computing in large-scale datacenters and reviews the major challenges and research opportunities.},
keywords={Servers},
doi={10.7873/DATE.2013.278},
ISSN={1530-1591},
month={March},}
@ARTICLE{6212411,
author={Delimitrou, Christina and Kozyrakis, Christos},
journal={IEEE Computer Architecture Letters}, title={The Netflix Challenge: Datacenter Edition},
year={2013},
volume={12},
number={1},
pages={29-32},
abstract={The hundreds of thousands of servers in modern warehouse-scale systems make performance and efficiency optimizations pressing design challenges. These systems are traditionally considered homogeneous. However, that is not typically the case. Multiple server generations compose a heterogeneous environment, whose performance opportunities have not been fully explored since techniques that account for platform heterogeneity typically do not scale to the tens of thousands of applications hosted in large-scale cloud providers. We present ADSM, a scalable and efficient recommendation system for application-to-server mapping in large-scale datacenters (DCs) that is QoS-aware. ADSM overcomes the drawbacks of previous techniques, by leveraging robust and computationally efficient analytical methods to scale to tens of thousands of applications with minimal overheads. It is also QoS-aware, mapping applications to platforms while enforcing strict QoS guarantees. ADSM is derived from validated analytical models, has low and bounded prediction errors, is simple to implement and scales to thousands of applications without significant changes to the system. Over 390 real DC workloads, ADSM improves performance by 16% on average and up to 2.5x and efficiency by 22% in a DC with 10 different server configurations.},
keywords={Data centers;Large-scale systems;Computer architecture;Scheduling;Data centers;Multiprocessing systems;Computer Systems Organization;Computer System Implementation;Large and Medium (“Mainframe”) Computers;Super (very large) computers;Computer Systems Organization;Processor Architectures;Other Architecture Styles;Heterogeneous (hybrid) systems;Computer Systems Organization;Processor Architectures;Parallel Architectures;Scheduling and task partitioning;Computer Systems Organization;Performance of Systems;Design studies;Computer Systems Organization;Special-Purpose and Application-Based Systems;Application studies resulting in better multiple-processor systems;Computer Systems Organization;Performance of Systems;Measurement;evaluation;modeling;simulation of multiple-processor systems},
doi={10.1109/L-CA.2012.10},
ISSN={1556-6064},
month={January},}
@INPROCEEDINGS{6402896,
author={Delimitrou, Christina and Sankar, Sriram and Kansal, Aman and Kozyrakis, Christos},
booktitle={2012 IEEE International Symposium on Workload Characterization (IISWC)}, title={ECHO: Recreating network traffic maps for datacenters with tens of thousands of servers},
year={2012},
volume={},
number={},
pages={14-24},
abstract={Large-scale datacenters now host a large part of the world's data and computation, which makes their design a crucial architectural challenge. Datacenter (DC) applications, unlike traditional workloads, are dominated by user patterns that only emerge in the large-scale. This creates the need for concise, accurate and scalable analytical models that capture both their temporal and spatial features and can be used to create representative activity patterns. Unfortunately, previous work lacks the ability to track the complex patterns that are present in these applications, or scales poorly with the size of the system. In this work, we focus on the network aspect of datacenter workloads. We present ECHO, a scalable and accurate modeling scheme that uses hierarchical Markov Chains to capture the network activity of large-scale applications in time and space. ECHO can also use these models to re-create representative network traffic patterns. We validate the model against real DC-scale applications, such as Websearch and show marginal deviations between original and generated workloads. We verify that ECHO captures all the critical features of DC workloads, such as the locality of communication and burstiness and evaluate the granularity necessary for this. Finally we perform a detailed characterization of the network traffic for workloads in DCs of tens of thousands of servers over significant time frames.},
keywords={Servers;Load modeling;Analytical models;Data models;Markov processes;Context;Accuracy},
doi={10.1109/IISWC.2012.6402896},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6322264,
author={Kazandjieva, Maria and Heller, Brandon and Gnawali, Omprakash and Levis, Philip and Kozyrakis, Christos},
booktitle={2012 International Green Computing Conference (IGCC)}, title={Green enterprise computing data: Assumptions and realities},
year={2012},
volume={},
number={},
pages={1-10},
abstract={Until now, green computing research has largely relied on few, short-term power measurements to characterize the energy use of enterprise computing. This paper brings new and comprehensive power datasets through Powernet, a hybrid sensor network that monitors the power and utilization of the IT systems in a large academic building. Over more than two years, we have collected power data from 250+ individual computing devices and have monitored a subset of CPU and network loads. This dense, long-term monitoring allows us to extrapolate the data to a detailed breakdown of electricity use across the building's computing systems. Our datasets provide an opportunity to examine assumptions commonly made in green computing. We show that power variability both between similar devices and over time for a single device can lead to cost or savings estimates that are off by 15-20%. Extending the coverage of measured devices and the duration (to at least one month) significantly reduces errors. Lastly, our experiences with collecting data and the subsequent analysis lead to a better understanding of how one should go about power characterization studies. We provide several methodology guidelines for future green computing research.},
keywords={Buildings;Power measurement;Green products;Servers;Monitoring;Electricity;Energy measurement},
doi={10.1109/IGCC.2012.6322264},
ISSN={},
month={June},}
@INPROCEEDINGS{6237004,
author={Malladi, Krishna T. and Nothaft, Frank A. and Periyathambi, Karthika and Lee, Benjamin C. and Kozyrakis, Christos and Horowitz, Mark},
booktitle={2012 39th Annual International Symposium on Computer Architecture (ISCA)}, title={Towards energy-proportional datacenter memory with mobile DRAM},
year={2012},
volume={},
number={},
pages={37-48},
abstract={To increase datacenter energy efficiency, we need memory systems that keep pace with processor efficiency gains. Currently, servers use DDR3 memory, which is designed for high bandwidth but not for energy proportionality. A system using 20% of the peak DDR3 bandwidth consumes 2.3× the energy per bit compared to the energy consumed by a system with fully utilized memory bandwidth. Nevertheless, many datacenter applications stress memory capacity and latency but not memory bandwidth. In response, we architect server memory systems using mobile DRAM devices, trading peak bandwidth for lower energy consumption per bit and more efficient idle modes. We demonstrate 3-5× lower memory power, better proportionality, and negligible performance penalties for data-center workloads.},
keywords={Bandwidth;Random access memory;Servers;Mobile communication;Memory management;Web search;Stress},
doi={10.1109/ISCA.2012.6237004},
ISSN={1063-6897},
month={June},}
@ARTICLE{6175882,
author={Sanchez, Daniel and Kozyrakis, Christos},
journal={IEEE Micro}, title={Scalable and Efficient Fine-Grained Cache Partitioning with Vantage},
year={2012},
volume={32},
number={3},
pages={26-37},
abstract={The Vantage cache-partitioning technique enables configurability and quality-of-service guarantees in large-scale chip multiprocessors with shared caches. Caches can have hundreds of partitions with sizes specified at cache line granularity, while maintaining high associativity and strict isolation among partitions.},
keywords={Apertures;Analytical models;Resource management;Quality of service;Cache memory;Interference;Quality of service;Large-scale systems;cache memories;design styles;memory structures;hardware;parallel architectures;processor architectures;computer systems organization;memory hierarchy;microarchitecture implementation considerations;processor architectures;computer systems organization;Vantage;cache partitioning;CMP;QoS},
doi={10.1109/MM.2012.19},
ISSN={1937-4143},
month={May},}
@INPROCEEDINGS{6168950,
author={Sanchez, Daniel and Kozyrakis, Christos},
booktitle={IEEE International Symposium on High-Performance Comp Architecture}, title={SCD: A scalable coherence directory with flexible sharer set encoding},
year={2012},
volume={},
number={},
pages={1-12},
abstract={Large-scale CMPs with hundreds of cores require a directory-based protocol to maintain cache coherence. However, previously proposed coherence directories are hard to scale beyond tens of cores, requiring either excessive area or energy, complex hierarchical protocols, or inexact representations of sharer sets that increase coherence traffic and degrade performance. We present SCD, a scalable coherence directory that relies on efficient highly-associative caches (such as zcaches) to implement a single-level directory that scales to thousands of cores, tracks sharer sets exactly, and incurs negligible directory-induced invalidations. SCD scales because, unlike conventional directories, it uses a variable number of directory tags to represent sharer sets: lines with one or few sharers use a single tag, while widely shared lines use additional tags, so tags remain small as the system scales up. We show that, thanks to the efficient highly-associative array it relies on, SCD can be fully characterized using analytical models, and can be sized to guarantee a negligible number of evictions independently of the workload. We evaluate SCD using simulations of a 1024-core CMP. For the same level of coverage, we find that SCD is 13× more area-efficient than full-map sparse directories, and 2× more area-efficient and faster than hierarchical directories, while requiring a simpler protocol. Furthermore, we show that SCD's analytical models are accurate in practice.},
keywords={Coherence;Arrays;Protocols;Organizations;Indexes;Analytical models;Vectors},
doi={10.1109/HPCA.2012.6168950},
ISSN={2378-203X},
month={Feb},}
@ARTICLE{6109202,
author={Delimitrou, Christina and Sankar, Sriram and Vaid, Kushagra and Kozyrakis, Christos},
journal={IEEE Computer Architecture Letters}, title={Decoupling Datacenter Storage Studies from Access to Large-Scale Applications},
year={2012},
volume={11},
number={2},
pages={53-56},
abstract={Suboptimal storage design has significant cost and power impact in large-scale datacenters (DCs). Performance, power and cost-optimized systems require deep understanding of target workloads, and mechanisms to effectively model different storage design choices. Traditional benchmarking is invalid in cloud data-stores, representative storage profiles are hard to obtain, while replaying applications in different storage configurations is impractical both in cost and time. Despite these issues, current workload generators are not able to reproduce key aspects of real application patterns (e.g., spatial/temporal locality, I/O intensity). In this paper, we propose a modeling and generation framework for large-scale storage applications. As part of this framework we use a state diagram-based storage model, extend it to a hierarchical representation, and implement a tool that consistently recreates DC application I/O loads. We present the principal features of the framework that allow accurate modeling and generation of storage workloads, and the validation process performed against ten original DC application traces. Finally, we explore two practical applications of this methodology: SSD caching and defragmentation benefits on enterprise storage. Since knowledge of the workload's spatial and temporal locality is necessary to model these use cases, our framework was instrumental in quantifying their performance benefits. The proposed methodology provides detailed understanding of the storage activity of large-scale applications, and enables a wide spectrum of storage studies, without the requirement to access application code and full application deployment.},
keywords={Load modeling;Very large scale integration;Throughput;Storage area networks;Computational modeling;Electronic mail;Generators;Modeling of computer architecture;Super (very large) computers;Mass storage;Modeling techniques},
doi={10.1109/L-CA.2011.37},
ISSN={1556-6064},
month={July},}
@INPROCEEDINGS{7477514,
author={Sanchez, Daniel and Kozyrakis, Christos},
booktitle={2011 IEEE Hot Chips 23 Symposium (HCS)}, title={A few ways can take you a long way: Efficient and highly associative caches with scalable partitioning for many-core CMPs},
year={2011},
volume={},
number={},
pages={1-1},
abstract={This article consists of a collection of slides from the author's conference presentation on a comparative analysis of ZCache versus Vantage, single chip multi-core processors.},
keywords={Program processors;Multicore processors;Cache storage;Memory management;Computer architecture},
doi={10.1109/HOTCHIPS.2011.7477514},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6307780,
author={Sanchez, Daniel and Kozyrakis, Christos},
booktitle={2011 38th Annual International Symposium on Computer Architecture (ISCA)}, title={Vantage: Scalable and efficient fine-grain cache partitioning},
year={2011},
volume={},
number={},
pages={57-68},
abstract={Cache partitioning has a wide range of uses in CMPs, from guaranteeing quality of service and controlled sharing to security-related techniques. However, existing cache partitioning schemes (such as way-partitioning) are limited to coarse-grain allocations, can only support few partitions, and reduce cache associativity, hurting performance. Hence, these techniques can only be applied to CMPs with 2-4 cores, but fail to scale to tens of cores. We present Vantage, a novel cache partitioning technique that overcomes the limitations of existing schemes: caches can have tens of partitions with sizes specified at cache line granularity, while maintaining high associativity and strong isolation among partitions. Vantage leverages cache arrays with good hashing and associativity, which enable soft-pinning a large portion of cache lines. It enforces capacity allocations by controlling the replacement process. Unlike prior schemes, Vantage provides strict isolation guarantees by partitioning most (e.g. 90%) of the cache instead of all of it. Vantage is derived from analytical models, which allow us to provide strong guarantees and bounds on associativity and sizing independent of the number of partitions and their behaviors. It is simple to implement, requiring around 1.5% state overhead and simple changes to the cache controller. We evaluate Vantage using extensive simulations. On a 32-core system, using 350 multi programmed workloads and one partition per core, partitioning the last-level cache with conventional techniques degrades throughput for 71 % of the workloads versus an unpartitioned cache (by 7% average, 25% maximum degradation), even when using 64-way caches. In contrast, Vantage improves throughput for 98% of the workloads, by 8% on average (up to 20%), using a 4-way cache.},
keywords={Abstracts;USA Councils;Frequency modulation;Silicon;Cache partitioning;shared cache;multi-core;QoS},
doi={},
ISSN={1063-6897},
month={June},}
@INPROCEEDINGS{6113785,
author={Sanchez, Daniel and Lo, David and Yoo, Richard M. and Sugerman, Jeremy and Kozyrakis, Christos},
booktitle={2011 International Conference on Parallel Architectures and Compilation Techniques}, title={Dynamic Fine-Grain Scheduling of Pipeline Parallelism},
year={2011},
volume={},
number={},
pages={22-32},
abstract={Scheduling pipeline-parallel programs, defined as a graph of stages that communicate explicitly through queues, is challenging. When the application is regular and the underlying architecture can guarantee predictable execution times, several techniques exist to compute highly optimized static schedules. However, these schedules do not admit run-time load balancing, so variability introduced by the application or the underlying hardware causes load imbalance, hindering performance. On the other hand, existing schemes for dynamic fine-grain load balancing (such as task-stealing) do not work well on pipeline-parallel programs: they cannot guarantee memory footprint bounds, and do not adequately schedule complex graphs or graphs with ordered queues. We present a scheduler implementation for pipeline-parallel programs that performs fine-grain dynamic load balancing efficiently. Specifically, we implement the first real runtime for GRAMPS, a recently proposed programming model that focuses on supporting irregular pipeline and data-parallel applications (in contrast to classical stream programming models and schedulers, which require programs to be regular). Task-stealing with per-stage queues and queuing policies, coupled with a backpressure mechanism, allow us to maintain strict footprint bounds, and a buffer management scheme based on packet-stealing allows low-overhead and locality-aware dynamic allocation of queue data. We evaluate our runtime on a multi-core SMP and find that it provides low-overhead scheduling of irregular workloads while maintaining locality. We also show that the GRAMPS scheduler outperforms several other commonly used scheduling approaches. Specifically, while a typical task-stealing scheduler performs on par with GRAMPS on simple graphs, it does significantly worse on complex ones, a canonical GPGPU scheduler cannot exploit pipeline parallelism and suffers from large memory footprints, and a typical static, streaming scheduler achieves somewhat better locality, but suffers significant load imbalance on a general-purpose multi-core due to fine-grain architecture variability (e.g., cache misses and SMT).},
keywords={Runtime;Programming;Parallel processing;Dynamic scheduling;Schedules;Instruction sets},
doi={10.1109/PACT.2011.9},
ISSN={1089-795X},
month={Oct},}
@INPROCEEDINGS{6114196,
author={Delimitrou, Christina and Sankar, Sriram and Vaid, Kushagra and Kozyrakis, Christos},
booktitle={2011 IEEE International Symposium on Workload Characterization (IISWC)}, title={Decoupling datacenter studies from access to large-scale applications: A modeling approach for storage workloads},
year={2011},
volume={},
number={},
pages={51-60},
abstract={The cost and power impact of suboptimal storage configurations is significant in datacenters (DCs) as inefficiencies are aggregated over several thousand servers and represent considerable losses in capital and operating costs. Designing performance, power and cost-optimized systems requires a deep understanding of target workloads, and mechanisms to effectively model different storage design choices. Traditional benchmarking is invalid in cloud data-stores, representative storage profiles are hard to obtain, while replaying the entire application in all storage configurations is impractical both from a cost and time perspective. Despite these issues, current workload generators are not able to accurately reproduce key aspects of real application patterns. Some of these features include spatial and temporal locality, as well as tuning the intensity of the workload to emulate different storage system configurations. To address these limitations, we propose a modeling and characterization framework for large-scale storage applications. As part of this framework we use a state diagram-based storage model, extend it to a hierarchical representation and implement a tool that consistently recreates I/O loads of DC applications. We present the principal features of the framework that allow accurate modeling and generation of storage workloads and the validation process performed against ten original DC applications traces. Furthermore, using our framework, we perform an in-depth, per-thread characterization of these applications and provide insights on their behavior. Finally, we explore two practical applications of this methodology: SSD caching and defragmentation benefits on enterprise storage. In both cases we observe significant speedup for most of the examined applications. Since knowledge of the workload's spatial and temporal locality is necessary to model these use cases, our framework was instrumental in quantifying their performance benefits. The proposed methodology provides a detailed understanding on the storage activity of large-scale applications and enables a wide spectrum of storage studies without the requirement for access to real applications and full application deployment.},
keywords={Load modeling;Servers;Generators;Production;Throughput;Switches;Instruments},
doi={10.1109/IISWC.2011.6114196},
ISSN={},
month={Nov},}
@INPROCEEDINGS{5961496,
author={Delimitrou, Christina and Kozyrakis, Christos},
booktitle={2011 31st International Conference on Distributed Computing Systems Workshops}, title={Cross-Examination of Datacenter Workload Modeling Techniques},
year={2011},
volume={},
number={},
pages={72-79},
abstract={Data center workload modeling has become a necessity in recent years due to the emergence of large-scale applications and cloud data-stores, whose implementation remains largely unknown. Detailed knowledge of target workloads is critical in order to correctly provision performance, power and cost-optimized systems. In this work we aggregate previous work on data center workload modeling and perform a qualitative comparison based on the representativeness, accuracy and completeness of these designs. We categorize modeling techniques in two main approaches, in-breadth and in-depth, based on the way they address the modeling of the workload. The former models the behavior of a workload in specific system parts, while the latter traces a user request throughout its execution. Furthermore, we propose the early concept of a new design, which bridges the gap between these two approaches by combining some features from each one. Some first results on the request features and performance metrics of the generated workload based on this design appear promising as far as the accuracy of the model is concerned.},
keywords={Computational modeling;Hidden Markov models;Servers;Measurement;Correlation;Load modeling;Predictive models;workload;modeling;datacenter;applications},
doi={10.1109/ICDCSW.2011.45},
ISSN={2332-5666},
month={June},}
@INPROCEEDINGS{5762724,
author={Delimitrou, Christina and Sankar, Sriram and Vaid, Kushagra and Kozyrakis, Christos},
booktitle={(IEEE ISPASS) IEEE International Symposium on Performance Analysis of Systems and Software}, title={Storage I/O generation and replay for datacenter applications},
year={2011},
volume={},
number={},
pages={123-124},
abstract={With the advent of social networking and cloud data-stores, user data is increasingly being stored in large capacity and high performance storage systems, which account for a significant portion of the total cost of ownership of a datacenter (DC) [3]. One of the main challenges when trying to evaluate storage system options is the difficulty in replaying the entire application in all possible system configurations. Furthermore, code and datasets of DC applications are rarely available to storage system designers. This makes the development of a representative model that captures key aspects of the workload's storage profile, even more appealing. Once such a model is available, the next step is to create a tool that convincingly reproduces the application's storage behavior via a synthetic I/O access pattern.},
keywords={Load modeling;Measurement;Servers;Throughput;Production;Accuracy;Message systems},
doi={10.1109/ISPASS.2011.5762724},
ISSN={},
month={April},}
@INPROCEEDINGS{5695536,
author={Sanchez, Daniel and Kozyrakis, Christos},
booktitle={2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture}, title={The ZCache: Decoupling Ways and Associativity},
year={2010},
volume={},
number={},
pages={187-198},
abstract={The ever-increasing importance of main memory latency and bandwidth is pushing CMPs towards caches with higher capacity and associativity. Associativity is typically improved by increasing the number of ways. This reduces conflict misses, but increases hit latency and energy, placing a stringent trade-off on cache design. We present the zcache, a cache design that allows much higher associativity than the number of physical ways (e.g. a 64-associative cache with 4 ways). The zcache draws on previous research on skew-associative caches and cuckoo hashing. Hits, the common case, require a single lookup, incurring the latency and energy costs of a cache with a very low number of ways. On a miss, additional tag lookups happen off the critical path, yielding an arbitrarily large number of replacement candidates for the incoming block. Unlike conventional designs, the zcache provides associativity by increasing the number of replacement candidates, but not the number of cache ways. To understand the implications of this approach, we develop a general analysis framework that allows to compare associativity across different cache designs (e.g. a set-associative cache and a zcache) by representing associativity as a probability distribution. We use this framework to show that for zcaches, associativity depends only on the number of replacement candidates, and is independent of other factors (such as the number of cache ways or the workload). We also show that, for the same number of replacement candidates, the associativity of a zcache is superior than that of a set-associative cache for most workloads. Finally, we perform detailed simulations of multithreaded and multiprogrammed workloads on a large-scale CMP with zcache as the last-level cache. We show that zcaches provide higher performance and better energy efficiency than conventional caches without incurring the overheads of designs with a large number of ways.},
keywords={Arrays;Bandwidth;Indexes;Radiation detectors;Process control;Delay;Program processors;cache;associativity;performance;multi-core;energy efficiency},
doi={10.1109/MICRO.2010.20},
ISSN={2379-3155},
month={Dec},}
@INPROCEEDINGS{5691253,
author={Leverich, Jacob and Talwar, Vanish and Ranganathan, Parthasarathy and Kozyrakis, Christos},
booktitle={2010 International Conference on Network and Service Management}, title={Evaluating impact of manageability features on device performance},
year={2010},
volume={},
number={},
pages={426-430},
abstract={Manageability is a key design constraint for IT solutions, defined as the range of operations required to maintain and administer system resources through their lifecycle phases. Emerging complex and powerful management platforms and automation software expose new tensions-between the host and management applications, between costs and performance, and between costs and complexity. In this paper, we take a systematic approach to the evaluation of manageability workloads and define metrics for evaluating manageability efficiency. We propose the Manageability Quotient (MaQ) as a holistic measure of a system's ability to deliver guarantees on both host and management application performance while minimizing cost. We evaluate a range of host and management workloads on various manageability platforms using the metrics. Our results, based on more than 4000 experiments, provides insights on the merits and demerits of different configurations.},
keywords={Measurement;Hardware;Benchmark testing;Software;Storage area networks;Complexity theory;Degradation},
doi={10.1109/CNSM.2010.5691253},
ISSN={2165-963X},
month={Oct},}
@INPROCEEDINGS{5648812,
author={Sungpack Hong and Oguntebi, Tayo and Casper, Jared and Bronson, Nathan and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={IEEE International Symposium on Workload Characterization (IISWC'10)}, title={Eigenbench: A simple exploration tool for orthogonal TM characteristics},
year={2010},
volume={},
number={},
pages={1-11},
abstract={There are a significant number of Transactional Memory(TM) proposals, varying in almost all aspects of the design space. Although several transactional benchmarks have been suggested, a simple, yet thorough, evaluation framework is still needed to completely characterize a TM system and allow for comparison among the various proposals. Unfortunately, TM system evaluation is difficult because the application characteristics which affect performance are often difficult to isolate from each other. We propose a set of orthogonal application characteristics that form a basis for transactional behavior and are useful in fully understanding the performance of a TM system. In this paper, we present EigenBench, a lightweight yet powerful microbenchmark for fully evaluating a transactional memory system. We show that EigenBench is useful for thoroughly exploring the orthogonal space of TM application characteristics. Because of its flexibility, our microbenchmark is also capable of reproducing a representative set of TM performance pathologies. In this paper, we use Eigenbench to evaluate two well-known TM systems and provide significant insight about their strengths and weaknesses. We also demonstrate how EigenBench can be used to mimic the evaluation coverage of a popular TM benchmark suite called STAMP.},
keywords={Benchmark testing;Pollution;Pathology;Indexes;Concurrent computing;Equations;History},
doi={10.1109/IISWC.2010.5648812},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5628620,
author={Baek, Woongki and Bronson, Nathan and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={2010 15th IEEE International Conference on Engineering of Complex Computer Systems}, title={Implementing and Evaluating a Model Checker for Transactional Memory Systems},
year={2010},
volume={},
number={},
pages={117-126},
abstract={Transactional Memory (TM) is a promising technique that addresses the difficulty of parallel programming. Since TM takes responsibility for all concurrency control, TM systems are highly vulnerable to subtle correctness errors. Due to the difficulty of fully proving the correctness of TM systems, many of them are used without any formal correctness guarantees. This paper presents ChkTM, a flexible model checking environment to verify the correctness of various TM systems. ChkTM aims to model TM systems close to the implementation level to reveal as many potential bugs as possible. For example, ChkTM accurately models the version control mechanism in timestamp-based software TMs (STMs). In addition, ChkTM can flexibly model TM systems that use additional hardware components or support nested parallelism. Using ChkTM, we model several TM systems including a widely-used industrial STM (TL2), a hybrid TM (SigTM) that uses hardware signatures, and an STM (NesTM) that supports nested parallel transactions. We then demonstrate how ChkTM can be used to find a previously unreported correctness bug in the current implementation of eager-versioning TL2. We also verify the serializability of TL2 and SigTM and strong isolation guarantees of SigTM. Finally, we quantitatively analyze ChkTM to understand the practical issues and motivate further research in model checking TM systems.},
keywords={Space exploration;Hardware;Program processors;Registers;Clocks;Atmospheric modeling;Model Checking;Transactional Memory},
doi={10.1109/ICECCS.2010.30},
ISSN={},
month={March},}
@ARTICLE{5550995,
author={Kozyrakis, Christos and Kansal, Aman and Sankar, Sriram and Vaid, Kushagra},
journal={IEEE Micro}, title={Server Engineering Insights for Large-Scale Online Services},
year={2010},
volume={30},
number={4},
pages={8-19},
abstract={The rapid growth of online services in the last decade has led to the development of large data centers to host these workloads. These large-scale online, user-facing services have unique engineering and capacity provisioning design requirements. The authors explore these requirements, focusing on system balancing, the impact of technology trends, and the challenges of online service workloads.},
keywords={Servers;Bandwidth;Magnetic cores;Production;Stress;Distributed databases;Hard disks;hardware;online services;data centers;large-scale production services;server design;server balancing;scale-out design;total cost of ownership},
doi={10.1109/MM.2010.73},
ISSN={1937-4143},
month={July},}
@INPROCEEDINGS{5507566,
author={Michelogiannakis, George and Sanchez, Daniel and Dally, William J. and Kozyrakis, Christos},
booktitle={2010 Fourth ACM/IEEE International Symposium on Networks-on-Chip}, title={Evaluating Bufferless Flow Control for On-chip Networks},
year={2010},
volume={},
number={},
pages={9-16},
abstract={With the emergence of on-chip networks, the power consumed by router buffers has become a primary concern. Bufferless flow control addresses this issue by removing router buffers, and handles contention by dropping or deflecting flits. This work compares virtual-channel (buffered) and deflection (packet-switched bufferless) flow control. Our evaluation includes optimizations for both schemes: buffered networks use custom SRAM-based buffers and empty buffer bypassing for energy efficiency, while bufferless networks feature a novel routing scheme that reduces average latency by 5%. Results show that unless process constraints lead to excessively costly buffers, the performance, cost and increased complexity of deflection flow control outweigh its potential gains: bufferless designs are only marginally (up to 1.5%) more energy efficient at very light loads, and buffered networks provide lower latency and higher throughput per unit power under most conditions.},
keywords={Network-on-a-chip;Routing;Costs;Delay;Energy efficiency;Lighting control;Proposals;Virtual colonoscopy;Throughput;Energy consumption;Networks;Buffers;Flow control;Multiprocessor interconnection},
doi={10.1109/NOCS.2010.10},
ISSN={},
month={May},}
@INPROCEEDINGS{5474046,
author={Oguntebi, Tayo and Hong, Sungpack and Casper, Jared and Bronson, Nathan and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={2010 18th IEEE Annual International Symposium on Field-Programmable Custom Computing Machines}, title={FARM: A Prototyping Environment for Tightly-Coupled, Heterogeneous Architectures},
year={2010},
volume={},
number={},
pages={221-228},
abstract={Computer architectures are increasingly turning to parallelism and heterogeneity as solutions for boosting performance in the face of power constraints. As this trend continues, the challenges of simulating and evaluating these architectures have grown. Hardware prototypes provide deeper insight into these systems when compared to simulators, but are traditionally more difficult and costly to build. We present the Flexible Architecture Research Machine (FARM), a hardware prototyping system based on an FPGA coherently connected to a multiprocessor system. FARM substantially reduces the difficulty and cost of building hardware prototypes by providing a ready-made framework for communicating with a custom design on the FPGA. FARM ensures efficient, low-latency communication with the FPGA via a variety of mechanisms, allowing a wide range of applications to effectively utilize the system. FARM's coherent FPGA includes a cache and participates in coherence activities with the processors. This tight coupling allows for realistic, innovative architecture prototypes that would otherwise be extremely difficult to simulate. We evaluate FARM by providing the reader with a profile of the overheads introduced across the full range of communication mechanisms. This will guide the potential FARM user towards an optimal configuration when designing his prototype.},
keywords={Prototypes;Field programmable gate arrays;Computer architecture;Hardware;Computational modeling;Virtual prototyping;Turning;Parallel processing;Boosting;Multiprocessing systems;prototyping;coherent FPGA;FPGA communication;HyperTransport;accelerators;coprocessors},
doi={10.1109/FCCM.2010.41},
ISSN={},
month={May},}
@INPROCEEDINGS{7478358,
author={Kozyrakis, Christos and Olukotun, Kunle},
booktitle={2009 IEEE Hot Chips 21 Symposium (HCS)}, title={The stanford pervasive parallelism lab},
year={2009},
volume={},
number={},
pages={1-29},
abstract={Presents a collection of slides covering the following topics: pervasive parallelism laboratory; parallel programming; PPL vision; PPL applications; virtual worlds; domain specific languages; Liszt DSL; Liszt Code; DSL infrastructure; parallel object language; Delite parallel runtime; hardware architecture; heterogeneous hardware; and fine-grain parallelism.},
keywords={Software systems;Computer architecture;Laboratories;Parallel processing;Performance evaluation},
doi={10.1109/HOTCHIPS.2009.7478358},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6375528,
author={Ahn, Jung Ho and Jouppi, Norman P. and Kozyrakis, Christos and Leverich, Jacob and Schreiber, Robert S.},
booktitle={Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis}, title={Future scaling of processor-memory interfaces},
year={2009},
volume={},
number={},
pages={1-12},
abstract={Continuous evolution in process technology brings energy-efficiency and reliability challenges, which are harder for memory system designs since chip multiprocessors demand high bandwidth and capacity, global wires improve slowly, and more cells are susceptible to hard and soft errors. Recently, there are proposals aiming at better main-memory energy efficiency by dividing a memory rank into subsets. We holistically assess the effectiveness of rank subsetting in the context of system-wide performance, energy-efficiency, and reliability perspectives. We identify the impact of rank subsetting on memory power and processor performance analytically, then verify the analyses by simulating a chip-multiprocessor system using multithreaded and consolidated workloads. We extend the design of Multicore DIMM, one proposal embodying rank subsetting, for high-reliability systems and show that compared with conventional chipkill approaches, it can lead to much higher system-level energy efficiency and performance at the cost of additional DRAM devices.},
keywords={},
doi={10.1145/1654059.1654102},
ISSN={2167-4337},
month={Nov},}
@INPROCEEDINGS{5306783,
author={Yoo, Richard M. and Romano, Anthony and Kozyrakis, Christos},
booktitle={2009 IEEE International Symposium on Workload Characterization (IISWC)}, title={Phoenix rebirth: Scalable MapReduce on a large-scale shared-memory system},
year={2009},
volume={},
number={},
pages={198-207},
abstract={Dynamic runtimes can simplify parallel programming by automatically managing concurrency and locality without further burdening the programmer. Nevertheless, implementing such runtime systems for large-scale, shared-memory systems can be challenging. This work optimizes Phoenix, a MapReduce runtime for shared-memory multi-cores and multiprocessors, on a quad-chip, 32-core, 256-thread UltraSPARC T2+ system with NUMA characteristics. We show how a multi-layered approach that comprises optimizations on the algorithm, implementation, and OS interaction leads to significant speedup improvements with 256 threads (average of 2.5times higher speedup, maximum of 19times). We also identify the roadblocks that limit the scalability of parallel runtimes on shared-memory systems, which are inherently tied to the OS scalability on large-scale systems.},
keywords={Large-scale systems;Runtime;Programming profession;Concurrent computing;Yarn;Load management;Scalability;Operating systems;Parallel programming;Delay},
doi={10.1109/IISWC.2009.5306783},
ISSN={},
month={Oct},}


@ARTICLE{5224259,
author={Leverich, Jacob and Monchiero, Matteo and Talwar, Vanish and Ranganathan, Parthasarathy and Kozyrakis, Christos},
journal={IEEE Computer Architecture Letters}, title={Power Management of Datacenter Workloads Using Per-Core Power Gating},
year={2009},
volume={8},
number={2},
pages={48-51},
abstract={While modern processors offer a wide spectrum of software-controlled power modes, most datacenters only rely on dynamic voltage and frequency scaling (DVFS, a.k.a. P-states) to achieve energy efficiency. This paper argues that, in the case of datacenter workloads, DVFS is not the only option for processor power management. We make the case for per-core power gating (PCPG) as an additional power management knob for multi-core processors. PCPG is the ability to cut the voltage supply to selected cores, thus reducing to almost zero the leakage power for the gated cores. Using a testbed based on a commercial 4-core chip and a set of real-world application traces from enterprise environments, we have evaluated the potential of PCPG. We show that PCPG can significantly reduce a processor's energy consumption (up to 40%) without significant performance overheads. When compared to DVFS, PCPG is highly effective saving up to 30% more energy than DVFS. When DVFS and PCPG operate together they can save up to almost 60%.},
keywords={Energy management;Dynamic voltage scaling;Energy consumption;Multicore processing;Power supplies;Frequency;Testing;Costs;Application software;Jacobian matrices;Energy-aware systems;System architectures;integration and modeling},
doi={10.1109/L-CA.2009.46},
ISSN={1556-6064},
month={Feb},}

@INPROCEEDINGS{7482506,
author={Kannan, Hari and Dalton, Michael and Kozyrakis, Christos},
booktitle={2007 IEEE Hot Chips 19 Symposium (HCS)}, title={Raksha: A flexible architecture for software security},
year={2007},
volume={},
number={},
pages={1-15},
abstract={This article consists of a collection of slides from the author's conference presentation on Raksha, a flexible architecture for software security. Some of the specific topics discussed include: the special features, system specifications, and system design of Raksha; system architecture; applications for use; security features supported; and targeted markets.},
keywords={Security;Software development;Computer architecture;Buffer overflows;Security of data},
doi={10.1109/HOTCHIPS.2007.7482506},
ISSN={},
month={Aug},}
@ARTICLE{4404808,
author={Rivoire, Suzanne and Shah, Mehul A. and Ranganathan, Parthasarathy and Kozyrakis, Christos and Meza, Justin},
journal={Computer}, title={Models and Metrics to Enable Energy-Efficiency Optimizations},
year={2007},
volume={40},
number={12},
pages={39-48},
abstract={Power consumption and energy efficiency are important factors in the initial design and day-to-day management of computer systems. Researchers and system designers need benchmarks that characterize energy efficiency to evaluate systems and identify promising new technologies. To predict the effects of new designs and configurations, they also need accurate methods of modeling power consumption.},
keywords={Energy efficiency;Power system modeling;Energy consumption;Hardware;Clocks;Frequency;Circuit simulation;Yield estimation;Counting circuits;Cooling;green computing;energy-efficiency optimizations;JouleSort benchmark;power models},
doi={10.1109/MC.2007.436},
ISSN={1558-0814},
month={Dec},}
@INPROCEEDINGS{4336227,
author={Baek, Woongki and Minh, Chi Cao and Trautmann, Martin and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={16th International Conference on Parallel Architecture and Compilation Techniques (PACT 2007)}, title={The OpenTM Transactional Application Programming Interface},
year={2007},
volume={},
number={},
pages={376-387},
abstract={Transactional Memory (TM) simplifies parallel programming by supporting atomic and isolated execution of user-identified tasks. To date, TM programming has re quired the use of libraries that make it difficult to achieve scalable performance with code that is easy to develop and maintain. For TM programming to become practical, it is important to integrate TM into familiar, high-level environments for parallel programming. This paper presents OpenTM, an application programming interface (API) for parallel programming with transactions. OpenTM extends OpenMP, a widely used API for shared-memory parallel programming, with a set of compiler directives to express non-blocking synchronization and speculative parallelization based on memory transactions. We also present a portable OpenTM implementation that produces code for hardware, software, and hybrid TM systems. The implementation builds upon the OpenMP support in the GCC compiler and includes a runtime for the C programming language. We evaluate the performance and programmability features of OpenTM. We show that it delivers the performance of fine-grain locks at the programming simplicity of coarse- grain locks. Compared to transactional programming with lower-level interfaces, it removes the burden of manual annotations for accesses to shared variables and enables easy changes of the scheduling and contention management policies. Overall, OpenTM provides a practical and efficient TM programming environment within the familiar scope of OpenMP.},
keywords={Parallel programming;Hardware;Application software;Content management;Scheduling;Program processors;Runtime;Computer languages;Programming environments;Software libraries},
doi={10.1109/PACT.2007.4336227},
ISSN={1089-795X},
month={Sep.},}
@ARTICLE{4287395,
author={Wawrzynek, John and Patterson, David and Oskin, Mark and Lu, Shih-Lien and Kozyrakis, Christoforos and Hoe, James C. and Chiou, Derek and Asanovic, Krste},
journal={IEEE Micro}, title={RAMP: Research Accelerator for Multiple Processors},
year={2007},
volume={27},
number={2},
pages={46-57},
abstract={The RAMP project's goal is to enable the intensive, multidisciplinary innovation that the computing industry will need to tackle the problems of parallel processing. RAMP itself is an open-source, community-developed, FPGA-based emulator of parallel architectures. its design framework lets a large, collaborative community develop and contribute reusable, composable design modules. three complete designs - for transactional memory, distributed systems, and distributed-shared memory - demonstrate the platform's potential.},
keywords={Hardware;Field programmable gate arrays;Parallel processing;Open source software;Computer industry;Technological innovation;Concurrent computing;Parallel architectures;Microprocessors;Computer architecture;hardware-software codesign;transactional memory;distributed systems;distributed-shared memory;parallel architectures;emulation;field-programmable gate arrays;modeling of computer architecture;integration},
doi={10.1109/MM.2007.39},
ISSN={1937-4143},
month={March},}
@INPROCEEDINGS{4211763,
author={Njoroge, Njuguna and Casper, Jared and Wee, Sewook and Teslyar, Yuriy and Ge, Daxia and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={2007 Design, Automation Test in Europe Conference Exhibition}, title={ATLAS: A Chip-Multiprocessor with Transactional Memory Support},
year={2007},
volume={},
number={},
pages={1-6},
abstract={Chip-multiprocessors are quickly becoming popular in embedded systems. However, the practical success of CMPs strongly depends on addressing the difficulty of multithreaded application development for such systems. Transactional memory (TM) promises to simplify concurrency management in multithreaded applications by allowing programmers to specify coarse-grain parallel tasks, while achieving performance comparable to fine-grain lock-based applications. This paper presents ATLAS, the first prototype of a CMP with hardware support for transactional memory. ATLAS includes 8 embedded PowerPC cores that access coherent shared memory in a transactional manner. The data cache for each core is modified to support the speculative buffering and conflict detection necessary for transactional execution. The authors have mapped ATLAS to the BEE2 multi-FPGA board to create a full-system prototype that operates at 100MHz, boots Linux, and provides significant performance and ease-of-use benefits for a range of parallel applications. Overall, the ATLAS prototype provides an excellent framework for further research on the software and hardware techniques necessary to deliver on the potential of transactional memory},
keywords={Prototypes;Hardware;Software prototyping;Embedded system;Memory management;Concurrent computing;Power system management;Programming profession;Linux;Application software},
doi={10.1109/DATE.2007.364558},
ISSN={1558-1101},
month={April},}
@INPROCEEDINGS{4211864,
author={Park, JongSoo and Park, Sung-Boem and Balfour, James D. and Black-Schaffer, David and Kozyrakis, Christos and Dally, William J.},
booktitle={2007 Design, Automation Test in Europe Conference Exhibition}, title={Register Pointer Architecture for Efficient Embedded Processors},
year={2007},
volume={},
number={},
pages={1-6},
abstract={Conventional register file architectures cannot optimally exploit temporal locality in data references due to their limited capacity and static encoding of register addresses in instructions. In conventional embedded architectures, the register file capacity cannot be increased without resorting to longer instruction words. Similarly, loop unrolling is often required to exploit locality in the register file accesses across iterations because naming registers statically is inflexible. Both optimizations lead to significant code size increases, which is undesirable in embedded systems. In this paper, the authors introduce the register pointer architecture (RPA), which allows registers to be accessed indirectly through register pointers. Indirection allows a larger register file to be used without increasing the length of instruction words. Additional register file capacity allows many loads and stores, such as those introduced by spill code, to be eliminated, which improves performance and reduces energy consumption. Moreover, indirection affords additional flexibility in naming registers, which reduces the need to apply loop unrolling in order to maximize reuse of register allocated variables},
keywords={Registers;Computer architecture;Microarchitecture;Embedded system;Energy consumption;Arithmetic;Application software;Computer aided instruction;Embedded computing;Laboratories},
doi={10.1109/DATE.2007.364659},
ISSN={1558-1101},
month={April},}
@ARTICLE{4205125,
author={McDonald, Austen and Carlstrom, Brian D. and Chung, JaeWoong and Minh, Chi Cao and Chafi, Hassan and Kozyrakis, Christos and Olukotun, Kunle},
journal={IEEE Micro}, title={Transactional Memory: The Hardware-Software Interface},
year={2007},
volume={27},
number={1},
pages={67-76},
abstract={As multicore chips become ubiquitous, the need to provide architectural support for practical parallel programming is reaching critical. Conventional lock-based concurrency control techniques are difficult to use, requiring the programmer to navigate through the minefield of coarse-versus fine-grained locks, deadlock, livelock, lock convoying, and priority inversion. This explicit management of concurrency is beyond the reach of the average programmer, threatening to waste the additional parallelism available with multicore architectures. This comprehensive architecture supports nested transactions, transactional handlers, and two-phase commit. The result is a seamless integration of transactional memory with modern programming languages and runtime environments},
keywords={Multicore processing;Programming profession;Parallel programming;Concurrency control;Navigation;System recovery;Waste management;Concurrent computing;Computer languages;Runtime environment;hardware/software interfaces;parallel architectures;transactional memory},
doi={10.1109/MM.2007.26},
ISSN={1937-4143},
month={Jan},}
@INPROCEEDINGS{4147652,
author={Chafi, Hassan and Casper, Jared and Carlstrom, Brian D. and McDonald, Austen and Minh, Chi Cao and Baek, Woongki and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={2007 IEEE 13th International Symposium on High Performance Computer Architecture}, title={A Scalable, Non-blocking Approach to Transactional Memory},
year={2007},
volume={},
number={},
pages={97-108},
abstract={Transactional memory (TM) provides mechanisms that promise to simplify parallel programming by eliminating the need for locks and their associated problems (deadlock, livelock, priority inversion, convoying). For TM to be adopted in the long term, not only does it need to deliver on these promises, but it needs to scale to a high number of processors. To date, proposals for scalable TM have relegated livelock issues to user-level contention managers. This paper presents the first scalable TM implementation for directory-based distributed shared memory systems that is livelock free without the need for user-level intervention. The design is a scalable implementation of optimistic concurrency control that supports parallel commits with a two-phase commit protocol, uses write-back caches, and filters coherence messages. The scalable design is based on transactional coherence and consistency (TCC), which supports continuous transactions and fault isolation. A performance evaluation of the design using both scientific and enterprise benchmarks demonstrates that the directory-based TCC design scales efficiently for NUMA systems up to 64 processors},
keywords={Protocols;Parallel programming;Concurrency control;Coherence;System recovery;Proposals;Content management;Filters;Programming profession;Large-scale systems},
doi={10.1109/HPCA.2007.346189},
ISSN={2378-203X},
month={Feb},}
@INPROCEEDINGS{4147644,
author={Ranger, Colby and Raghuraman, Ramanan and Penmetsa, Arun and Bradski, Gary and Kozyrakis, Christos},
booktitle={2007 IEEE 13th International Symposium on High Performance Computer Architecture}, title={Evaluating MapReduce for Multi-core and Multiprocessor Systems},
year={2007},
volume={},
number={},
pages={13-24},
abstract={This paper evaluates the suitability of the MapReduce model for multi-core and multi-processor systems. MapReduce was created by Google for application development on data-centers with thousands of servers. It allows programmers to write functional-style code that is automatically parallelized and scheduled in a distributed system. We describe Phoenix, an implementation of MapReduce for shared-memory systems that includes a programming API and an efficient runtime system. The Phoenix runtime automatically manages thread creation, dynamic task scheduling, data partitioning, and fault tolerance across processor nodes. We study Phoenix with multi-core and symmetric multiprocessor systems and evaluate its performance potential and error recovery features. We also compare MapReduce code to code written in lower-level APIs such as P-threads. Overall, we establish that, given a careful implementation, MapReduce is a promising model for scalable performance on shared-memory systems with simple parallel code},
keywords={Multiprocessing systems;Runtime;Programming profession;Yarn;Parallel programming;Concurrent computing;Processor scheduling;Dynamic scheduling;Fault tolerance;Laboratories},
doi={10.1109/HPCA.2007.346181},
ISSN={2378-203X},
month={Feb},}
@INPROCEEDINGS{7847589,
author={Manovit, Chaiyasit and Hangal, Sudheendra and Chafi, Hassan and McDonald, Austen and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={2006 International Conference on Parallel Architectures and Compilation Techniques (PACT)}, title={Testing implementations of transactional memory},
year={2006},
volume={},
number={},
pages={134-143},
abstract={Transactional memory is an attractive design concept for scalable multiprocessors because it offers efficient lock-free synchronization and greatly simplifies parallel software. Given the subtle issues involved with concurrency and atomicity, however, it is important that transactional memory systems be carefully designed and aggressively tested to ensure their correctness. In this paper, we propose an axiomatic framework to model the formal specification of a realistic transactional memory system which may contain a mix of transactional and non-transactional operations. Using this framework and extensions to analysis algorithms originally developed for checking traditional memory consistency, we show that the widely practiced pseudo-random testing methodology can be effectively applied to transactional memory systems. Our testing methodology was successful in finding previously unknown bugs in the implementation of TCC, a transactional memory system. We study two flavors of the underlying analysis algorithm, one incomplete and the other complete, and show that the complete algorithm while being theoretically intractable is very efficient in practice.},
keywords={Silicon;Algorithm design and analysis;Program processors;Testing;Load modeling;Context;Transactional memory;Testing;Verification;Specification},
doi={},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7477751,
author={Patterson, David and Arvind and Asanovíc, Krste and Chiou, Derek and Hoe, James and Kozyrakis, Christos and Shih-Lien Lu and Oskin, Mark and Rabaey, Jan and Wawrzynek, John},
booktitle={2006 IEEE Hot Chips 18 Symposium (HCS)}, title={Research accelerator for multiple processors},
year={2006},
volume={},
number={},
pages={1-42},
abstract={This article consists of a collection of slides from the author's conference presentation on RAMP, or research acclerators for multiple processors. Some of the specific topics discussed include: system specifications and architecture; uniprocessor performance capabilities; RAMP hardware and description language features; RAMP applications development; storage capabilities; and future areas of technological development.},
keywords={Computer architecture;Reduced instruction set computing;Microprocessors;Product design},
doi={10.1109/HOTCHIPS.2006.7477751},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7477740,
author={Kozyrakis, Christos},
booktitle={2006 IEEE Hot Chips 18 Symposium (HCS)}, title={Transactional memory implementation overview},
year={2006},
volume={},
number={},
pages={1-31},
abstract={This article consists of a collection of slides from the author's conference presentation on transactional memory (TM). Some of the specific topics discussed include: TM implementation that provide atomicity and isolation; basic TM implementation requirements; and implementation options that incorporate hardware TM, software TM and hybrid TM.},
keywords={Programming;Software development;Instruction sets;Hardware;Transactional memory;Fault tolerant systems;Memory management},
doi={10.1109/HOTCHIPS.2006.7477740},
ISSN={},
month={Aug},}

@INPROCEEDINGS{1690605,
author={Rivoire, Suzanne and Schultz, Rebecca and Okuda, Tomofumi and Kozyrakis, Christos},
booktitle={2006 International Conference on Parallel Processing (ICPP'06)}, title={Vector Lane Threading},
year={2006},
volume={},
number={},
pages={55-64},
abstract={Multi-lane vector processors achieve excellent computational throughput for programs with high data-level parallelism (DLP). However, application phases without significant DLP are unable to fully utilize the datapaths in the vector lanes. In this paper, we propose vector lane threading (VLT), an architectural enhancement that allows idle vector lanes to run short-vector or scalar threads. VLT-enhanced vector hardware can exploit both data-level and thread-level parallelism to achieve higher performance. We investigate implementation alternatives for VLT, focusing mostly on the instruction issue bandwidth requirements. We demonstrate that VLT's area overhead is small. For applications with short vectors, VLT leads to additional speedup of IA to 23 over the base vector design. For scalar threads, VLT outperforms a 2-way CMP design by a factor of two. Overall, VLT allows vector processors to reach high computational throughput for a wider range of parallel programs and become a competitive alternative to CMP systems},
keywords={Vector processors;Parallel processing;Yarn;Concurrent computing;Throughput;Hardware;Oceans;Multithreading;Bandwidth;Telecommunication computing},
doi={10.1109/ICPP.2006.74},
ISSN={2332-5690},
month={Aug},}


@INPROCEEDINGS{1656880,
author={Zmily, A. and Kozyrakis, C.},
booktitle={Proceedings of the Design Automation Test in Europe Conference}, title={Simultaneously Improving Code Size, Performance, and Energy in Embedded Processors},
year={2006},
volume={1},
number={},
pages={1-6},
abstract={Code size and energy consumption are critical design concerns for embedded processors as they determine the cost of the overall system. Techniques such as reduced length instruction sets lead to significant code size savings but also introduce performance and energy consumption impediments such as additional dynamic instructions or decompression latency. In this paper, we show that a block-aware instruction set (BLISS) which stores basic block descriptors in addition to and separately from the actual instructions in the program allows embedded processors to achieve significant improvements in all three metrics: reduced code size and improved performance and lower energy consumption},
keywords={Energy consumption;Costs;Instruction sets;Embedded system;Impedance;Delay;Batteries;Energy efficiency;Logic;Bars},
doi={10.1109/DATE.2006.244090},
ISSN={1558-1101},
month={March},}
@INPROCEEDINGS{1635940,
author={McDonald, A. and JaeWoong Chung and Carlstrom, B.D. and Chi Cao Minh and Chafi, H. and Kozyrakis, C. and Olukotun, K.},
booktitle={33rd International Symposium on Computer Architecture (ISCA'06)}, title={Architectural Semantics for Practical Transactional Memory},
year={2006},
volume={},
number={},
pages={53-65},
abstract={Transactional Memory (TM) simplifies parallel programming by allowing for parallel execution of atomic tasks. Thus far, TM systems have focused on implementing transactional state buffering and conflict resolution. Missing is a robust hardware/software interface, not limited to simplistic instructions defining transaction boundaries. Without rich semantics, current TM systems cannot support basic features of modern programming languages and operating systems such as transparent library calls, conditional synchronization, system calls, I/O, and runtime exceptions. This paper presents a comprehensive instruction set architecture (ISA) for TM systems. Our proposal introduces three key mechanisms: two-phase commit; support for software handlers on commit, violation, and abort; and full support for open- and closed-nested transactions with independent rollback. These mechanisms provide a flexible interface to implement programming language and operating system functionality. We also show that these mechanisms are practical to implement at the ISA and microarchitecture level for various TM systems. Using an execution-driven simulation, we demonstrate both the functionality (e.g., I/O and conditional scheduling within transactions) and performance potential (2.2× improvement for SPECjbb2000) of the proposed mechanisms. Overall, this paper establishes a rich and efficient interface to foster both hardware and software research on transactional memory.},
keywords={Hardware;Computer languages;Operating systems;Instruction sets;Parallel programming;Robustness;Software libraries;Runtime library;Computer architecture;Proposals},
doi={10.1109/ISCA.2006.9},
ISSN={1063-6897},
month={June},}

@INPROCEEDINGS{1598135,
author={Chung, J.W. and Chafi, H. and Minh, C.C. and McDonald, A. and Carlstrom, B. and Kozyrakis, C. and Olukotun, K.},
booktitle={The Twelfth International Symposium on High-Performance Computer Architecture, 2006.}, title={The common case transactional behavior of multithreaded programs},
year={2006},
volume={},
number={},
pages={266-277},
abstract={Transactional memory (TM) provides an easy-to-use and high-performance parallel programming model for the upcoming chip-multiprocessor systems. Several researchers have proposed alternative hardware and software TM implementations. However, the lack of transaction-based programs makes it difficult to understand the merits of each proposal and to tune future TM implementations to the common case behavior of real application. This work addresses this problem by analyzing the common case transactional behavior for 35 multithreaded programs from a wide range of application domains. We identify transactions within the source code by mapping existing primitives for parallelism and synchronization management to transaction boundaries. The analysis covers basic characteristics such as transaction length, distribution of read-set and write-set size, and the frequency of nesting and I/O operations. The measured characteristics provide key insights into the design of efficient TM systems for both non-blocking synchronization and speculative parallelization.},
keywords={Computer aided software engineering;Frequency synchronization;Application software;Concurrent computing;Parallel programming;Hardware;Proposals;Programming profession;System recovery;Robustness},
doi={10.1109/HPCA.2006.1598135},
ISSN={2378-203X},
month={Feb},}


@INPROCEEDINGS{1577776,
author={Mastroleon, L. and Bambos, N. and Kozyrakis, C. and Economou, D.},
booktitle={GLOBECOM '05. IEEE Global Telecommunications Conference, 2005.}, title={Automatic power management schemes for Internet servers and data centers},
year={2005},
volume={2},
number={},
pages={5 pp.-},
abstract={We investigate autonomic power control policies for Internet servers and data centers. In particular, by monitoring the system load and thermal status, we decide how to vary the utilized processing resources to achieve acceptable delay and power performance. We formulate the problem using a dynamic programming approach that captures the power-performance tradeoff. We study the structural properties of the optimal solution and develop low-complexity justified heuristics, which achieve significant performance gains over standard benchmarks. The performance gains are higher when the load exhibits stronger temporal variations. We also demonstrate that the heuristics are very efficient, in the sense that they perform very close to the optimal solution obtained via dynamic programming.},
keywords={Energy management;Internet;Web server;Dynamic programming;Performance gain;Power system management;Power control;Monitoring;Thermal loading;Delay},
doi={10.1109/GLOCOM.2005.1577776},
ISSN={1930-529X},
month={Nov},}

@INPROCEEDINGS{1522731,
author={Zmily, A. and Kozyrakis, C.},
booktitle={ISLPED '05. Proceedings of the 2005 International Symposium on Low Power Electronics and Design, 2005.}, title={Energy-efficient and high-performance instruction fetch using a block-aware ISA},
year={2005},
volume={},
number={},
pages={36-41},
abstract={The front-end in superscalar processors must deliver high application performance in an energy-effective manner. Impediments such as multi-cycle instruction accesses, instruction-cache misses, and mispredictions reduce performance by 48% and increase energy consumption by 21%. This paper presents a block-aware instruction set architecture (BLISS) that defines basic block descriptors in addition to the actual instructions in a program. BLISS allows for a decoupled front-end that reduces the time and energy spent on misspeculated instructions. It also allows for accurate instruction prefetching and energy efficient instruction access. A BLISS-based front-end leads to 14% IPC, 16% total energy, and 83% energy-delay-squared product improvements for wide-issue processors.},
keywords={Energy efficiency;Instruction sets;Energy consumption;Pipelines;Costs;Performance loss;Engines;Permission;Impedance;Prefetching},
doi={10.1145/1077603.1077614},
ISSN={},
month={Aug},}
@INPROCEEDINGS{1515581,
author={McDonald, A. and JaeWoong Chung and Chafi, H. and Chi Cao Minh and Carlstrom, B.D. and Hammond, L. and Kozyrakis, C. and Olukotun, K.},
booktitle={14th International Conference on Parallel Architectures and Compilation Techniques (PACT'05)}, title={Characterization of TCC on chip-multiprocessors},
year={2005},
volume={},
number={},
pages={63-74},
abstract={Transactional coherence and consistency (TCC) is a novel coherence scheme for shared memory multiprocessors that uses programmer-defined transactions as the fundamental unit of parallel work, synchronization, coherence, and consistency. TCC has the potential to simplify parallel program development and optimization by providing a smooth transition from sequential to parallel programs. In this paper, we study the implementation of TCC on chip-multiprocessors (CMPs). We explore design alternatives such as the granularity of state tracking, double-buffering, and write-update and write-invalidate protocols. Furthermore, we characterize the performance of TCC in comparison to conventional snoopy cache coherence (SCC) using parallel applications optimized for each scheme. We conclude that the two coherence schemes perform similarly, with each scheme having a slight advantage for some applications. The bandwidth requirements of TCC are slightly higher but well within the capabilities of CMP systems. Also, we find that overflow of speculative state can be effectively handled by a simple victim cache. Our results suggest TCC can provide its programming advantages without compromising the performance expected from well-tuned parallel applications.},
keywords={Parallel programming;Bandwidth;Programming profession;Coherence;Broadcasting;Hardware;Permission;Concurrent computing;Laboratories;Protocols},
doi={10.1109/PACT.2005.11},
ISSN={1089-795X},
month={Sep.},}
@INPROCEEDINGS{1488610,
author={Whaley, J. and Kozyrakis, C.},
booktitle={2005 International Conference on Parallel Processing (ICPP'05)}, title={Heuristics for profile-driven method-level speculative parallelization},
year={2005},
volume={},
number={},
pages={147-156},
abstract={Thread level speculation (TLS) is an effective technique for extracting parallelism from sequential code. Method calls provide good templates for the boundaries of speculative threads as they often describe independent tasks. However, selecting the most profitable methods to speculate on is difficult as it involves complicated trade-offs between speculation violations, thread overheads, and resource utilization. This paper presents a first analysis of heuristics for automatic selection of speculative threads across method boundaries using a dynamic or profile-driven compiler. We study the potential of three classes of heuristics that involve increasing amounts of profiling information and runtime complexity. Several of the heuristics allow for speculation to start at internal method points, nested speculation, and speculative thread preemption. Using a set of Java benchmarks, we demonstrate that careful thread selection at method boundaries leads to speedups of 1.4 to 1.8 on practical TLS hardware. Single-pass heuristics that filter out less profitable methods using simple speedup estimates lead to the best average performance by consistently providing a good balance between over- and under-speculation. On the other hand, multi-pass heuristics that perform additional filtering by taking into account interactions between nested method calls often lead to significant under-speculation and perform poorly.},
keywords={Yarn;Parallel processing;Runtime;Programming profession;Hardware;Program processors;Concurrent computing;Dynamic compiler;Java;Filters},
doi={10.1109/ICPP.2005.44},
ISSN={2332-5690},
month={June},}
@INPROCEEDINGS{1342560,
author={Labonte, F. and Mattson, P. and Thies, W. and Buck, I. and Kozyrakis, C. and Horowitz, M.},
booktitle={Proceedings. 13th International Conference on Parallel Architecture and Compilation Techniques, 2004. PACT 2004.}, title={The stream virtual machine},
year={2004},
volume={},
number={},
pages={267-277},
abstract={Stream programming is currently being pushed as a way to expose concurrency and separate communication from computation. Since there are many stream languages and potential stream execution engines, we propose an abstract machine model that captures the essential characteristics of stream architectures, the stream virtual machine (SVM). The goal of the SVM is to improve interoperability, allow development of common compilation tools and reason about stream program performance. The SVM contains control processors, slave kernel processors, and slave DMA units. The compilation process takes a stream program down to the SVM and finally down to machine binary. To extract the parameters for our SVM model, we use micro-kernels to characterize two graphics processors and a stream engine, Imagine. The results are encouraging; the model estimates the performance of the target machines with high accuracy.},
keywords={Virtual machining;Streaming media;Support vector machines;Kernel;Programming profession;Computer architecture;Power system modeling;Parallel processing;Mathematical model;Engines},
doi={10.1109/PACT.2004.1342560},
ISSN={1089-795X},
month={Oct},}
@INPROCEEDINGS{1310767,
author={Hammond, L. and Wong, V. and Chen, M. and Carlstrom, B.D. and Davis, J.D. and Hertzberg, B. and Prabhu, M.K. and Honggo Wijaya and Kozyrakis, C. and Olukotun, K.},
booktitle={Proceedings. 31st Annual International Symposium on Computer Architecture, 2004.}, title={Transactional memory coherence and consistency},
year={2004},
volume={},
number={},
pages={102-113},
abstract={In this paper, we propose a new shared memory model: transactional memory coherence and consistency (TCC). TCC provides a model in which atomic transactions are always the basic unit of parallel work, communication, memory coherence, and memory reference consistency. TCC greatly simplifies parallel software by eliminating the need for synchronization using conventional locks and semaphores, along with their complexities. TCC hardware must combine all writes from each transaction region in a program into a single packet and broadcast this packet to the permanent shared memory state atomically as a large block. This simplifies the coherence hardware because it reduces the need for small, low-latency messages and completely eliminates the need for conventional snoopy cache coherence protocols, as multiple speculatively written versions of a cache line may safely coexist within the system. Meanwhile, automatic, hardware-controlled rollback of speculative transactions resolves any correctness violations that may occur when several processors attempt to read and write the same data simultaneously. The cost of this simplified scheme is higher interprocessor bandwidth. To explore the costs and benefits of TCC, we study the characteristics of an optimal transaction-based memory system, and examine how different design parameters could affect the performance of real systems. Across a spectrum of applications, the TCC model itself did not limit available parallelism. Most applications are easily divided into transactions requiring only small write buffers, on the order of 4-8 KB. The broadcast requirements of TCC are high, but are well within the capabilities of CMPs and small-scale SMPs with high-speed interconnects.},
keywords={Computer architecture},
doi={10.1109/ISCA.2004.1310767},
ISSN={1063-6897},
month={June},}
@ARTICLE{1261385,
author={Kozyrakis, C.E. and Patterson, D.A.},
journal={IEEE Micro}, title={Scalable, vector processors for embedded systems},
year={2003},
volume={23},
number={6},
pages={36-45},
abstract={For embedded applications with data-level parallelism, a vector processor offers high performance at low power consumption and low design complexity. Unlike superscalar and VLIW designs, a vector processor is scalable and can optimally match specific application requirements.To demonstrate that vector architectures meet the requirements of embedded media processing, we evaluate the Vector IRAM, or VIRAM (pronounced "V-IRAM"), architecture developed at UC Berkeley, using benchmarks from the Embedded Microprocessor Benchmark Consortium (EEMBC). Our evaluation covers all three components of the VIRAM architecture: the instruction set, the vectorizing compiler, and the processor microarchitecture. We show that a compiler can vectorize embedded tasks automatically without compromising code density. We also describe a prototype vector processor that outperforms high-end superscalar and VLIW designs by 1.5x to 100x for media tasks, without compromising power consumption. Finally, we demonstrate that clustering and modular design techniques let a vector processor scale to tens of arithmetic data paths before wide instruction-issue capabilities become necessary.},
keywords={Vector processors;Embedded system;Registers;VLIW;Computer architecture;Energy consumption;Embedded computing;Parallel processing;Process design;High performance computing},
doi={10.1109/MM.2003.1261385},
ISSN={1937-4143},
month={Nov},}
@INPROCEEDINGS{1207017,
author={Kozyrakis, C. and Patterson, D.},
booktitle={30th Annual International Symposium on Computer Architecture, 2003. Proceedings.}, title={Overcoming the limitations of conventional vector processors},
year={2003},
volume={},
number={},
pages={399-409},
abstract={Despite their superior performance for multimedia applications, vector processors have three limitations that hinder their widespread acceptance. First, the complexity and size of the centralized vector register file limits the number of functional units. Second, precise exceptions for vector instructions are difficult to implement. Third, vector processors require an expensive on-chip memory system that supports high bandwidth at low access latency. We introduce CODE, a scalable vector microarchitecture that addresses these three shortcomings. It is designed around a clustered vector register file and uses a separate network for operand transfers across functional units. With extensive use of decoupling, it can hide the latency of communication across functional units and provides 26% performance improvement over a centralized organization. CODE scales efficiently to 8 functional units without requiring wide instruction issue capabilities. A renaming table makes the clustered register file transparent at the instruction set level. Renaming also enables precise exceptions for vector instructions at a performance loss of less than 5%. Finally, decoupling allows CODE to tolerate large increases in memory latency at sublinear performance degradation without using on-chip caches. Thus, CODE can use economical, off-chip, memory systems.},
keywords={Vector processors;Registers;Delay;System-on-a-chip;Microarchitecture;Performance loss;Computer architecture;Bandwidth;CMOS technology;Computer science},
doi={10.1109/ISCA.2003.1207017},
ISSN={1063-6897},
month={June},}
@INPROCEEDINGS{1176257,
author={Kozyrakis, C. and Patterson, D.},
booktitle={35th Annual IEEE/ACM International Symposium on Microarchitecture, 2002. (MICRO-35). Proceedings.}, title={Vector vs. superscalar and VLIW architectures for embedded multimedia benchmarks},
year={2002},
volume={},
number={},
pages={283-293},
abstract={Multimedia processing on embedded devices requires an architecture that leads to high performance, low power consumption, reduced design complexity, and small code size. In this paper, we use EEMBC, an industrial benchmark suite, to compare the VIRAM vector architecture to superscalar and VLIW processors for embedded multimedia applications. The comparison covers the VIRAM instruction set, vectorizing compiler and the prototype chip that integrates a vector processor with DRAM main memory. We demonstrate that executable code for VIRAM is up to 10 times smaller than VLIW code and comparable to /spl times/86 CISC code. The simple, cache-less VIRAM chip is 2 times faster than a 4-way superscalar RISC processor that uses a 5 times faster clock frequency and consumes 10 times more power VIRAM is also 10 times faster than cache-based VLIW processors. Even after manual optimization of the VLIW code and insertion of SIMD and DSP instructions, the single-issue VIRAM processor is 60% faster than 5-way to 8-way VLIW designs.},
keywords={VLIW;Energy consumption;Prototypes;Vector processors;Reduced instruction set computing;Clocks;Frequency;Manuals;Design optimization;Digital signal processing chips},
doi={10.1109/MICRO.2002.1176257},
ISSN={1072-4451},
month={Nov},}
@ARTICLE{964446,
author={Kozyrakis, C. and Judd, D. and Gebis, J. and Williams, S. and Patterson, D. and Yelick, K.},
journal={Proceedings of the IEEE}, title={Hardware/compiler codevelopment for an embedded media processor},
year={2001},
volume={89},
number={11},
pages={1694-1709},
abstract={Embedded and portable systems running multimedia applications create a new challenge for hardware architects. A microprocessor for such applications needs to be easy to program like a general-purpose processor and have the performance and power efficiency of a digital signal processor. This paper presents the codevelopment of the instruction set, the hardware, and the compiler for the Vector IRAM media processor. A vector architecture is used to exploit the data parallelism of multimedia programs, which allows the use of highly modular hardware and enables implementations that combine high performance, low power consumption, and reduced design complexity. It also leads to a compiler model that is efficient both in terms of performance and executable code size. The memory system for the vector processor is implemented using embedded DRAM technology, which provides high bandwidth in an integrated, cost-effective manner. The hardware and the compiler for this architecture make complementary contributions to the efficiency of the overall system. This paper explores the interactions and tradeoffs between them, as well as the enhancements to a vector architecture necessary for multimedia processing. We also describe how the architecture, design, and compiler features come together in a prototype system-on-a-chip, able to execute 3.2 billion operations per second per watt.},
keywords={Hardware;Multimedia systems;Microprocessors;Digital signal processors;Energy consumption;Power system modeling;Vector processors;Random access memory;Bandwidth;Prototypes},
doi={10.1109/5.964446},
ISSN={1558-2256},
month={Nov},}
@INPROCEEDINGS{840306,
author={Catthoor, F. and Dutt, N.D. and Kozyrakis, C.E.},
booktitle={Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)}, title={How to solve the current memory access and data transfer bottlenecks: at the processor architecture or at the compiler level?},
year={2000},
volume={},
number={},
pages={426-433},
abstract={Current processor architectures, both in the programmable and custom case, become more and more dominated by the data access bottlenecks in the cache, system bus and main memory subsystems. In order to provide sufficiently high data throughput in the emerging era of highly parallel processors where many arithmetic resources can work concurrently, novel solutions for the memory access and data transfer will have to be introduced. The crucial question we want to address is where one can expect these novel solutions to reside: will they be mainly innovative processor architecture ideas, or novel approaches in the application compiler/synthesis technology, or a mix.},
keywords={Hardware;Parallel processing;Multithreading;Microprocessors;Runtime;Throughput;Arithmetic;Electronic switching systems;Microarchitecture;Memory management},
doi={10.1109/DATE.2000.840306},
ISSN={},
month={March},}
@ARTICLE{730733,
author={Kozyrakis, C.E. and Patterson, D.A.},
journal={Computer}, title={A new direction for computer architecture research},
year={1998},
volume={31},
number={11},
pages={24-32},
abstract={In the past few years, two important trends have evolved that could change the shape of computing: multimedia applications and portable electronics. Together, these trends will lead to a personal mobile-computing environment, a small device carried all the time that incorporates the functions of the pager, cellular phone, laptop computer, PDA, digital camera, and video game. The microprocessor needed for these devices is actually a merged general-purpose processor and digital-signal processor, with the power budget of the latter. Yet for almost two decades, architecture research has focused on desktop or server machines. We are designing processors of the future with a heavy bias toward the past. To design successful processor architectures for the future, we first need to explore future applications and match their requirements in a scalable, cost-effective way. The authors describe Vector IRAM, an initial approach in this direction, and challenge others in the very successful computer architecture community to investigate architectures with a heavy bias for the future.},
keywords={Computer architecture;Portable computers;Application software;Personal digital assistants;Process design;Shape;Computer applications;Multimedia computing;Cellular phones;Digital cameras},
doi={10.1109/2.730733},
ISSN={1558-0814},
month={Nov},}
@ARTICLE{592312,
author={Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
journal={IEEE Micro}, title={A case for intelligent RAM},
year={1997},
volume={17},
number={2},
pages={34-44},
abstract={Two trends call into question the current practice of fabricating microprocessors and DRAMs as different chips on different fabrication lines. The gap between processor and DRAM speed is growing at 50% per year; and the size and organization of memory on a single DRAM chip is becoming awkward to use, yet size is growing at 60% per year. Intelligent RAM, or IRAM, merges processing and memory into a single chip to lower memory latency, increase memory bandwidth, and improve energy efficiency. It also allows more flexible selection of memory size and organization, and promises savings in board area. This article reviews the state of microprocessors and DRAMs today, explores some of the opportunities and challenges for IRAMs, and finally estimates performance and energy efficiency of three IRAM designs.},
keywords={Computer aided software engineering;Random access memory;Delay;Clocks;Bandwidth;Databases;Read-write memory;Sparse matrices;Pins;Out of order},
doi={10.1109/40.592312},
ISSN={1937-4143},
month={March},}
@INPROCEEDINGS{585348,
author={Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
booktitle={1997 IEEE International Solids-State Circuits Conference. Digest of Technical Papers}, title={Intelligent RAM (IRAM): chips that remember and compute},
year={1997},
volume={},
number={},
pages={224-225},
abstract={It is time to reconsider unifying logic and memory. Since most of the transistors on this merged chip will be devoted to memory, it is called 'intelligent RAM'. IRAM is attractive because the gigabit DRAM chip has enough transistors for both a powerful processor and a memory big enough to contain whole programs and data sets. It contains 1024 memory blocks each 1kb wide. It needs more metal layers to accelerate the long lines of 600mm/sup 2/ chips. It may require faster transistors for the high-speed interface of synchronous DRAM. Potential advantages of IRAM include lower memory latency, higher memory bandwidth, lower system power, adjustable memory width and size, and less board space. Challenges for IRAM include high chip yield given processors have not been repairable via redundancy, high memory retention rates given processors usually need higher power than DRAMs, and a fast processor given logic is slower in a DRAM process.},
keywords={Random access memory;Microprocessors;Delay;Bandwidth;Switches;Logic;Vector processors;Read-write memory;Computer science;Electronics industry},
doi={10.1109/ISSCC.1997.585348},
ISSN={0193-6530},
month={Feb},}
@ARTICLE{612252,
author={Kozyrakis, C.E. and Perissakis, S. and Patterson, D. and Anderson, T. and Asanovic, K. and Cardwell, N. and Fromm, R. and Golbus, J. and Gribstad, B. and Keeton, K. and Thomas, R. and Treuhaft, N. and Yelick, K.},
journal={Computer}, title={Scalable processors in the billion-transistor era: IRAM},
year={1997},
volume={30},
number={9},
pages={75-78},
abstract={Members of the University of California, Berkeley, argue that the memory system will be the greatest inhibitor of performance gains in future architectures. Thus, they propose the intelligent RAM or IRAM. This approach greatly increases the on-chip memory capacity by using DRAM technology instead of much less dense SRAM memory cells. The resultant on-chip memory capacity coupled with the high bandwidths available on chip should allow cost-effective vector processors to reach performance levels much higher than those of traditional architectures. Although vector processors require explicit compilation, the authors claim that vector compilation technology is mature (having been used for decades in supercomputers), and furthermore, that future workloads will contain more heavily vectorizable components.},
keywords={Random access memory;Delay;Bandwidth;Read-write memory;Out of order;VLIW;Computer architecture;Fabrication;Bridges;Microprocessor chips},
doi={10.1109/2.612252},
ISSN={1558-0814},
month={Sep.},}
@INPROCEEDINGS{634851,
author={Kornaros, G. and Kozyrakis, C. and Vatsolaki, P. and Katevenis, M.},
booktitle={Proceedings Seventeenth Conference on Advanced Research in VLSI}, title={Pipelined multi-queue management in a VLSI ATM switch chip with credit-based flow-control},
year={1997},
volume={},
number={},
pages={127-144},
abstract={We describe the queue management block of ATLAS I, a single-chip ATM switch (roster) with optional credit-based (backpressure) flow control. ATLAS I is a 4-million-transistor 0.35-micron CMOS chip, currently under development, offering 20 Gbit/s aggregate I/O throughput, sub-microsecond cut-through latency, 256-cell shared buffer containing multiple logical output queues, priorities, multicasting, and load monitoring. The queue management block of ATLAS I is a dual parallel pipeline that manages the multiple queues of ready cells, the per-flow-group credits, and the cells that are waiting for credits. All cells, in all queues, share one, common buffer space. These 3- and Q-stage pipelines handle events at the rate of one cell arrival or departure per clock cycle, and one credit arrival per clock cycle. The queue management block consists of two compiled SRAMs, pipeline bypass logic, and multi-port CAM and SRAM blocks that are laid out in full-custom and support special access operations. The full-custom part of queue management contains approximately 65 thousand transistors in logic and 14 Kbits in various special memories, it occupies 2.3 mm/sup 2/, it consumes 270 mW (worst case), and it operates at 80 MHz (worst case) versus 50 MHz which is the required clock frequency to support the 622 Mb/s switch link rate.},
keywords={Switches;Very large scale integration;Asynchronous transfer mode;Pipelines;Clocks;Logic;Aggregates;Throughput;Delay;Monitoring},
doi={10.1109/ARVLSI.1997.634851},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{604742,
author={Fromm, R. and Perissakis, S. and Cardwell, N. and Kozyrakis, C. and McGaughy, B. and Patterson, D. and Anderson, T. and Yelick, K.},
booktitle={Conference Proceedings. The 24th Annual International Symposium on Computer Architecture}, title={The Energy Efficiency Of Iram Architectures},
year={1997},
volume={},
number={},
pages={327-337},
abstract={},
keywords={Energy efficiency;Random access memory;Portable computers;Personal digital assistants;Computer architecture;Energy consumption;Permission;Power system modeling;Power system simulation;Central Processing Unit},
doi={10.1145/264107.264214},
ISSN={1063-6897},
month={June},}
@INPROCEEDINGS{628842,
author={Patterson, D. and Asanovic, K. and Brown, A. and Fromm, R. and Golbus, J. and Gribstad, B. and Keeton, K. and Kozyrakis, C. and Martin, D. and Perissakis, S. and Thomas, R. and Treuhaft, N. and Yelick, K.},
booktitle={Proceedings International Conference on Computer Design VLSI in Computers and Processors}, title={Intelligent RAM (IRAM): the industrial setting, applications, and architectures},
year={1997},
volume={},
number={},
pages={2-7},
abstract={The goal of intelligent RAM (IRAM) is to design a cost-effective computer by designing a processor in a memory fabrication process, instead of in a conventional logic fabrication process, and include memory on-chip. To design a processor in a DRAM process one must learn about the business and culture of the DRAMs, which is quite different from microprocessors. The authors describe some of those differences and their current vision of IRAM applications, architectures, and implementations.},
keywords={Random access memory;Costs;Logic testing;Application software;Fabrication;Microprocessors;Read-write memory;Logic design;Process design;Plastic packaging},
doi={10.1109/ICCD.1997.628842},
ISSN={1063-6404},
month={Oct},}

@ARTICLE{6493296,
author={Kozyrakis, Christos and Zahir, Rumi},
journal={IEEE Micro}, title={Selected Research from Hot Chips 24},
year={2013},
volume={33},
number={2},
pages={6-7},
abstract={This introduction to the special issue introduces the articles selected for publication from Hot Chips 24.},
keywords={Special issues and sections;Multiprocessors;Multicore processing;Program processors;Hot Chips;energy-efficient scaling;processing engine;low voltage;instructed set},
doi={10.1109/MM.2013.44},
ISSN={1937-4143},
month={March},}
@INPROCEEDINGS{7476477,
author={Zahir, Rumi and Kozyrakis, Christos},
booktitle={2012 IEEE Hot Chips 24 Symposium (HCS)}, title={Welcome to Hot Chips 24},
year={2012},
volume={},
number={},
pages={1-11},
abstract={Presents the opening message from the Hot Chips 24 Proceedings that was held 27-29 August 2012, in Cupertino, CA. Also includes conference program committee members, program statistics, keynote speakers, conference tutorials, conference proceedings, conference etiquette, and persons named in memoriam.},
keywords={Transforms;Google;Graphics;Servers;Multimedia communication;System-on-chip;Welding},
doi={10.1109/HOTCHIPS.2012.7476477},
ISSN={},
month={Aug},}
@ARTICLE{4812131,
author={Kozyrakis, Christos and Waerdt, Jan-Willem van de},
journal={IEEE Micro}, title={Hot Chips Turns 20 [Guest editors' introduction]},
year={2009},
volume={29},
number={2},
pages={4-5},
abstract={The five papers in this special issue are extended versions of papers presented at the Hot Chips conference in August 1008.},
keywords={Multicore processing;Field programmable gate arrays;Reduced instruction set computing;Application specific integrated circuits;CMOS technology;Technological innovation;Chip scale packaging;Parallel processing;Integrated circuit interconnections;Acceleration},
doi={10.1109/MM.2009.31},
ISSN={1937-4143},
month={March},}
@INPROCEEDINGS{7476527,
author={Kozyrakis, Christos and van de Waerdt, Jan-Willem},
booktitle={2008 IEEE Hot Chips 20 Symposium (HCS)}, title={From the program co-chairs of hot chips 20},
year={2008},
volume={},
number={},
pages={1-2},
abstract={This is an introduction to the 20th Annual Hot Chips Symposium from the program co-chairs. On behalf of the Program Committee, we are pleased to welcome you to the 20th Annual Hot Chips Symposium.},
keywords={Tutorials;Conferences;Multicore processing;Graphics processing units;Servers;Multimedia systems;Meetings},
doi={10.1109/HOTCHIPS.2008.7476527},
ISSN={},
month={Aug},}


