@article{2022-rail-tos,
author = {Litz, Heiner and Gonzalez, Javier and Klimovic, Ana and Kozyrakis, Christos},
title = {RAIL: Predictable, Low Tail Latency for NVMe Flash},
year = {2022},
month = feb,
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1553-3077},
doi = {10.1145/3465406},
abstract = {Flash-based storage is replacing disk for an increasing number of data center applications, providing orders of magnitude higher throughput and lower average latency. However, applications also require predictable storage latency. Existing Flash devices fail to provide low tail read latency in the presence of write operations. We propose two novel techniques to address SSD read tail latency, including Redundant Array of Independent LUNs (RAIL) which avoids serialization of reads behind user writes as well as latency-aware hot-cold separation (HC) which improves write throughput while maintaining low tail latency. RAIL leverages the internal parallelism of modern Flash devices and allocates data and parity pages to avoid reads getting stuck behind writes. We implement RAIL in the Linux Kernel as part of the LightNVM Flash translation layer and show that it can reduce read tail latency by 7\texttimes{} at the 99.99th percentile, while reducing relative bandwidth by only 33\%.},
journal = {ACM Transactions on Storage},
month = jan,
articleno = {5},
numpages = {21},
keywords = {LightNVM, Linux, SSD, OpenChannel}
}


@inproceedings{2022-recshard-asplos,
author = {Sethi, Geet and Acun, Bilge and Agarwal, Niket and Kozyrakis, Christos and Trippel, Caroline and Wu, Carole-Jean},
title = {RecShard: Statistical Feature-Based Memory Optimization for Industry-Scale Neural Recommendation},
year = {2022},
month = Mar,
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3503222.3507777},
abstract = {We propose RecShard, a fine-grained embedding table (EMB) partitioning and placement technique for deep learning recommendation models (DLRMs). RecShard is designed based on two key observations. First, not all EMBs are equal, nor all rows within an EMB are equal in terms of access patterns. EMBs exhibit distinct memory characteristics, providing performance optimization opportunities for intelligent EMB partitioning and placement across a tiered memory hierarchy. Second, in modern DLRMs, EMBs function as hash tables. As a result, EMBs display interesting phenomena, such as the birthday paradox, leaving EMBs severely under-utilized. RecShard determines an optimal EMB sharding strategy for a set of EMBs based on training data distributions and model characteristics, along with the bandwidth characteristics of the underlying tiered memory hierarchy. In doing so, RecShard achieves over 6 times higher EMB training throughput on average for capacity constrained DLRMs. The throughput increase comes from improved EMB load balance by over 12 times and from the reduced access to the slower memory by over 87 times.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {344–358},
numpages = {15},
keywords = {AI training systems, Neural networks, Memory optimization, Deep learning recommendation models},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@inproceedings{2022-shef-asplos,
author = {Zhao, Mark and Gao, Mingyu and Kozyrakis, Christos},
title = {ShEF: Shielded Enclaves for Cloud FPGAs},
year = {2022},
month = Mar,
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3503222.3507733},
abstract = {FPGAs are now used in public clouds to accelerate a wide range of applications, including many that operate on sensitive data such as financial and medical records. We present ShEF, a trusted execution environment (TEE) for cloud-based reconfigurable accelerators. ShEF is independent from CPU-based TEEs and allows secure execution under a threat model where the adversary can control all software running on the CPU connected to the FPGA, has physical access to the FPGA, and can compromise the FPGA interface logic of the cloud provider. ShEF provides a secure boot and remote attestation process that relies solely on existing FPGA mechanisms for root of trust. It also includes a Shield component that provides secure access to data while the accelerator is in use. The Shield is highly customizable and extensible, allowing users to craft a bespoke security solution that fits their accelerator's memory access patterns, bandwidth, and security requirements at minimum performance and area overheads. We describe a prototype implementation of ShEF for existing cloud FPGAs, map ShEF to a performant and secure storage application, and measure the performance benefits of customizable security using five additional accelerators.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {1070–1085},
numpages = {16}
}

@inproceedings{2022-sol-asplos,
author = {Wang, Yawen and Crankshaw, Daniel and Yadwadkar, Neeraja J. and Berger, Daniel and Kozyrakis, Christos and Bianchini, Ricardo},
title = {SOL: Safe on-Node Learning in Cloud Platforms},
year = {2022},
month = Mar,
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3503222.3507704},
abstract = {Cloud platforms run many software agents on each server node. These agents manage all aspects of node operation, and in some cases frequently collect data and make decisions. Unfortunately, their behavior is typically based on pre-defined static heuristics or offline analysis; they do not leverage on-node machine learning (ML). In this paper, we first characterize the spectrum of node agents in Azure, and identify the classes of agents that are most likely to benefit from on-node ML. We then propose SOL, an extensible framework for designing ML-based agents that are safe and robust to the range of failure conditions that occur in production. SOL provides a simple API to agent developers and manages the scheduling and running of the agent-specific functions they write. We illustrate the use of SOL by implementing three ML-based agents that manage CPU cores, node power, and memory placement. Our experiments show that (1) ML substantially improves our agents, and (2) SOL ensures that agents operate safely under a variety of failure conditions. We conclude that ML-based agents show significant potential and that SOL can help build them.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {622–634},
numpages = {13},
keywords = {Cloud computing, on-node agents, machine learning for systems, systems for machine learning},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@misc{2021-practical-arxiv,
  doi = {10.48550/ARXIV.2111.07226},
  author = {Kaffes, Kostis and Yadwadkar, Neeraja J. and Kozyrakis, Christos},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Practical Scheduling for Real-World Serverless Computing},
  booktitle = {arXiv},
  year = {2021},
  month = Nov,
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{2021-faast-socc,
author = {Romero, Francisco and Chaudhry, Gohar Irfan and Goiri, \'{I}\~{n}igo and Gopa, Pragna and Batum, Paul and Yadwadkar, Neeraja J. and Fonseca, Rodrigo and Kozyrakis, Christos and Bianchini, Ricardo},
title = {Faa\$T: A Transparent Auto-Scaling Cache for Serverless Applications},
year = {2021},
month = Aug,
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3472883.3486974},
abstract = {Function-as-a-Service (FaaS) has become an increasingly popular way for users to deploy their applications without the burden of managing the underlying infrastructure. However, existing FaaS platforms rely on remote storage to maintain state, limiting the set of applications that can be run efficiently. Recent caching work for FaaS platforms has tried to address this problem, but has fallen short: it disregards the widely different characteristics of FaaS applications, does not scale the cache based on data access patterns, or requires changes to applications. To address these limitations, we present Faa\$T, a transparent auto-scaling distributed cache for serverless applications. Each application gets its own cache. After a function executes and the application becomes inactive, the cache is unloaded from memory with the application. Upon reloading for the next invocation, Fa\$T pre-warms the cache with objects likely to be accessed. In addition to traditional compute-based scaling, Faa\$T scales based on working set and object sizes to manage cache space and I/O bandwidth. We motivate our design with a comprehensive study of data access patterns on Azure Functions. We implement Faa\$T for Azure Functions, and show that Faa\$T can improve performance by up to 92\% (57\% on average) for challenging applications, and reduce cost for most users compared to state-of-the-art caching systems, i.e. the cost of having to stand up additional serverful resources.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing (SoCC)},
pages = {122–137},
numpages = {16},
keywords = {Cloud computing, caching, serverless computing, function as a service (FaaS)},
location = {Seattle, WA, USA},
series = {SoCC '21}
}

@inproceedings{2021-llama-socc,
author = {Romero, Francisco and Zhao, Mark and Yadwadkar, Neeraja J. and Kozyrakis, Christos},
title = {Llama: A Heterogeneous &amp; Serverless Framework for Auto-Tuning Video Analytics Pipelines},
year = {2021},
month = aug,
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3472883.3486972},
abstract = {The proliferation of camera-enabled devices and large video repositories has led to a diverse set of video analytics applications. These applications rely on video pipelines, represented as DAGs of operations, to transform videos, process extracted metadata, and answer questions like, "Is this intersection congested?" The latency and resource efficiency of pipelines can be optimized using configurable knobs for each operation (e.g., sampling rate, batch size, or type of hardware used). However, determining efficient configurations is challenging because (a) the configuration search space is exponentially large, and (b) the optimal configuration depends on users' desired latency and cost targets, (c) input video contents may exercise different paths in the DAG and produce a variable amount intermediate results. Existing video analytics and processing systems leave it to the users to manually configure operations and select hardware resources.We present Llama: a heterogeneous and serverless framework for auto-tuning video pipelines. Given an end-to-end latency target, Llama optimizes for cost efficiency by (a) calculating a latency target for each operation invocation, and (b) dynamically running a cost-based optimizer to assign configurations across heterogeneous hardware that best meet the calculated per-invocation latency target. This makes the problem of auto-tuning large video pipelines tractable and allows us to handle input-dependent behavior, conditional branches in the DAG, and execution variability. We describe the algorithms in Llama and evaluate it on a cloud platform using serverless CPU and GPU resources. We show that compared to state-of-the-art cluster and serverless video analytics and processing systems, Llama achieves 7.8x lower latency and 16x cost reduction on average.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing (SoCC)},
pages = {1–17},
numpages = {17},
keywords = {scheduling, distributed systems, serverless computing, heterogeneous, video analytics},
location = {Seattle, WA, USA},
series = {SoCC '21}
}


@inproceedings{2021-syrup-sosp,
author = {Kaffes, Kostis and Humphries, Jack Tigar and Mazi\`{e}res, David and Kozyrakis, Christos},
title = {Syrup: User-Defined Scheduling Across the Stack},
year = {2021},
month = oct,
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3477132.3483548},
abstract = {Suboptimal scheduling decisions in operating systems, networking stacks, and application runtimes are often responsible for poor application performance, including higher latency and lower throughput. These poor decisions stem from a lack of insight into the applications and requests the scheduler is handling and a lack of coherence and coordination between the various layers of the stack, including NICs, kernels, and applications.We propose Syrup, a framework for user-defined scheduling. Syrup enables untrusted application developers to express application-specific scheduling policies across these system layers without being burdened with the low-level system mechanisms that implement them. Application developers write a scheduling policy with Syrup as a set of matching functions between inputs (threads, network packets, network connections) and executors (cores, network sockets, NIC queues) and then deploy it across system layers without modifying their code. Syrup supports multi-tenancy as multiple co-located applications can each safely and securely specify a custom policy. We present several examples of uses of Syrup to define application and workload-specific scheduling policies in a few lines of code, deploy them across the stack, and improve performance up to 8x compared with default policies.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP)},
pages = {605–620},
numpages = {16},
keywords = {kernel, scheduling, programmability},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@inproceedings{2021-ghost-sosp,
author = {Humphries, Jack Tigar and Natu, Neel and Chaugule, Ashwin and Weisse, Ofir and Rhoden, Barret and Don, Josh and Rizzo, Luigi and Rombakh, Oleg and Turner, Paul and Kozyrakis, Christos},
title = {GhOSt: Fast &amp; Flexible User-Space Delegation of Linux Scheduling},
year = {2021},
month = oct,
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3477132.3483542},
abstract = {We present ghOSt, our infrastructure for delegating kernel scheduling decisions to userspace code. ghOSt is designed to support the rapidly evolving needs of our data center workloads and platforms.Improving scheduling decisions can drastically improve the throughput, tail latency, scalability, and security of important workloads. However, kernel schedulers are difficult to implement, test, and deploy efficiently across a large fleet. Recent research suggests bespoke scheduling policies, within custom data plane operating systems, can provide compelling performance results in a data center setting. However, these gains have proved difficult to realize as it is impractical to deploy a custom OS image(s) at an application granularity, particularly in a multi-tenant environment, limiting the practical applications of these new techniques.ghOSt provides general-purpose delegation of scheduling policies to userspace processes in a Linux environment. ghOSt provides state encapsulation, communication, and action mechanisms that allow complex expression of scheduling policies within a userspace agent, while assisting in synchronization. Programmers use any language to develop and optimize policies, which are modified without a host reboot. ghOSt supports a wide range of scheduling models, from per-CPU to centralized, run-to-completion to preemptive, and incurs low overheads for scheduling actions. We demonstrate ghOSt's performance on both academic and real-world workloads, including Google Snap and Google Search. We show that by using ghOSt instead of the kernel scheduler, we can quickly achieve comparable throughput and latency while enabling policy optimization, non-disruptive upgrades, and fault isolation for our data center workloads. We open-source our implementation to enable future research and development based on ghOSt.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP)},
pages = {588–604},
numpages = {17},
keywords = {Operating systems, thread scheduling},
location = {Virtual Event, Germany},
series = {SOSP '21}
}


@inproceedings{2021-dbos-vldb,
author = {Skiadopoulos, Athinagoras and Li, Qian and Kraft, Peter and Kaffes, Kostis and Hong, Daniel and Mathew, Shana and Bestor, David and Cafarella, Michael and Gadepally, Vijay and Graefe, Goetz and Kepner, Jeremy and Kozyrakis, Christos and Kraska, Tim and Stonebraker, Michael and Suresh, Lalith and Zaharia, Matei},
title = {DBOS: A DBMS-Oriented Operating System},
year = {2021},
issue_date = {September 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {1},
issn = {2150-8097},
doi = {10.14778/3485450.3485454},
abstract = {This paper lays out the rationale for building a completely new operating system (OS) stack. Rather than build on a single node OS together with separate cluster schedulers, distributed filesystems, and network managers, we argue that a distributed transactional DBMS should be the basis for a scalable cluster OS. We show herein that such a database OS (DBOS) can do scheduling, file management, and inter-process communication with competitive performance to existing systems. In addition, significantly better analytics can be provided as well as a dramatic reduction in code complexity through implementing OS services as standard database queries, while implementing low-latency transactions and high availability only once.},
booktitle = {Proceedings of the VLDB Endowment (VLDB)},
month = sep,
pages = {21–30},
numpages = {10}
}


@inproceedings{2021-case-hotos,
author = {Humphries, Jack Tigar and Kaffes, Kostis and Mazi\`{e}res, David and Kozyrakis, Christos},
title = {A Case against (Most) Context Switches},
year = {2021},
month = jun,
isbn = {9781450384384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3458336.3465274},
abstract = {Multiplexing software threads onto hardware threads and serving interrupts, VM-exits, and system calls require frequent context switches, causing high overheads and significant kernel and application complexity. We argue that context switching is an idea whose time has come and gone, and propose eliminating it through a radically different hardware threading model targeted to solve software rather than hardware problems. The new model adds a large number of hardware threads to each physical core - making thread multiplexing unnecessary - and lets software manage them. The only state change directly triggered in hardware by system calls, exceptions, and asynchronous hardware events will be blocking and unblocking hardware threads. We also present ISA extensions to allow kernel and user software to exploit this new threading model. Developers can use these extensions to eliminate interrupts and implement fast I/O without polling, exception-less system and hypervisor calls, practical microkernels, simple distributed programming models, and untrusted but fast hypervisors. Finally, we suggest practical hardware implementations and discuss the hardware and software challenges toward realizing this novel approach.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems (HotOS)},
pages = {17–25},
numpages = {9},
location = {Ann Arbor, Michigan},
series = {HotOS '21}
}



@inproceedings{2021-serving-euromlsys,
author = {Mendoza, Daniel and Romero, Francisco and Li, Qian and Yadwadkar, Neeraja J. and Kozyrakis, Christos},
title = {Interference-Aware Scheduling for Inference Serving},
year = {2021},
month = apr
isbn = {9781450382984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3437984.3458837},
abstract = {Machine learning inference applications have proliferated through diverse domains such as healthcare, security, and analytics. Recent work has proposed inference serving systems for improving the deployment and scalability of models. To improve resource utilization, multiple models can be co-located on the same backend machine. However, co-location can cause latency degradation due to interference and can subsequently violate latency requirements. Although interference-aware schedulers for general workloads have been introduced, they do not scale appropriately to heterogeneous inference serving systems where the number of co-location configurations grows exponentially with the number of models and machine types.This paper proposes an interference-aware scheduler for heterogeneous inference serving systems, reducing the latency degradation from co-location interference. We characterize the challenges in predicting the impact of co-location interference on inference latency (e.g., varying latency degradation across machine types), and identify properties of models and hardware that should be considered during scheduling. We then propose a unified prediction model that estimates an inference model's latency degradation during co-location, and develop an interference-aware scheduler that leverages this predictor. Our preliminary results show that our interference-aware scheduler achieves 2\texttimes{} lower latency degradation than a commonly used least-loaded scheduler. We also discuss future research directions for interference-aware schedulers for inference serving systems.},
booktitle = {Proceedings of the 1st Workshop on Machine Learning and Systems (EuroMLSys)},
pages = {80–88},
numpages = {9},
keywords = {cloud computing, heterogeneity, inference serving, interference-aware scheduling, machine learning},
location = {Online, United Kingdom},
series = {EuroMLSys '21}
}


@inproceedings{2021-smartharvest-eurosys,
author = {Wang, Yawen and Arya, Kapil and Kogias, Marios and Vanga, Manohar and Bhandari, Aditya and Yadwadkar, Neeraja J. and Sen, Siddhartha and Elnikety, Sameh and Kozyrakis, Christos and Bianchini, Ricardo},
title = {SmartHarvest: Harvesting Idle CPUs Safely and Efficiently in the Cloud},
year = {2021},
month = apr,
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3447786.3456225},
abstract = {We can increase the efficiency of public cloud datacenters by harvesting allocated but temporarily idling CPU cores from customer virtual machines (VMs) to run batch or analytics workloads. Even small efficiency gains translate into substantial savings, since provisioning and operating a datacenter costs hundreds of millions of dollars per year. The main challenge is to harvest idle cores with little or no impact on customer VMs, which could be running latency-sensitive services and are essentially black-boxes to the cloud provider.We introduce ElasticVM, a new VM type that can run batch workloads cheaply using mainly harvested cores. We also propose SmartHarvest, a system that dynamically manages the number of cores available to ElasticVMs in each fine-grained time window. SmartHarvest uses online learning to predict the core demand of primary, customer VMs and compute the number of cores that can be safely harvested. Our results show that SmartHarvest can harvest a significant amount of CPU resources without increasing the 99th-percentile tail latency of latency-critical primary workloads by more than 10\%. Unlike static harvesting techniques that rely on offline profiling, SmartHarvest is robust to different primary workloads, batch workloads, and load changes. Finally, we show that the online learning in SmartHarvest is complementary to systems optimizations for VM management.},
booktitle = {Proceedings of the 16th European Conference on Computer Systems (EuroSys)},
pages = {1–16},
numpages = {16},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@ARTICLE{2021-rambo-cal,
author={Li, Qian and Li, Bin and Mercati, Pietro and Illikkal, Ramesh and Tai, Charlie and Kishinevsky, Michael and Kozyrakis, Christos},
journal={IEEE Computer Architecture Letters},
title={RAMBO: Resource Allocation for Microservices Using Bayesian Optimization},
year={2021},
volume={20},
number={1},
pages={46-49},
abstract={Microservices are becoming the defining paradigm of cloud applications, which raises urgent challenges for efficient datacenter management. Guaranteeing end-to-end Service Level Agreement (SLA) while optimizing resource allocation is critical to both cloud service providers and users. However, one application may contain hundreds of microservices, which constitute an enormous search space that is unfeasible to explore exhaustively. Thus, we propose RAMBO, an SLA-aware framework for microservices that leverages multi-objective Bayesian Optimization (BO) to allocate resources and meet performance/cost goals. Experiments conducted on a real microservice workload demonstrate that RAMBO can correctly characterize each microservice and efficiently discover Pareto-optimal solutions. We envision that the proposed methodology and results will benefit future resource planning, cluster orchestration, and job scheduling.},
keywords={Optimization;Resource management;Service level agreements;Servers;Distributed computing;Bayes methods;Distributed computing;Distributed applications;emerging technologies},
doi={10.1109/LCA.2021.3066142},
ISSN={1556-6064},
month=Jan}


@inproceedings {2021-infaas-atc,
author = {Francisco Romero and Qian Li and Neeraja J. Yadwadkar and Christos Kozyrakis},
title = {{INFaaS}: Automated Model-less Inference Serving},
booktitle = {2021 USENIX Annual Technical Conference (ATC)},
year = {2021},
month = Jul,
isbn = {978-1-939133-23-6},
pages = {397--411},
url = {https://www.usenix.org/conference/atc21/presentation/romero},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{2020-racksched-osdi,
author = {Zhu, Hang and Kaffes, Kostis and Chen, Zixu and Liu, Zhenming and Kozyrakis, Christos and Stoica, Ion and Jin, Xin},
title = {RackSched: A Microsecond-Scale Scheduler for Rack-Scale Computers},
year = {2020},
month = Oct,
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
abstract = {Low-latency online services have strict Service Level Objectives (SLOs) that require datacenter systems to support high throughput at microsecond-scale tail latency. Dataplane operating systems have been designed to scale up multicore servers with minimal overhead for such SLOs. However, as application demands continue to increase, scaling up is not enough, and serving larger demands requires these systems to scale out to multiple servers in a rack.We present RackSched, the first rack-level microsecond-scale scheduler that provides the abstraction of a rack-scale computer (i.e., a huge server with hundreds to thousands of cores) to an external service with network-system co-design. The core of RackSched is a two-layer scheduling framework that integrates inter-server scheduling in the top-of-rack (ToR) switch with intra-server scheduling in each server. We use a combination of analytical results and simulations to show that it provides near-optimal performance as centralized scheduling policies, and is robust for both low-dispersion and high-dispersion workloads. We design a custom switch data plane for the inter-server scheduler, which realizes power-of-k-choices, ensures request affinity, and tracks server loads accurately and efficiently. We implement a RackSched prototype on a cluster of commodity servers connected by a Barefoot Tofino switch. End-to-end experiments on a twelve-server testbed show that RackSched improves the throughput by up to 1.44\texttimes{}, and scales out the throughput near linearly, while maintaining the same tail latency as one server until the system is saturated.},
booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation (OSDI)},
articleno = {69},
numpages = {16}
}


@inproceedings{2020-power-socc,
author = {Kaffes, Kostis and Sbirlea, Dragos and Lin, Yiyan and Lo, David and Kozyrakis, Christos},
title = {Leveraging Application Classes to Save Power in Highly-Utilized Data Centers},
year = {2020},
month = aug,
isbn = {9781450381376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3419111.3421274},
abstract = {Data center energy consumption has become an increasingly significant contributor both to greenhouse emissions and costs. To increase utilization of individual hosts and improve efficiency, most modern data centers co-locate workloads belonging to different application classes, some being latency-sensitive (LS) and others best-effort (BE) which are more tolerant to performance variation. It is therefore necessary to design mechanisms that reduce power consumption even in the resulting high-utilization environment, while preserving LS task performance. Moreover, the abundance of different workloads and the security implications of public cloud make mechanisms that rely on extensive knowledge of workload characteristics or on application-exported metrics challenging to deploy.We present PACT, Per Application Class Turbo Controller, a system that leverages two novel mechanisms to reduce power consumption even in highly-utilized data centers. By treating applications like opaque boxes that do not need to provide application-specific performance signals, the first mechanism, Turbo Control, reduces power consumption by decreasing the operating frequency and throttling only BE tasks, without affecting performance-sensitive LS tasks. We identify the shortcomings of Turbo Control and increase its effectiveness by introducing CPUJailing, a mechanism that allocates different sets of cores to LS and BE applications. We deploy PACT (Turbo Control + CPUJailing) in production at Google's data centers and demonstrate that it provides workload-agnostic power savings of 9\% on average together with a 4\% performance improvement for LS tasks across thousands of workloads and nodes.},
booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing (SoCC)},
pages = {134–149},
numpages = {16},
location = {Virtual Event, USA},
series = {SoCC '20}
}


@inproceedings{2020-dbos-polystore,
author = {Cafarella, Michael and DeWitt, David and Gadepally, Vijay and Kepner, Jeremy and Kozyrakis, Christos and Kraska, Tim and Stonebraker, Michael and Zaharia, Matei},
title = {A Polystore Based Database Operating System (DBOS)},
year = {2020},
isbn = {978-3-030-71054-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
doi = {10.1007/978-3-030-71055-2_1},
abstract = {Current operating systems are complex systems that were designed before today’s computing environments. This makes it difficult for them to meet the scalability, heterogeneity, availability, and security challenges in current cloud and parallel computing environments. To address these problems, we propose a radically new OS design based on data-centric architecture: all operating system state should be represented uniformly as database tables, and operations on this state should be made via queries from otherwise stateless tasks. This design makes it easy to scale and evolve the OS without whole-system refactoring, inspect and debug system state, upgrade components without downtime, manage decisions using machine learning, and implement sophisticated security features. We discuss how a database OS (DBOS) can improve the programmability and performance of many of today’s most important applications and propose a plan for the development of a DBOS proof of concept.},
booktitle = {Heterogeneous Data Management, Polystores, and Analytics for Healthcare: VLDB Workshops, Poly 2020 and DMAH 2020, Virtual Event, August 31 and September 4, 2020, Revised Selected Papers},
pages = {3–24},
numpages = {22}
}

@misc{202-dbos-arxiv,
  doi = {10.48550/ARXIV.2007.11112},  
  author = {Cafarella, Michael and DeWitt, David and Gadepally, Vijay and Kepner, Jeremy and Kozyrakis, Christos and Kraska, Tim and Stonebraker, Michael and Zaharia, Matei},
  keywords = {Operating Systems (cs.OS), Hardware Architecture (cs.AR), Databases (cs.DB), Distributed, Parallel, and Cluster Computing (cs.DC), Networking and Internet Architecture (cs.NI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DBOS: A Proposal for a Data-Centric Operating System},
  howpublished = {arXiv},
  year = {2020},
  month = jul,
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{2020asmdb-toppicks,
author={Nagendra, Nayana Prasad and Ayers, Grant and August, David I. and Cho, Hyoun Kyu and Kanev, Svilen and Kozyrakis, Christos and Krishnamurthy, Trivikram and Litz, Heiner and Moseley, Tipp and Ranganathan, Parthasarathy},
journal={IEEE Micro},
title={AsmDB: Understanding and Mitigating Front-End Stalls in Warehouse-Scale Computers},
year={2020},
volume={40},
number={3},
pages={56-63},
abstract={It is well known that the datacenters hosting today's cloud services waste a significant number of cycles on front-end stalls. However, prior work has provided little insights about the source of these front-end stalls and how to address them. This work analyzes the cause of instruction cache misses at a fleet-wide scale and proposes a new compiler-driven software code prefetching strategy to reduce instruction caches misses by 90\%.},
keywords={Prefetching;Optimization;Servers;Hardware;Databases;Complexity theory},
doi={10.1109/MM.2020.2986212},
ISSN={1937-4143},
month=may}



@ARTICLE{2020-hotchips-micro,
author={Kozyrakis, Christos and Bratt, Ian},
journal={IEEE Micro},
title={The Hot Chips Renaissance},
year={2020},
volume={40},
number={2},
pages={6-7},
abstract={The articles in this special section report on the technologies and events that were part of the 31st Annual Hot Chips symposium was held at Stanford University in August 2019.},
keywords={Special issues and sections;Meetings;Machine learning;Computer applications;Wafer scale integration},
doi={10.1109/MM.2020.2977409},
ISSN={1937-4143},
month=mar}


@inproceedings{2020-interstellar-asplos,
author = {Yang, Xuan and Gao, Mingyu and Liu, Qiaoyi and Setter, Jeff and Pu, Jing and Nayak, Ankita and Bell, Steven and Cao, Kaidi and Ha, Heonjae and Raina, Priyanka and Kozyrakis, Christos and Horowitz, Mark},
title = {Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators},
year = {2020},
month = Mar,
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We show that DNN accelerator micro-architectures and their program mappings represent specific choices of loop order and hardware parallelism for computing the seven nested loops of DNNs, which enables us to create a formal taxonomy of all existing dense DNN accelerators. Surprisingly, the loop transformations needed to create these hardware variants can be precisely and concisely represented by Halide's scheduling language. By modifying the Halide compiler to generate hardware, we create a system that can fairly compare these prior accelerators. As long as proper loop blocking schemes are used, and the hardware can support mapping replicated loops, many different hardware dataflows yield similar energy efficiency with good performance. This is because the loop blocking can ensure that most data references stay on-chip with good locality and the processing units have high resource utilization. How resources are allocated, especially in the memory system, has a large impact on energy and performance. By optimizing hardware resource allocation while keeping throughput constant, we achieve up to 4.2X energy improvement for Convolutional Neural Networks (CNNs), 1.6X and 1.8X improvement for Long Short-Term Memories (LSTMs) and multi-layer perceptrons (MLPs), respectively.},
booktitle = {Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {369–383},
numpages = {15}
}


@inproceedings{2020-classify-asplos,
author = {Ayers, Grant and Litz, Heiner and Kozyrakis, Christos and Ranganathan, Parthasarathy},
title = {Classifying Memory Access Patterns for Prefetching},
year = {2020},
month = Mar,
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378498},
abstract = {Prefetching is a well-studied technique for addressing the memory access stall time of contemporary microprocessors. However, despite a large body of related work, the memory access behavior of applications is not well understood, and it remains difficult to predict whether a particular application will benefit from a given prefetcher technique. In this work we propose a novel methodology to classify the memory access patterns of applications, enabling well-informed reasoning about the applicability of a certain prefetcher. Our approach leverages instruction dataflow information to uncover a wide range of access patterns, including arbitrary combinations of offsets and indirection. These combinations or prefetch kernels represent reuse, strides, reference locality, and complex address generation. By determining the complexity and frequency of these access patterns, we enable reasoning about prefetcher timeliness and criticality, exposing the limitations of existing prefetchers today. Moreover, using these kernels, we are able to compute the next address for the majority of top-missing instructions, and we propose a software prefetch injection methodology that is able to outperform state-of-the-art hardware prefetchers.},
booktitle = {Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {513–526},
numpages = {14}
}


@article{2019-gg-login,
  author    = {Sadjad Fouladi and
               Francisco Romero and
               Dan Iter and
               Qian Li and
               Alex Ozdemir and
               Shuvo Chatterjee and
               Matei Zaharia and
               Christos Kozyrakis and
               Keith Winstein},
  title     = {Outsourcing Everyday Jobs to Thousands of Cloud Functions with gg},
  journal   = {login Usenix Mag.},
  volume    = {44},
  number    = {3},
  year      = {2019},
  url       = {https://www.usenix.org/publications/login/fall2019/fouladi},
  timestamp = {Tue, 26 May 2020 14:44:55 +0200},
  biburl    = {https://dblp.org/rec/journals/usenix-login/FouladiRILOCZKW19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2019-functions-socc,
author = {Kaffes, Kostis and Yadwadkar, Neeraja J. and Kozyrakis, Christos},
title = {Centralized Core-Granular Scheduling for Serverless Functions},
year = {2019},
month = Aug,
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3357223.3362709},
abstract = {In recent years, many applications have started using serverless computing platforms primarily due to the ease of deployment and cost efficiency they offer. However, the existing scheduling mechanisms of serverless platforms fall short in catering to the unique characteristics of such applications: burstiness, short and variable execution times, statelessness and use of a single core. Specifically, the existing mechanisms fall short in meeting the requirements generated due to the combined effect of these characteristics: scheduling at a scale of millions of function invocations per second while achieving predictable performance.In this paper, we argue for a cluster-level centralized and core-granular scheduler for serverless functions. By maintaining a global view of the cluster resources, the centralized approach eliminates queue imbalances while the core granularity reduces interference; together these properties enable reduced performance variability. We expect such a scheduler to increase the adoption of serverless computing platforms by various latency and throughput sensitive applications.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing (SoCC)},
pages = {158–164},
numpages = {7},
keywords = {scheduling, cloud computing, serverless computing, resource allocation},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}


@misc{2019-frontier-arxiv,
  doi = {10.48550/ARXIV.1903.07754},
  author = {Grossman, Samuel and Kozyrakis, Christos},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A New Frontier for Pull-Based Graph Processing},
  howpublished = {arXiv},
  year = {2019},
  month = Mar, 
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{2019-gap-hotnets,
author = {Humphries, Jack Tigar and Kaffes, Kostis and Mazi\`{e}res, David and Kozyrakis, Christos},
title = {Mind the Gap: A Case for Informed Request Scheduling at the NIC},
year = {2019},
month = Nov,
isbn = {9781450370202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3365609.3365856},
abstract = {Recent research in high-throughput networked systems has established the need for centralized and preemptive request scheduling in order to achieve good hardware utilization and low tail latency for a wide variety of workloads. However, this approach is expensive to scale as it requires an increasing number of CPU cores dedicated to scheduling. Moreover, passing every request through a scheduling core introduces latency for inter-core communication and reduces the effectiveness of data preloading and caching optimizations.In this paper, we advocate in favor of pushing request-to-core scheduling back into the NIC. Instead of the simple request distribution of receive-side-scaling (RSS) in current NICs, we suggest implementing preemptive request scheduling by passing to the NIC up-to-date information about core availability and execution status of active requests. We present a prototype implementation on a commercial Smart-NIC that indeed shows performance benefits for different workload scenarios. The prototype allows us to also observe bottlenecks that largely come from artifacts of existing NIC hardware. We propose research towards addressing these limitations in order to achieve low overhead, low latency, and highly efficient request scheduling.},
booktitle = {Proceedings of the 18th ACM Workshop on Hot Topics in Networks (HotNets)},
pages = {60–68},
numpages = {9},
location = {Princeton, NJ, USA},
series = {HotNets '19}
}



@article{2019-pocket-login,
  author    = {Ana Klimovic and
               Yawen Wang and
               Patrick Stuedi and
               Animesh Trivedi and
               Jonas Pfefferle and
               Christos Kozyrakis},
  title     = {Pocket: Elastic Ephemeral Storage for Serverless Analytics},
  journal   = {login Usenix Magazine},
  volume    = {44},
  number    = {1},
  year      = {2019},
  url       = {https://www.usenix.org/publications/login/spring2019/klimovic},
  timestamp = {Thu, 14 Oct 2021 09:43:17 +0200},
  biburl    = {https://dblp.org/rec/journals/usenix-login/KlimovicWSTPK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2019-gg-atc,
author = {Fouladi, Sadjad and Romero, Francisco and Iter, Dan and Li, Qian and Chatterjee, Shuvo and Kozyrakis, Christos and Zaharia, Matei and Winstein, Keith},
title = {From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers},
year = {2019},
month = jul,
isbn = {9781939133038},
publisher = {USENIX Association},
address = {USA},
abstract = {We present gg, a framework and a set of command-line tools that helps people execute everyday applications--e.g., software compilation, unit tests, video encoding, or object recognition--using thousands of parallel threads on a cloud-functions service to achieve near-interactive completion times. In the future, instead of running these tasks on a laptop, or keeping a warm cluster running in the cloud, users might push a button that spawns 10,000 parallel cloud functions to execute a large job in a few seconds from start. gg is designed to make this practical and easy.With gg, applications express a job as a composition of lightweight OS containers that are individually transient (lifetimes of 1-60 seconds) and functional (each container is hermetically sealed and deterministic). gg takes care of instantiating these containers on cloud functions, loading dependencies, minimizing data movement, moving data between containers, and dealing with failure and stragglers.We ported several latency-sensitive applications to run on gg and evaluated its performance. In the best case, a distributed compiler built on gg outperformed a conventional tool (icecc) by 2-5\texttimes{}, without requiring a warm cluster running continuously. In the worst case, gg was within 20\% of the hand-tuned performance of an existing tool for video encoding (ExCamera).},
booktitle = {Proceedings of the Usenix Annual Technical Conference (ATC)},
pages = {475–488},
numpages = {14},
location = {Renton, WA, USA},
series = {USENIX ATC '19}
}


@inproceedings{2019-asmdb-isca,
author = {Ayers, Grant and Nagendra, Nayana Prasad and August, David I. and Cho, Hyoun Kyu and Kanev, Svilen and Kozyrakis, Christos and Krishnamurthy, Trivikram and Litz, Heiner and Moseley, Tipp and Ranganathan, Parthasarathy},
title = {AsmDB: Understanding and Mitigating Front-End Stalls in Warehouse-Scale Computers},
year = {2019},
month  = jun,
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3307650.3322234},
abstract = {The large instruction working sets of private and public cloud workloads lead to frequent instruction cache misses and costs in the millions of dollars. While prior work has identified the growing importance of this problem, to date, there has been little analysis of where the misses come from, and what the opportunities are to improve them. To address this challenge, this paper makes three contributions. First, we present the design and deployment of a new, always-on, fleet-wide monitoring system, AsmDB, that tracks front-end bottlenecks. AsmDB uses hardware support to collect bursty execution traces, fleet-wide temporal and spatial sampling, and sophisticated offline post-processing to construct full-program dynamic control-flow graphs. Second, based on a longitudinal analysis of AsmDB data from real-world online services, we present two detailed insights on the sources of front-end stalls: (1) cold code that is brought in along with hot code leads to significant cache fragmentation and a corresponding large number of instruction cache misses; (2) distant branches and calls that are not amenable to traditional cache locality or next-line prefetching strategies account for a large fraction of cache misses. Third, we prototype two optimizations that target these insights. For misses caused by fragmentation, we focus on memcmp, one of the hottest functions contributing to cache misses, and show how fine-grained layout optimizations lead to significant benefits. For misses at the targets of distant jumps, we propose new hardware support for software code prefetching and prototype a new feedback-directed compiler optimization that combines static program flow analysis with dynamic miss profiles to demonstrate significant benefits for several large warehouse-scale workloads. Improving upon prior work, our proposal avoids invasive hardware modifications by prefetching via software in an efficient and scalable way. Simulation results show that such an approach can eliminate up to 96\% of instruction cache misses with negligible overheads.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture (ISCA)},
pages = {462–473},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}



@inproceedings{2019-infaas-hotos,
author = {Yadwadkar, Neeraja J. and Romero, Francisco and Li, Qian and Kozyrakis, Christos},
title = {A Case for Managed and Model-Less Inference Serving},
year = {2019},
month = may,
isbn = {9781450367271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3317550.3321443},
abstract = {The number of applications relying on inference from machine learning models, especially neural networks, is already large and expected to keep growing. For instance, Facebook applications issue tens-of-trillions of inference queries per day with varying performance, accuracy, and cost constraints. Unfortunately, today's inference serving systems are neither easy to use nor cost effective. Developers must manually match the performance, accuracy, and cost constraints of their applications to a large design space that includes decisions such as selecting the right model and model optimizations, selecting the right hardware architecture, selecting the right scale-out factor, and avoiding cold-start effects. These interacting decisions are difficult to make, especially when the application load varies over time, applications evolve over time, and the available resources vary over time.If we want an increasing number of applications to use machine learning, we must automate issues that affect ease-of-use, performance, and cost efficiency for both users and providers. Hence, we define and make the case for managed and model-less inference serving. In this paper, we identify and discuss open research directions to realize this vision.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems (HotOS)},
pages = {184–191},
numpages = {8},
keywords = {Inference Serving, Automatic Resource Management, Model-less},
location = {Bertinoro, Italy},
series = {HotOS '19}
}


@inproceedings{2019-tangram-asplos,
author = {Gao, Mingyu and Yang, Xuan and Pu, Jing and Horowitz, Mark and Kozyrakis, Christos},
title = {TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators},
year = {2019},
month = apr,
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3297858.3304014},
abstract = {The use of increasingly larger and more complex neural networks (NNs) makes it critical to scale the capabilities and efficiency of NN accelerators. Tiled architectures provide an intuitive scaling solution that supports both coarse-grained parallelism in NNs: intra-layer parallelism, where all tiles process a single layer, and inter-layer pipelining, where multiple layers execute across tiles in a pipelined manner. This work proposes dataflow optimizations to address the shortcomings of existing parallel dataflow techniques for tiled NN accelerators. For intra-layer parallelism, we develop buffer sharing dataflow that turns the distributed buffers into an idealized shared buffer, eliminating excessive data duplication and the memory access overheads. For inter-layer pipelining, we develop alternate layer loop ordering that forwards the intermediate data in a more fine-grained and timely manner, reducing the buffer requirements and pipeline delays. We also make inter-layer pipelining applicable to NNs with complex DAG structures. These optimizations improve the performance of tiled NN accelerators by 2x and reduce their energy consumption by 45\% across a wide range of NNs. The effectiveness of our optimizations also increases with the NN size and complexity.},
booktitle = {Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {807–820},
numpages = {14},
keywords = {dataflow, parallelism, neural networks},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}


@inproceedings{2019-shinjuku-nsdi,
author = {Kaffes, Kostis and Chong, Timothy and Humphries, Jack Tigar and Belay, Adam and Mazi\`{e}res, David and Kozyrakis, Christos},
title = {Shinjuku: Preemptive Scheduling for µSecond-Scale Tail Latency},
year = {2019},
month = Feb,
isbn = {9781931971492},
publisher = {USENIX Association},
address = {USA},
abstract = {The recently proposed dataplanes for microsecond scale applications, such as IX and ZygOS, use nonpreemptive policies to schedule requests to cores. For the many real-world scenarios where request service times follow distributions with high dispersion or a heavy tail, they allow short requests to be blocked behind long requests, which leads to poor tail latency.Shinjuku is a single-address space operating system that uses hardware support for virtualization to make preemption practical at the microsecond scale. This allows Shinjuku to implement centralized scheduling policies that preempt requests as often as every 5µsec and work well for both light and heavy tailed request service time distributions. We demonstrate that Shinjuku provides significant tail latency and throughput improvements over IX and ZygOS for a wide range of workload scenarios. For the case of a RocksDB server processing both point and range queries, Shinjuku achieves up to 6.6\texttimes{} higher throughput and 88% lower tail latency.},
booktitle = {Proceedings of the 16th USENIX Conference on Networked Systems Design and Implementation (NSDI)},
pages = {345–359},
numpages = {15},
location = {Boston, MA, USA},
series = {NSDI'19}
}


@InProceedings{2018-prefetch-icml,
  title = 	 {Learning Memory Access Patterns},
  author =       {Hashemi, Milad and Swersky, Kevin and Smith, Jamie and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning  (ICML)},
  pages = 	 {1919--1928},
  year = 	 {2018},
  month = jul,
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 Jul,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/hashemi18a/hashemi18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/hashemi18a.html},
  abstract = 	 {The explosion in workload complexity and the recent slow-down in Moore’s law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations; augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.}
}


@inproceedings{2018-pocket-osdi,
author = {Klimovic, Ana and Wang, Yawen and Stuedi, Patrick and Trivedi, Animesh and Pfefferle, Jonas and Kozyrakis, Christos},
title = {Pocket: Elastic Ephemeral Storage for Serverless Analytics},
year = {2018},
month = oct,
isbn = {9781931971478},
publisher = {USENIX Association},
address = {USA},
abstract = {Serverless computing is becoming increasingly popular, enabling users to quickly launch thousands of short-lived tasks in the cloud with high elasticity and fine-grain billing. These properties make serverless computing appealing for interactive data analytics. However exchanging intermediate data between execution stages in an analytics job is a key challenge as direct communication between serverless tasks is difficult. The natural approach is to store such ephemeral data in a remote data store. However, existing storage systems are not designed to meet the demands of serverless applications in terms of elasticity, performance, and cost. We present Pocket, an elastic, distributed data store that automatically scales to provide applications with desired performance at low cost. Pocket dynamically rightsizes resources across multiple dimensions (CPU cores, network bandwidth, storage capacity) and leverages multiple storage technologies to minimize cost while ensuring applications are not bottlenecked on I/O. We show that Pocket achieves similar performance to ElastiCache Redis for serverless analytics applications while reducing cost by almost 60%.},
booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation (OSDI)},
pages = {427–444},
numpages = {18},
location = {Carlsbad, CA, USA},
series = {OSDI'18},
month = oct
}

@article{2018-quman-taco,
author = {Sfakianakis, Yannis and Kozanitis, Christos and Kozyrakis, Christos and Bilas, Angelos},
title = {<i>QuMan</i>: Profile-Based Improvement of Cluster Utilization},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1544-3566},
doi = {10.1145/3210560},
abstract = {Modern data centers consolidate workloads to increase server utilization and reduce total cost of ownership, and cope with scaling limitations. However, server resource sharing introduces performance interference across applications and, consequently, increases performance volatility, which negatively affects user experience. Thus, a challenging problem is to increase server utilization while maintaining application QoS.In this article, we present QuMan, a server resource manager that uses application isolation and profiling to increase server utilization while controlling degradation of application QoS. Previous solutions, either estimate interference across applications and then restrict colocation to “compatible” applications, or assume that application requirements are known. Instead, QuMan estimates the required resources of applications. It uses an isolation mechanism to create properly-sized resource slices for applications, and arbitrarily colocates applications. QuMan’s mechanisms can be used with a variety of admission control policies, and we explore the potential of two such policies: (1) A policy that allows users to specify a minimum performance threshold and (2) an automated policy, which operates without user input and is based on a new combined QoS-utilization metric. We implement QuMan on top of Linux servers, and we evaluate its effectiveness using containers and real applications. Our single-node results show that QuMan balances highly effectively the tradeoff between server utilization and application performance, as it achieves 80\% server utilization while the performance of each application does not drop below 80\% the respective standalone performance. We also deploy QuMan on a cluster of 100 AWS instances that are managed by a modified version of the Sparrow scheduler [37] and, we observe a 48\% increase in application performance on a highly utilized cluster, compared to the performance of the same cluster under the same load when it is managed by native Sparrow or Apache Mesos.},
journal = {ACM Transactions on Architecture and Code Optimization},
month = aug,
articleno = {27},
numpages = {25},
keywords = {Resource management, slices, I/O caching, isolation, input/output, solid state disk}
}


@article{2018-latency-cacm,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Amdahl's Law for Tail Latency},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {8},
issn = {0001-0782},
doi = {10.1145/3232559},
abstract = {Queueing theoretic models can guide design trade-offs in systems targeting tail latency, not just average performance.},
journal = {Communications of ACM},
month = jul,
pages = {65–72},
numpages = {8}
}


@ARTICLE{2018-bolt-toppicks,
author={Delimitrou, Christina and Kozyrakis, Christos},
journal={IEEE Micro},
title={Uncovering the Security Implications of Cloud Multi-Tenancy with Bolt},
year={2018},
volume={38},
number={3},
pages={86-97},
abstract={Cloud providers routinely schedule multiple applications per physical host to increase efficiency. The resulting interference on shared resources often leads to performance degradation and, more importantly, security vulnerabilities. Interference can leak important information about an application, ranging from a services placement to confidential data, such as private keys. We present Bolt, a practical system that accurately detects the type and characteristics of applications sharing a cloud platform based on the interference an adversary sees on shared resources. Bolt leverages online data mining techniques that only require 2-5 seconds for detection. In a multi-user study on Amazon Elastic Compute Cloud (EC2), Bolt correctly identifies the characteristics of 385 out of a set of 436 diverse workloads. Extracting this information enables a wide spectrum of previously impractical cloud attacks, including denial of service (DoS) attacks that increase tail latency by 140X, as well as resource freeing attacks (RFAs), and co-residency attacks. Finally, we show that, while advanced isolation mechanisms such as cache partitioning lower detection accuracy, they are insufficient to eliminate these vulnerabilities altogether. To do so, one must either disallow core sharing or allow it only between threads of the same application, leading to significant inefficiencies and performance penalties.},
keywords={Computer security;Cloud computing;Data mining;Interference;cloud computing;security;quality of service;isolation;partitioning;denial of service;hardware},
doi={10.1109/MM.2018.032271065},
ISSN={1937-4143},
month={May},}


@ARTICLE{2018-plasticine-toppicks,
author={Prabhakar, Raghu and Zhang, Yaqi and Koeplinger, David and Feldman, Matt and Zhao, Tian and Hadjis, Stefan and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
journal={IEEE Micro},
title={Plasticine: A Reconfigurable Accelerator for Parallel Patterns},
year={2018},
volume={38},
number={3},
pages={20-31},
abstract={Plasticine is a new spatially reconfigurable architecture designed to efficiently execute applications composed of high-level parallel patterns. With an area footprint of 113 mm2 in a 28-nm process and a 1-GHz clock, Plasticine has a peak floating-point performance of 12.3 single-precision Tflops and a total on-chip memory capacity of 16 MB, consuming a maximum power of 49 W. Plasticine provides an improvement of up to 76.9X in performance-per-watt over a conventional FPGA over a wide range of dense and sparse applications.},
keywords={Phasor measurement units;Random access memory;Computer architecture;Reconfigurable architecture;System-on-chip;Field programmable gate arrays;hardware accelerators;dataflow architectures;FPGA;coarse-grained reconfigurable architectures;parallel patterns;reconfigurable architectures;hardware},
doi={10.1109/MM.2018.032271058},
ISSN={1937-4143},
month={May},}


@inproceedings{2018-ephemeral-atc,
author = {Klimovic, Ana and Wang, Yawen and Kozyrakis, Christos and Stuedi, Patrick and Pfefferle, Jonas and Trivedi, Animesh},
title = {Understanding Ephemeral Storage for Serverless Analytics},
year = {2018},
month = jul,
isbn = {9781931971447},
publisher = {USENIX Association},
address = {USA},
abstract = {Serverless computing frameworks allow users to launch thousands of concurrent tasks with high elasticity and fine-grain resource billing without explicitly managing computing resources. While already successful for IoT and web microservices, there is increasing interest in leveraging serverless computing to run data-intensive jobs, such as interactive analytics. A key challenge in running analytics workloads on serverless platforms is enabling tasks in different execution stages to efficiently communicate data between each other via a shared data store. In this paper, we explore the suitability of different cloud storage services (e.g., object stores and distributed caches) as remote storage for serverless analytics. Our analysis leads to key insights to guide the design of an ephemeral cloud storage system, including the performance and cost efficiency of Flash storage for serverless application requirements and the need for a pay-what-you-use storage service that can support the high throughput demands of highly parallel applications.},
booktitle = {Proceedings of the Usenix Annual Technical Conference (ATC)},
pages = {789–794},
numpages = {6},
location = {Boston, MA, USA},
series = {USENIX ATC '18}
}

@inproceedings{2018-selecta-atc,
author = {Klimovic, Ana and Litz, Heiner and Kozyrakis, Christos},
title = {Selecta: Heterogeneous Cloud Storage Configuration for Data Analytics},
year = {2018},
month = jul,
isbn = {9781931971447},
publisher = {USENIX Association},
address = {USA},
abstract = {Data analytics are an important class of data-intensive workloads on public cloud services. However, selecting the right compute and storage configuration for these applications is difficult as the space of available options is large and the interactions between options are complex. Moreover, the different data streams accessed by analytics workloads have distinct characteristics that may be better served by different types of storage devices.We present Selecta, a tool that recommends near-optimal configurations of cloud compute and storage resources for data analytics workloads. Selecta uses latent factor collaborative filtering to predict how an application will perform across different configurations, based on sparse data collected by profiling training workloads. We evaluate Selecta with over one hundred Spark SQL and ML applications, showing that Selecta chooses a near-optimal performance configuration (within 10\% of optimal) with 94\% probability and a near-optimal cost configuration with 80\% probability. We also use Selecta to draw significant insights about cloud storage systems, including the performance-cost efficiency of NVMe Flash devices, the need for cloud storage with support for fine-grain capacity and bandwidth allocation, and the motivation for end-to-end storage optimizations.},
booktitle = {Proceedings of the Usenix Annual Technical Conference (ATC)},
pages = {759–773},
numpages = {15},
location = {Boston, MA, USA},
series = {USENIX ATC '18}
}


@inproceedings{2018-spatial-pldi,
author = {Koeplinger, David and Feldman, Matthew and Prabhakar, Raghu and Zhang, Yaqi and Hadjis, Stefan and Fiszel, Ruben and Zhao, Tian and Nardi, Luigi and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Spatial: A Language and Compiler for Application Accelerators},
year = {2018},
month = jun,
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3192366.3192379},
abstract = {Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs for improved performance and energy efficiency. Unfortunately, adoption of these architectures has been limited by their programming models. HDLs lack abstractions for productivity and are difficult to target from higher level languages. HLS tools are more productive, but offer an ad-hoc mix of software and hardware abstractions which make performance optimizations difficult.  In this work, we describe a new domain-specific language and compiler called Spatial for higher level descriptions of application accelerators. We describe Spatial's hardware-centric abstractions for both programmer productivity and design performance, and summarize the compiler passes required to support these abstractions, including pipeline scheduling, automatic memory banking, and automated design tuning driven by active machine learning. We demonstrate the language's ability to target FPGAs and CGRAs from common source code. We show that applications written in Spatial are, on average, 42\% shorter and achieve a mean speedup of 2.9x over SDAccel HLS when targeting a Xilinx UltraScale+ VU9P FPGA on an Amazon EC2 F1 instance.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
pages = {296–311},
numpages = {16},
keywords = {compilers, reconfigurable architectures, CGRAs, hardware accelerators, high-level synthesis, domain-specific languages, FPGAs},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}


@misc{2018-prefetch-arxiv,
  doi = {10.48550/ARXIV.1803.02329},
  author = {Hashemi, Milad and Swersky, Kevin and Smith, Jamie A. and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning Memory Access Patterns},
  howpublished = {arXiv},  
  year = {2018},
      month = mar,
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@INPROCEEDINGS{2018-graphp-hpca,
author={Zhang, Mingxing and Zhuo, Youwei and Wang, Chao and Gao, Mingyu and Wu, Yongwei and Chen, Kang and Kozyrakis, Christos and Qian, Xuehai},
booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
title={GraphP: Reducing Communication for PIM-Based Graph Processing with Efficient Data Partition},
year={2018},
volume={},
number={},
pages={544-557},
abstract={Processing-In-Memory (PIM) is an effective technique that reduces data movements by integrating processing units within memory. The recent advance of “big data” and 3D stacking technology make PIM a practical and viable solution for the modern data processing workloads. It is exemplified by the recent research interests on PIM-based acceleration. Among them, TESSERACT is a PIM-enabled parallel graph processing architecture based on Micron's Hybrid Memory Cube (HMC), one of the most prominent 3D-stacked memory technologies. It implements a Pregel-like vertex-centric programming model, so that users could develop programs in the familiar interface while taking advantage of PIM. Despite the orders of magnitude speedup compared to DRAM-based systems, TESSERACT generates excessive crosscube communications through SerDes links, whose bandwidth is much less than the aggregated local bandwidth of HMCs. Our investigation indicates that this is because of the restricted data organization required by the vertex programming model. In this paper, we argue that a PIM-based graph processing system should take data organization as a first-order design consideration. Following this principle, we propose GraphP, a novel HMC-based software/hardware co-designed graph processing system that drastically reduces communication and energy consumption compared to TESSERACT. GraphP features three key techniques. 1) “Source-cut” partitioning, which fundamentally changes the cross-cube communication from one remote put per cross-cube edge to one update per replica. 2) “Two-phase Vertex Program”, a programming model designed for the “source-cut” partitioning with two operations: GenUpdate and ApplyUpdate. 3) Hierarchical communication and overlapping, which further improves performance with unique opportunities offered by the proposed partitioning and programming model. We evaluate GraphP using a cycle accurate simulator with 5 real-world graphs and 4 algorithms. The results show that it provides on average 1.7 speedup and 89% energy saving compared to TESSERACT.},
keywords={Bandwidth;Programming;Organizations;Three-dimensional displays;Partitioning algorithms;Memory management;Graph processing;Processing In Memory;Hybrid Memory Cube},
doi={10.1109/HPCA.2018.00053},
ISSN={2378-203X},
month=Feb,}

@INPROCEEDINGS{2018-search-hpca,
author={Ayers, Grant and Ahn, Jung Ho and Kozyrakis, Christos and Ranganathan, Parthasarathy},
booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
title={Memory Hierarchy for Web Search},
year={2018},
volume={},
number={},
pages={643-656},
abstract={Online data-intensive services, such as search, serve billions of users, utilize millions of cores, and comprise a significant and growing portion of datacenter-scale workloads. However, the complexity of these workloads and their proprietary nature has precluded detailed architectural evaluations and optimizations of processor design trade-offs. We present the first detailed study of the memory hierarchy for the largest commercial search engine today. We use a combination of measurements from longitudinal studies across tens of thousands of deployed servers, systematic microarchitectural evaluation on individual platforms, validated trace-driven simulation, and performance modeling – all driven by production workloads servicing real-world user requests. Our data quantifies significant differences between production search and benchmarks commonly used in the architecture community. We identify the memory hierarchy as an important opportunity for performance optimization, and present new insights pertaining to how search stresses the cache hierarchy, both for instructions and data. We show that, contrary to conventional wisdom, there is significant reuse of data that is not captured by current cache hierarchies, and discuss why this precludes state-of-the-art tiled and scale-out architectures. Based on these insights, we rethink a new cache hierarchy optimized for search that trades off the inefficient use of L3 cache transistors for higher-performance cores, and adds a latency-optimized on-package eDRAM L4 cache. Compared to state-of-the-art processors, our proposed design performs 27% to 38% better.},
keywords={Production;Servers;Indexes;Microarchitecture;Web search;Benchmark testing;Hardware;web search;warehouse scale computing;memory systems},
doi={10.1109/HPCA.2018.00061},
ISSN={2378-203X},
month=Feb}

@inproceedings{2018-pull-ppopp,
author = {Grossman, Samuel and Litz, Heiner and Kozyrakis, Christos},
title = {Making Pull-Based Graph Processing Performant},
year = {2018},
month = feb,
isbn = {9781450349826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3178487.3178506},
abstract = {Graph processing engines following either the push-based or pull-based pattern conceptually consist of a two-level nested loop structure. Parallelizing and vectorizing these loops is critical for high overall performance and memory bandwidth utilization. Outer loop parallelization is simple for both engine types but suffers from high load imbalance. This work focuses on inner loop parallelization for pull engines, which when performed naively leads to a significant increase in conflicting memory writes that must be synchronized.Our first contribution is a scheduler-aware interface for parallel loops that allows us to optimize for the common case in which each thread executes several consecutive iterations. This eliminates most write traffic and avoids all synchronization, leading to speedups of up to 50X.Our second contribution is the Vector-Sparse format, which addresses the obstacles to vectorization that stem from the commonly-used Compressed-Sparse data structure. Our new format eliminates unaligned memory accesses and bounds checks within vector operations, two common problems when processing low-degree vertices. Vectorization with Vector-Sparse leads to speedups of up to 2.5X.Our contributions are embodied in Grazelle, a hybrid graph processing framework. On a server equipped with four Intel Xeon E7-4850 v3 processors, Grazelle respectively outperforms Ligra, Polymer, GraphMat, and X-Stream by up to 15.2X, 4.6X, 4.7X, and 66.8X.},
booktitle = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)},
pages = {246–260},
numpages = {15},
location = {Vienna, Austria},
series = {PPoPP '18}
}


@article{2018-ix-tocs,
author = {Belay, Adam and Prekas, George and Primorac, Mia and Klimovic, Ana and Grossman, Samuel and Kozyrakis, Christos and Bugnion, Edouard},
title = {Corrigendum to “The IX Operating System: Combining Low Latency, High Throughput and Efficiency in a Protected Dataplane”},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {0734-2071},
doi = {10.1145/3154292},
journal = {ACM Transactions Computer Systems},
month = dec,
articleno = {10},
numpages = {1}
}


@misc{2018-trevor-arxiv,
  doi = {10.48550/ARXIV.1812.09442},
  author = {Bansal, Manu and Cidon, Eyal and Balasingam, Arjun and Gudipati, Aditya and Kozyrakis, Christos and Katti, Sachin},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Trevor: Automatic configuration and scaling of stream processing pipelines},
  howpublished = {arXiv},
  year = {2018},
  month = Dec,
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{2017-appswitch-arxiv,
  doi = {10.48550/ARXIV.1711.02294},
  author = {Subhraveti, Dinesh and Goli, Sri and Hallyn, Serge and Chamarthy, Ravi and Kozyrakis, Christos},
  keywords = {Networking and Internet Architecture (cs.NI), Operating Systems (cs.OS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {AppSwitch: Resolving the Application Identity Crisis},
  howpublished = {arXiv},
  year = {2017},
  month = Nov,
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{2017-draf-toppicks,
author = {Gao, Mingyu and Delimitrou, Christina and Niu, Dimin and Malladi, Krishna T. and Zheng, Hongzhong and Brennan, Bob and Kozyrakis, Christos},
title = {DRAF: A Low-Power DRAM-Based Reconfigurable Acceleration Fabric},
year = {2017},
issue_date = {2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {37},
number = {3},
issn = {0272-1732},
doi = {10.1109/MM.2017.50},
abstract = {The DRAM-Based Reconfigurable Acceleration Fabric (DRAF) uses commodity DRAM technology to implement a bit-level, reconfigurable fabric that improves area density by 10 times and power consumption by more than 3 times over conventional field-programmable gate arrays. Latency overlapping and multicontext support allow DRAF to meet the performance and density requirements of demanding applications in datacenter and mobile environments.},
journal = {IEEE Micro},
month = jan,
pages = {70–78},
numpages = {9}
}

@inproceedings{2017-n3xt-codes,
author = {Hwang, William and Aly, Mohamed M. Sabry and Malviya, Yash H. and Gao, Mingyu and Wu, Tony F. and Kozyrakis, Christos and Wong, H.-S. Philip and Mitra, Subhasish},
title = {3D Nanosystems Enable <i>Embedded</i> Abundant-Data Computing: Special Session Paper},
year = {2017},
isbn = {9781450351850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3125502.3125531},
abstract = {The world's appetite for abundant-data computing, where a massive amount of structured and unstructured data is analyzed, has increased dramatically. The computational demands of these applications, such as deep learning, far exceed the capabilities of today's systems, especially for energy-constrained embedded systems (e.g., mobile systems with limited battery capacity). These demands are unlikely to be met by isolated improvements in transistor or memory technologies, or integrated circuit (IC) architectures alone. Transformative nanosystems, which leverage the unique properties of emerging nanotechnologies to create new IC architectures, are required to deliver unprecedented functionality, performance, and energy efficiency. We show that the projected energy efficiency benefits of domain-specific 3D nanosystems is in the range of 1,000x (quantified using the product of system-level energy consumption and execution time) over today's domain-specific 2D systems with off-chip DRAM. Such a drastic improvement is key to enabling new capabilities such as deep learning in embedded systems.},
booktitle = {Proceedings of the 12th IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis Companion (CODES+ISSS)},
articleno = {29},
numpages = {2},
location = {Seoul, Republic of Korea},
series = {CODES '17}
}

@inproceedings{2017-persona-atc,
author = {Byma, Stuart and Whitlock, Sam and Flueratoru, Laura and Tseng, Ethan and Kozyrakis, Christos and Bugnion, Edouard and Larus, James},
title = {Persona: A High-Performance Bioinformatics Framework},
year = {2017},
month = jul,
isbn = {9781931971386},
publisher = {USENIX Association},
address = {USA},
abstract = {Next-generation genome sequencing technology has reached a point at which it is becoming cost-effective to sequence all patients. Biobanks and researchers are faced with an oncoming deluge of genomic data, whose processing requires new and scalable bioinformatics architectures and systems. Processing raw genetic sequence data is computationally expensive and datasets are large. Current software systems can require many hours to process a single genome and generally run only on a single computer. Common file formats are monolithic and row-oriented, a barrier to distributed computation.To address these challenges, we built Persona, a cluster-scale, high-throughput bioinformatics framework. Persona currently supports paired-read alignment, sorting, and duplicate marking using well-known algorithms and techniques. Persona can significantly reduce end-to-end processing times for bioinformatics computations. A new Aggregate Genomic Data (AGD) format unifies sample data and analysis results, while enabling efficient distributed computation and I/O.In a case study on sequence alignment, Persona sustains 1.353 gigabases aligned per second with 101 base pair reads on a 32-node cluster and can align a full genome in ∼16.7 seconds using the SNAP algorithm. Our results demonstrate that: (1) alignment computation with Persona scales linearly across servers with no measurable completion-time imbalance and negligible framework overheads; (2) on a single server, sorting with Persona and AGD is up to 2.3\texttimes{} faster than commonly used tools, while duplicate marking is 3\texttimes{} faster; (3) with AGD, a 7 node COTS network storage system can service up to 60 alignment compute nodes; (4) server cost dominates for a balanced system running Persona, while long-term data storage dwarfs the cost of computation.},
booktitle = {Proceedings of the Usenix Annual Technical Conference (ATC)},
pages = {153–165},
numpages = {13},
location = {Santa Clara, CA, USA},
series = {USENIX ATC '17}
}

@inproceedings{2017-plasticine-isca,
author = {Prabhakar, Raghu and Zhang, Yaqi and Koeplinger, David and Feldman, Matt and Zhao, Tian and Hadjis, Stefan and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Plasticine: A Reconfigurable Architecture For Parallel Paterns},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3079856.3080256},
abstract = {Reconfigurable architectures have gained popularity in recent years as they allow the design of energy-efficient accelerators. Fine-grain fabrics (e.g. FPGAs) have traditionally suffered from performance and power inefficiencies due to bit-level reconfigurable abstractions. Both fine-grain and coarse-grain architectures (e.g. CGRAs) traditionally require low level programming and suffer from long compilation times. We address both challenges with Plasticine, a new spatially reconfigurable architecture designed to efficiently execute applications composed of parallel patterns. Parallel patterns have emerged from recent research on parallel programming as powerful, high-level abstractions that can elegantly capture data locality, memory access patterns, and parallelism across a wide range of dense and sparse applications.We motivate Plasticine by first observing key application characteristics captured by parallel patterns that are amenable to hardware acceleration, such as hierarchical parallelism, data locality, memory access patterns, and control flow. Based on these observations, we architect Plasticine as a collection of Pattern Compute Units and Pattern Memory Units. Pattern Compute Units are multi-stage pipelines of reconfigurable SIMD functional units that can efficiently execute nested patterns. Data locality is exploited in Pattern Memory Units using banked scratchpad memories and configurable address decoders. Multiple on-chip address generators and scatter-gather engines make efficient use of DRAM bandwidth by supporting a large number of outstanding memory requests, memory coalescing, and burst mode for dense accesses. Plasticine has an area footprint of 113 mm2 in a 28nm process, and consumes a maximum power of 49 W at a 1 GHz clock. Using a cycle-accurate simulator, we demonstrate that Plasticine provides an improvement of up to 76.9x in performance-per-Watt over a conventional FPGA over a wide range of dense and sparse applications.},
booktitle = {Proceedings of the 44th International Symposium on Computer Architecture (ISCA)},
pages = {389–402},
numpages = {14},
keywords = {CGRAs, reconfigurable architectures, parallel patterns, hardware accelerators},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}


@inproceedings{2017-reflex-asplos,
author = {Klimovic, Ana and Litz, Heiner and Kozyrakis, Christos},
title = {ReFlex: Remote Flash ≈ Local Flash},
year = {2017},
month = apr,
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3037697.3037732},
abstract = {Remote access to NVMe Flash enables flexible scaling and high utilization of Flash capacity and IOPS within a datacenter. However, existing systems for remote Flash access either introduce significant performance overheads or fail to isolate the multiple remote clients sharing each Flash device. We present ReFlex, a software-based system for remote Flash access, that provides nearly identical performance to accessing local Flash. ReFlex uses a dataplane kernel to closely integrate networking and storage processing to achieve low latency and high throughput at low resource requirements. Specifically, ReFlex can serve up to 850K IOPS per core over TCP/IP networking, while adding 21us over direct access to local Flash. ReFlex uses a QoS scheduler that can enforce tail latency and throughput service-level objectives (SLOs) for thousands of remote clients. We show that ReFlex allows applications to use remote Flash while maintaining their original performance with local Flash.},
booktitle = {Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {345–359},
numpages = {15},
keywords = {flash, i/o scheduling, network storage, qos, datacenter storage},
location = {Xi'an, China},
series = {ASPLOS '17}
}



@inproceedings{2017-tetris-asplos,
author = {Gao, Mingyu and Pu, Jing and Yang, Xuan and Horowitz, Mark and Kozyrakis, Christos},
title = {TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory},
year = {2017},
month = apr,
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3037697.3037702},
abstract = {The high accuracy of deep neural networks (NNs) has led to the development of NN accelerators that improve performance by two orders of magnitude. However, scaling these accelerators for higher performance with increasingly larger NNs exacerbates the cost and energy overheads of their memory systems, including the on-chip SRAM buffers and the off-chip DRAM channels.This paper presents the hardware architecture and software scheduling and partitioning techniques for TETRIS, a scalable NN accelerator using 3D memory. First, we show that the high throughput and low energy characteristics of 3D memory allow us to rebalance the NN accelerator design, using more area for processing elements and less area for SRAM buffers. Second, we move portions of the NN computations close to the DRAM banks to decrease bandwidth pressure and increase performance and energy efficiency. Third, we show that despite the use of small SRAM buffers, the presence of 3D memory simplifies dataflow scheduling for NN computations. We present an analytical scheduling scheme that matches the efficiency of schedules derived through exhaustive search. Finally, we develop a hybrid partitioning scheme that parallelizes the NN computations over multiple accelerators. Overall, we show that TETRIS improves mthe performance by 4.1x and reduces the energy by 1.5x over NN accelerators with conventional, low-power DRAM memory systems.},
booktitle = {Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {751–764},
numpages = {14},
keywords = {neural networks, partitioning, 3d memory, dataflow scheduling, acceleration},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@inproceedings{2017-bolt-asplos,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Bolt: I Know What You Did Last Summer... In The Cloud},
year = {2017},
month = apr,
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3037697.3037703},
abstract = {Cloud providers routinely schedule multiple applications per physical host to increase efficiency. The resulting interference on shared resources often leads to performance degradation and, more importantly, security vulnerabilities. Interference can leak important information ranging from a service's placement to confidential data, like private keys. We present Bolt, a practical system that accurately detects the type and characteristics of applications sharing a cloud platform based on the interference an adversary sees on shared resources. Bolt leverages online data mining techniques that only require 2-5 seconds for detection. In a multi-user study on EC2, Bolt correctly identifies the characteristics of 385 out of 436 diverse workloads. Extracting this information enables a wide spectrum of previously-impractical cloud attacks, including denial of service attacks (DoS) that increase tail latency by 140x, as well as resource freeing (RFA) and co-residency attacks. Finally, we show that while advanced isolation mechanisms, such as cache partitioning lower detection accuracy, they are insufficient to eliminate these vulnerabilities altogether. To do so, one must either disallow core sharing, or only allow it between threads of the same application, leading to significant inefficiencies and performance penalties.},
booktitle = {Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {599–613},
numpages = {15},
keywords = {data mining, interference, cloud computing, security, datacenter, isolation, denial of service attack, latency},
location = {Xi'an, China},
series = {ASPLOS '17}
}


@article{2016-ix-tocs,
author = {Belay, Adam and Prekas, George and Primorac, Mia and Klimovic, Ana and Grossman, Samuel and Kozyrakis, Christos and Bugnion, Edouard},
title = {The IX Operating System: Combining Low Latency, High Throughput, and Efficiency in a Protected Dataplane},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {0734-2071},
doi = {10.1145/2997641},
abstract = {The conventional wisdom is that aggressive networking requirements, such as high packet rates for small messages and μs-scale tail latency, are best addressed outside the kernel, in a user-level networking stack. We present ix, a dataplane operating system that provides high I/O performance and high resource efficiency while maintaining the protection and isolation benefits of existing kernels.ix uses hardware virtualization to separate management and scheduling functions of the kernel (control plane) from network processing (dataplane). The dataplane architecture builds upon a native, zero-copy API and optimizes for both bandwidth and latency by dedicating hardware threads and networking queues to dataplane instances, processing bounded batches of packets to completion, and eliminating coherence traffic and multicore synchronization. The control plane dynamically adjusts core allocations and voltage/frequency settings to meet service-level objectives.We demonstrate that ix outperforms Linux and a user-space network stack significantly in both throughput and end-to-end latency. Moreover, ix improves the throughput of a widely deployed, key-value store by up to 6.4\texttimes{} and reduces tail latency by more than 2\texttimes{} . With three varying load patterns, the control plane saves 46\%--54\% of processor energy, and it allows background jobs to run at 35\%--47\% of their standalone throughput.},
journal = {ACM Transactions on Computer Systems},
month = dec,
articleno = {11},
numpages = {39},
keywords = {Virtualization, latency-critical applications, workload consolidation, dataplane operating systems, microsecond-scale computing, energy-proportionality}
}


@article{2016-sec-cal,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Security Implications of Data Mining in Cloud Scheduling},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Computer Society},
address = {USA},
volume = {15},
number = {2},
issn = {1556-6056},
doi = {10.1109/LCA.2015.2461215},
abstract = {Cloud providers host an increasing number of popular applications, on the premise of resource flexibility  and cost efficiency. Most of these systems expose virtualized resources of different types and sizes. As instances share the same physical host to increase utilization, they contend on hardware resources, e.g., last-level cache, making them vulnerable to side-channel attacks from co-scheduled applications. In this work we show that using data mining techniques can help an adversarial user of the cloud determine the nature and characteristics of co-scheduled applications and negatively impact their performance through targeted contention injections. We design Bolt, a simple runtime that extracts the sensitivity of co-scheduled applications to various types of interference and uses this signal to determine the type of these applications by applying a set of data mining techniques. We validate the accuracy of Bolt on a 39-server cluster. Bolt correctly identifies the type and characteristics of 81 percent out of 108 victim applications, and constructs specialized contention signals that degrade their performance. We also use Bolt to find the most commonly-run applications on EC2. We hope that underlining such security vulnerabilities in modern cloud facilities will encourage cloud providers to introduce stronger resource isolation primitives in their systems.},
journal = {IEEE Computer Architecture Letters},
month = jul,
pages = {109–112},
numpages = {4}
}


@inproceedings{2016-draf-isca,
author = {Gao, Mingyu and Delimitrou, Christina and Niu, Dimin and Malladi, Krishna T. and Zheng, Hongzhong and Brennan, Bob and Kozyrakis, Christos},
title = {DRAF: A Low-Power DRAM-Based Reconfigurable Acceleration Fabric},
year = {2016},
month = jun,
isbn = {9781467389471},
publisher = {IEEE Press},
doi = {10.1109/ISCA.2016.51},
abstract = {FPGAs are a popular target for application-specific accelerators because they lead to a good balance between flexibility and energy efficiency. However, FPGA lookup tables introduce significant area and power overheads, making it difficult to use FPGA devices in environments with tight cost and power constraints. This is the case for datacenter servers, where a modestly-sized FPGA cannot accommodate the large number of diverse accelerators that datacenter applications need.This paper introduces DRAF, an architecture for bit-level reconfigurable logic that uses DRAM subarrays to implement dense lookup tables. DRAF overlaps DRAM operations like bitline precharge and charge restoration with routing within the reconfigurable routing fabric to minimize the impact of DRAM latency. It also supports multiple configuration contexts that can be used to quickly switch between different accelerators with minimal latency. Overall, DRAF trades off some of the performance of FPGAs for significant gains in area and power. DRAF improves area density by 10x over FPGAs and power consumption by more than 3x, enabling DRAF to satisfy demanding applications within strict power and cost constraints. While accelerators mapped to DRAF are 2-3x slower than those in FPGAs, they still deliver a 13x speedup and an 11x reduction in power consumption over a Xeon core for a wide range of datacenter tasks, including analytics and interactive services like speech recognition.},
booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture (ISCA)},
pages = {506–518},
numpages = {13},
keywords = {FPGA, low-power, DRAM, reconfigurable logic},
location = {Seoul, Republic of Korea},
series = {ISCA '16}
}


@inproceedings{2016-generate-isca,
author={Koeplinger, David and Prabhakar, Raghu and Zhang, Yaqi and Delimitrou, Christina and Kozyrakis, Christos and Olukotun, Kunle},
title = {Automatic Generation of Efficient Accelerators for Reconfigurable Hardware},
year = {2016},
month = jun,
isbn = {9781467389471},
publisher = {IEEE Press},
doi = {10.1109/ISCA.2016.20},
abstract = {Acceleration in the form of customized datapaths offer large performance and energy improvements over general purpose processors. Reconfigurable fabrics such as FPGAs are gaining popularity for use in implementing application-specific accelerators, thereby increasing the importance of having good high-level FPGA design tools. However, current tools for targeting FPGAs offer inadequate support for high-level programming, resource estimation, and rapid and automatic design space exploration.We describe a design framework that addresses these challenges. We introduce a new representation of hardware using parameterized templates that captures locality and parallelism information at multiple levels of nesting. This representation is designed to be automatically generated from high-level languages based on parallel patterns. We describe a hybrid area estimation technique which uses template-level models and design-level artificial neural networks to account for effects from hardware place-and-route tools, including routing overheads, register and block RAM duplication, and LUT packing. Our runtime estimation accounts for off-chip memory accesses. We use our estimation capabilities to rapidly explore a large space of designs across tile sizes, parallelization factors, and optional coarse-grained pipelining, all at multiple loop levels. We show that estimates average 4.8\% error for logic resources, 6.1\% error for runtimes, and are 279 to 6533 times faster than a commercial high-level synthesis tool. We compare the best-performing designs to optimized CPU code running on a server-grade 6 core processor and show speedups of up to 16.7\texttimes{}.},
booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture (ISCA)},
pages = {115–127},
numpages = {13},
location = {Seoul, Republic of Korea},
series = {ISCA '16}
}


@TechReport{2016-tr-ix,
  author = 	 {George Prekas, Adam Belay, Mia Primorac, Ana Klimovic, Sam Grossman, Marios Kogias, Bernard Gütermann, Christos Kozyrakis, Edouard Bugnion},
  title = 	 {IX Open-source v1.0 – Deployment and Evaluation Guide},
  institution =  {EPFL},
  year = 	 2016,
  number = 	 218568,
  month = 	 May}

@INPROCEEDINGS{2016-hrl-hpca,
author={Gao, Mingyu and Kozyrakis, Christos},
booktitle={2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
title={HRL: Efficient and flexible reconfigurable logic for near-data processing},
year={2016},
month = Mar,
volume={},
number={},
pages={126-137},
abstract={The energy constraints due to the end of Dennard scaling, the popularity of in-memory analytics, and the advances in 3D integration technology have led to renewed interest in near-data processing (NDP) architectures that move processing closer to main memory. Due to the limited power and area budgets of the logic layer, the NDP compute units should be area and energy efficient while providing sufficient compute capability to match the high bandwidth of vertical memory channels. They should also be flexible to accommodate a wide range of applications. Towards this goal, NDP units based on fine-grained (FPGA) and coarse-grained (CGRA) reconfigurable logic have been proposed as a compromise between the efficiency of custom engines and the flexibility of programmable cores. Unfortunately, FPGAs incur significant area overheads for bit-level reconfiguration, while CGRAs consume significant power in the interconnect and are inefficient for irregular data layouts and control flows. This paper presents Heterogeneous Reconfigurable Logic (HRL), a reconfigurable array for NDP systems that improves on both FPGA and CGRA arrays. HRL combines both coarse-grained and fine-grained logic blocks, separates routing networks for data and control signals, and uses specialized units to effectively support branch operations and irregular data layouts in analytics workloads. HRL has the power efficiency of FPGA and the area efficiency of CGRA. It improves performance per Watt by 2.2x over FPGA and 1.7x over CGRA. For NDP systems running MapReduce, graph processing, and deep neural networks, HRL achieves 92\% of the peak performance of an NDP system based on custom accelerators for each application.},
keywords={Field programmable gate arrays;Arrays;Three-dimensional displays;Bandwidth;Random access memory;Layout},
doi={10.1109/HPCA.2016.7446059},
ISSN={2378-203X},
month=Mar}


@article{2016-heracles-toppicks,
author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Ranganathan, Parthasarathy and Kozyrakis, Christos},
title = {Improving Resource Efficiency at Scale with Heracles},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {0734-2071},
doi = {10.1145/2882783},
abstract = {User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low traffic. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy efficiency of large-scale datacenters. With the slowdown in technology scaling caused by the sunsetting of Moore’s law, it becomes important to address this opportunity.We present Heracles, a feedback-based controller that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks. We evaluate Heracles using production latency-critical and batch workloads from Google and demonstrate average server utilizations of 90\% without latency violations across all the load and colocation scenarios that we evaluated.},
journal = {ACM Transactions on Computer Systems},
month = may,
articleno = {6},
numpages = {33},
keywords = {interference, QoS, performance isolation, warehouse-scale computer, Datacenter, latency-critical applications, scheduling, resource efficiency}
}


@inproceedings{2016-flash-eurosys,
author = {Klimovic, Ana and Kozyrakis, Christos and Thereska, Eno and John, Binu and Kumar, Sanjeev},
title = {Flash Storage Disaggregation},
year = {2016},
month = apr,
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2901318.2901337},
abstract = {PCIe-based Flash is commonly deployed to provide datacenter applications with high IO rates. However, its capacity and bandwidth are often underutilized as it is difficult to design servers with the right balance of CPU, memory and Flash resources over time and for multiple applications. This work examines Flash disaggregation as a way to deal with Flash overprovisioning. We tune remote access to Flash over commodity networks and analyze its impact on workloads sampled from real datacenter applications. We show that, while remote Flash access introduces a 20\% throughput drop at the application level, disaggregation allows us to make up for these overheads through resource-efficient scale-out. Hence, we show that Flash disaggregation allows scaling CPU and Flash resources independently in a cost effective manner. We use our analysis to draw conclusions about data and control plane issues in remote storage.},
booktitle = {Proceedings of the 11th European Conference on Computer Systems (EuroSys)},
articleno = {29},
numpages = {15},
keywords = {network storage, flash, datacenter},
location = {London, United Kingdom},
series = {EuroSys '16}
}


@inproceedings{2016-generate-asplos,
author = {Prabhakar, Raghu and Koeplinger, David and Brown, Kevin J. and Lee, HyoukJoong and De Sa, Christopher and Kozyrakis, Christos and Olukotun, Kunle},
title = {Generating Configurable Hardware from Parallel Patterns},
year = {2016},
month = Apr,
isbn = {9781450340915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2872362.2872415},
abstract = {In recent years the computing landscape has seen an increasing shift towards specialized accelerators. Field programmable gate arrays (FPGAs) are particularly promising for the implementation of these accelerators, as they offer significant performance and energy improvements over CPUs for a wide class of applications and are far more flexible than fixed-function ASICs. However, FPGAs are difficult to program. Traditional programming models for reconfigurable logic use low-level hardware description languages like Verilog and VHDL, which have none of the productivity features of modern software languages but produce very efficient designs, and low-level software languages like C and OpenCL coupled with high-level synthesis (HLS) tools that typically produce designs that are far less efficient. Functional languages with parallel patterns are a better fit for hardware generation because they provide high-level abstractions to programmers with little experience in hardware design and avoid many of the problems faced when generating hardware from imperative languages. In this paper, we identify two important optimizations for using parallel patterns to generate efficient hardware: tiling and metapipelining. We present a general representation of tiled parallel patterns, and provide rules for automatically tiling patterns and generating metapipelines. We demonstrate experimentally that these optimizations result in speedups up to 39.4\texttimes{} on a set of benchmarks from the data analytics domain.},
booktitle = {Proceedings of the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {651–665},
numpages = {15},
keywords = {reconfigurable hardware, metapipelining, parallel patterns, hardware generation, tiling, FPGAs},
location = {Atlanta, Georgia, USA},
series = {ASPLOS '16}
}



@inproceedings{2016-hcloud-asplos,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {HCloud: Resource-Efficient Provisioning in Shared Cloud Systems},
year = {2016},
month = Apr,
isbn = {9781450340915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2872362.2872365},
abstract = {Cloud computing promises flexibility and high performance for users and cost efficiency for operators. To achieve this, cloud providers offer instances of different sizes, both as long-term reservations and short-term, on-demand allocations. Unfortunately, determining the best provisioning strategy is a complex, multi-dimensional problem that depends on the load fluctuation and duration of incoming jobs, and the performance unpredictability and cost of resources. We first compare the two main provisioning strategies (reserved and on-demand resources) on Google Compute Engine (GCE) using three representative workload scenarios with batch and latency-critical applications. We show that either approach is suboptimal for performance or cost. We then present HCloud, a hybrid provisioning system that uses both reserved and on-demand resources. HCloud determines which jobs should be mapped to reserved versus on-demand resources based on overall load, and resource unpredictability. It also determines the optimal instance size an application needs to satisfy its Quality of Service (QoS) constraints. We demonstrate that hybrid configurations improve performance by 2.1x compared to fully on-demand provisioning, and reduce cost by 46\% compared to fully reserved systems. We also show that hybrid strategies are robust to variation in system and job parameters, such as cost and system load.},
booktitle = {Proceedings of the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {473–488},
numpages = {16},
keywords = {datacenter, cloud computing, latency, hybrid, resource efficiency, QoS, provisioning},
location = {Atlanta, Georgia, USA},
series = {ASPLOS '16}
}

@article{2015-n3xt-computer,
author = {Sabry Aly, Mohamed M. and Gao, Mingyu and Hills, Gage and Lee, Chi-Shuen and Pitner, Greg and Shulaker, Max M. and Wu, Tony F. and Asheghi, Mehdi and Bokor, Jeff and Franchetti, Franz and Goodson, Kenneth E. and Kozyrakis, Christos and Markov, Igor and Olukotun, Kunle and Pileggi, Larry and Pop, Eric and Rabaey, Jan and Re, Christopher and Wong, H.-S. Philip and Mitra, Subhasish},
title = {Energy-Efficient Abundant-Data Computing: The N3XT 1,000x},
year = {2015},
issue_date = {Dec. 2015},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {48},
number = {12},
issn = {0018-9162},
doi = {10.1109/MC.2015.376},
abstract = {Next-generation information technologies will process unprecedented amounts of loosely structured data that overwhelm existing computing systems. N3XT improves the energy efficiency of abundant-data applications 1,000-fold by using new logic and memory technologies, 3D integration with fine-grained connectivity, and new architectures for computation immersed in memory.},
journal = {IEEE Computer},
month = dec,
pages = {24–33},
numpages = {10}
}


@inproceedings{2015-ndp-pact,
author = {Gao, Mingyu and Ayers, Grant and Kozyrakis, Christos},
title = {Practical Near-Data Processing for In-Memory Analytics Frameworks},
year = {2015},
month = Sep,
isbn = {9781467395243},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/PACT.2015.22},
abstract = {The end of Dennard scaling has made all systemsenergy-constrained. For data-intensive applications with limitedtemporal locality, the major energy bottleneck is data movementbetween processor chips and main memory modules. For such workloads, the best way to optimize energy is to place processing near the datain main memory. Advances in 3D integrationprovide an opportunity to implement near-data processing (NDP) withoutthe technology problems that similar efforts had in the past. This paper develops the hardware and software of an NDP architecturefor in-memory analytics frameworks, including MapReduce, graphprocessing, and deep neural networks. We develop simple but scalablehardware support for coherence, communication, and synchronization, anda runtime system that is sufficient to support analytics frameworks withcomplex data patterns while hiding all thedetails of the NDP hardware. Our NDP architecture provides up to 16x performance and energy advantageover conventional approaches, and 2.5x over recently-proposed NDP systems. We also investigate the balance between processing and memory throughput, as well as the scalability and physical and logical organization of the memory system. Finally, we show that it is critical to optimize software frameworksfor spatial locality as it leads to 2.9x efficiency improvements for NDP.},
booktitle = {Proceedings of the 2015 International Conference on Parallel Architecture and Compilation (PACT)},
pages = {113–124},
numpages = {12},
series = {PACT '15}
}



@inproceedings{2015-ixctl-socc,
author = {Prekas, George and Primorac, Mia and Belay, Adam and Kozyrakis, Christos and Bugnion, Edouard},
title = {Energy Proportionality and Workload Consolidation for Latency-Critical Applications},
year = {2015},
month = Aug,
isbn = {9781450336512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2806777.2806848},
abstract = {Energy proportionality and workload consolidation are important objectives towards increasing efficiency in large-scale datacenters. Our work focuses on achieving these goals in the presence of applications with μs-scale tail latency requirements. Such applications represent a growing subset of datacenter workloads and are typically deployed on dedicated servers, which is the simplest way to ensure low tail latency across all loads. Unfortunately, it also leads to low energy efficiency and low resource utilization during the frequent periods of medium or low load.We present the OS mechanisms and dynamic control needed to adjust core allocation and voltage/frequency settings based on the measured delays for latency-critical workloads. This allows for energy proportionality and frees the maximum amount of resources per server for other background applications, while respecting service-level objectives. Monitoring hardware queue depths allows us to detect increases in queuing latencies. Carefully coordinated adjustments to the NIC's packet redirection table enable us to reassign flow groups between the threads of a latency-critical application in milliseconds without dropping or reordering packets. We compare the efficiency of our solution to the Pareto-optimal frontier of 224 distinct static configurations. Dynamic resource control saves 44\%--54\% of processor energy, which corresponds to 85\%--93\% of the Pareto-optimal upper bound. Dynamic resource control also allows background jobs to run at 32\%--46\% of their standalone throughput, which corresponds to 82%--92% of the Pareto bound.},
booktitle = {Proceedings of the 6th ACM Symposium on Cloud Computing (SOCC)},
pages = {342–355},
numpages = {14},
keywords = {energy proportionality, microsecond-scale computing, workload consolidation, latency-critical applications},
location = {Kohala Coast, Hawaii},
series = {SoCC '15}
}

@inproceedings{2015-tarcil-socc,
author = {Delimitrou, Christina and Sanchez, Daniel and Kozyrakis, Christos},
title = {Tarcil: Reconciling Scheduling Speed and Quality in Large Shared Clusters},
year = {2015},
month = Aug,
isbn = {9781450336512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2806777.2806779},
abstract = {Scheduling diverse applications in large, shared clusters is particularly challenging. Recent research on cluster scheduling focuses either on scheduling speed, using sampling to quickly assign resources to tasks, or on scheduling quality, using centralized algorithms that search for the resources that improve both task performance and cluster utilization.We present Tarcil, a distributed scheduler that targets both scheduling speed and quality. Tarcil uses an analytically derived sampling framework that adjusts the sample size based on load, and provides statistical guarantees on the quality of allocated resources. It also implements admission control when sampling is unlikely to find suitable resources. This makes it appropriate for large, shared clusters hosting short- and long-running jobs. We evaluate Tarcil on clusters with hundreds of servers on EC2. For highly-loaded clusters running short jobs, Tarcil improves task execution time by 41\% over a distributed, sampling-based scheduler. For more general scenarios, Tarcil achieves near-optimal performance for 4\texttimes{} and 2\texttimes{} more jobs than sampling-based and centralized schedulers respectively.},
booktitle = {Proceedings of the 6th ACM Symposium on Cloud Computing (SOCC)},
pages = {97–110},
numpages = {14},
keywords = {scalability, scheduling, QoS, cloud computing, datacenters, resource-efficiency},
location = {Kohala Coast, Hawaii},
series = {SoCC '15}
}


@inproceedings{2015-heracles-isca,
author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Ranganathan, Parthasarathy and Kozyrakis, Christos},
title = {Heracles: Improving Resource Efficiency at Scale},
year = {2015},
month = jun,
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2749469.2749475},
abstract = {User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low traffic. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy-efficiency of large-scale datacenters. With technology scaling slowing down, it becomes important to address this opportunity.We present Heracles, a feedback-based controller that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks. We evaluate Heracles using production latency-critical and batch workloads from Google and demonstrate average server utilizations of 90\% without latency violations across all the load and colocation scenarios that we evaluated.},
booktitle = {Proceedings of the 42nd International Symposium on Computer Architecture (ISCA)},
pages = {450–462},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}




@article{2009-hotchips-micro,
author = {Kozyrakis, Christos and Waerdt, Jan-Willem van de},
title = {Guest Editors' Introduction: Hot Chips Turns 20},
year = {2009},
issue_date = {March 2009},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {29},
number = {2},
issn = {0272-1732},
doi = {10.1109/MM.2009.31},
abstract = {The five papers in this special issue are extended versions of papers presented at the Hot Chips conference in August 1008.},
journal = {IEEE Micro},
month = mar,
pages = {4–5},
numpages = {2}
}

@article{2015-conv-toppicks,
author = {Qadeer, Wajahat and Hameed, Rehan and Shacham, Ofer and Venkatesan, Preethi and Kozyrakis, Christos and Horowitz, Mark},
title = {Convolution Engine: Balancing Efficiency and Flexibility in Specialized Computing},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {4},
issn = {0001-0782},
doi = {10.1145/2735841},
abstract = {General-purpose processors, while tremendously versatile, pay a huge cost for their flexibility by wasting over 99\% of the energy in programmability overheads. We observe that reducing this waste requires tuning data storage and compute structures and their connectivity to the data-flow and data-locality patterns in the algorithms. Hence, by backing off from full programmability and instead targeting key data-flow patterns used in a domain, we can create efficient engines that can be programmed and reused across a wide range of applications within that domain.We present the Convolution Engine (CE)---a programmable processor specialized for the convolution-like data-flow prevalent in computational photography, computer vision, and video processing. The CE achieves energy efficiency by capturing data-reuse patterns, eliminating data transfer overheads, and enabling a large number of operations per memory access. We demonstrate that the CE is within a factor of 2--3\texttimes{} of the energy and area efficiency of custom units optimized for a single kernel. The CE improves energy and area efficiency by 8--15\texttimes{} over data-parallel Single Instruction Multiple Data (SIMD) engines for most image processing applications.&lt;!-- END_PAGE_1 --&gt;},
journal = {Communications of the ACM},
month = mar,
pages = {85–93},
numpages = {9}
}


@ARTICLE{2014-paragon-toppicks,
  author={Delimitrou, Christina and Kozyrakis, Christos},
  journal={IEEE Micro}, 
  title={Quality-of-Service-Aware Scheduling in Heterogeneous Data centers with Paragon}, 
  year={2014},
  volume={34},
  number={3},
  pages={17-30},
  month = may,
  doi={10.1109/MM.2014.7}}



@Misc{2014-kvs-nvmw,
  author = 	 {Shingo Tanaka, Christos Kozyrakis},
  title = 	 {High Performance Hardware-Accelerated Flash Key-Value Store},
  howpublished = {The 2014 Non-volatile Memories Workshop (NVMW)},
  month = 	 Mar,
  year = 	 2014}

@INPROCEEDINGS{2014-turbo-hpca,
author={Lo, David and Kozyrakis, Christos},
booktitle={Proceedings of the IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)},
title={Dynamic management of TurboMode in modern multi-core chips},
year={2014},
month = Feb,
volume={},
number={},
pages={603-613},
abstract={Dynamic overclocking of CPUs, or TurboMode, is a feature recently introduced on all x86 multi-core chips. It leverages thermal and power headroom from idle execution resources to overclock active cores to increase performance. TurboMode can accelerate CPU-bound applications at the cost of additional power consumption. Nevertheless, naive use of TurboMode can significantly increase power consumption without increasing performance. Thus far, there is no strategy for managing TurboMode to optimize its use across all workloads and efficiency metrics. This paper analyzes the impact of TurboMode on a wide range of efficiency metrics (performance, power, cost, and combined metrics such as QPS/W and ED2) for representative server workloads on various hardware configurations. We determine that TurboMode is generally beneficial for performance (up to +24\%), cost efficiency (QPS/\$ up to +8\%), energy-delay product (ED, up to +47\%), and energy-delay-squared product (ED2, up to +68\%). However, TurboMode is inefficient for workloads that exhibit interference for shared resources. We use this information to build and validate a model that predicts the optimal TurboMode setting for each efficiency metric. We then implement autoturbo, a background daemon that dynamically manages TurboMode in real time without any hardware changes. We demonstrate that autoturbo improves QPS/\$, ED, and ED2 by 8\%, 47\%, and 68\% respectively over not using TurboMode. At the same time, autoturbo virtually eliminates all the large drops in those same metrics (-12\%, -25\%, -25\% for QPS/\$, ED, and ED2) that occur when TurboMode is used naively (always on).},
keywords={Hardware;Servers;Interference;Bridges;Clocks;Quality of service},
doi={10.1109/HPCA.2014.6835969},
ISSN={2378-203X},
month=Feb,}

@inproceedings{2014-ix-osdi,
author = {Belay, Adam and Prekas, George and Klimovic, Ana and Grossman, Samuel and Kozyrakis, Christos and Bugnion, Edouard},
title = {IX: A Protected Dataplane Operating System for High Throughput and Low Latency},
year = {2014},
month = oct,
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {The conventional wisdom is that aggressive networking requirements, such as high packet rates for small messages and microsecond-scale tail latency, are best addressed outside the kernel, in a user-level networking stack. We present IX, a dataplane operating system that provides high I/O performance, while maintaining the key advantage of strong protection offered by existing kernels. IX uses hardware virtualization to separate management and scheduling functions of the kernel (control plane) from network processing (dataplane). The data-plane architecture builds upon a native, zero-copy API and optimizes for both bandwidth and latency by dedicating hardware threads and networking queues to data-plane instances, processing bounded batches of packets to completion, and by eliminating coherence traffic and multi-core synchronization. We demonstrate that IX outperforms Linux and state-of-the-art, user-space network stacks significantly in both throughput and end-to-end latency. Moreover, IX improves the throughput of a widely deployed, key-value store by up to 3.6x and reduces tail latency by more than 2x.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation (OSDI)},
pages = {49–65},
numpages = {17},
location = {Broomfield, CO},
series = {OSDI'14}
}

@inproceedings{2014-proportional-isca,
author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Barroso, Luiz Andr\'{e} and Kozyrakis, Christos},
title = {Towards Energy Proportionality for Large-Scale Latency-Critical Workloads},
year = {2014},
month = jun,
isbn = {9781479943944},
publisher = {IEEE Press},
abstract = {Reducing the energy footprint of warehouse-scale computer (WSC) systems is key to their affordability, yet difficult to achieve in practice. The lack of energy proportionality of typical WSC hardware and the fact that important workloads (such as search) require all servers to remain up regardless of traffic intensity renders existing power management techniques ineffective at reducing WSC energy use.We present PEGASUS, a feedback-based controller that significantly improves the energy proportionality of WSC systems, as demonstrated by a real implementation in a Google search cluster. PEGASUS uses request latency statistics to dynamically adjust server power management limits in a fine-grain manner, running each server just fast enough to meet global service-level latency objectives. In large cluster experiments, PEGASUS reduces power consumption by up to 20\%. We also estimate that a distributed version of PEGASUS can nearly double these savings},
booktitle = {Proceeding of the 41st  International Symposium on Computer Architecuture (ISCA)},
pages = {301–312},
numpages = {12},
location = {Minneapolis, Minnesota, USA},
series = {ISCA '14}
}

@inproceedings{2014-usec-eurosys,
author = {Leverich, Jacob and Kozyrakis, Christos},
title = {Reconciling High Server Utilization and Sub-Millisecond Quality-of-Service},
year = {2014},
month = apr,
isbn = {9781450327046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2592798.2592821},
abstract = {The simplest strategy to guarantee good quality of service (QoS) for a latency-sensitive workload with sub-millisecond latency in a shared cluster environment is to never run other workloads concurrently with it on the same server. Unfortunately, this inevitably leads to low server utilization, reducing both the capability and cost effectiveness of the cluster.In this paper, we analyze the challenges of maintaining high QoS for low-latency workloads when sharing servers with other workloads. We show that workload co-location leads to QoS violations due to increases in queuing delay, scheduling delay, and thread load imbalance. We present techniques that address these vulnerabilities, ranging from provisioning the latency-critical service in an interference aware manner, to replacing the Linux CFS scheduler with a scheduler that provides good latency guarantees and fairness for co-located workloads. Ultimately, we demonstrate that some latency-critical workloads can be aggressively co-located with other workloads, achieve good QoS, and that such co-location can improve a datacenter's effective throughput per TCO-\$ by up to 52%.},
booktitle = {Proceedings of the 9th European Conference on Computer Systems (EuroSys)},
articleno = {4},
numpages = {14},
location = {Amsterdam, The Netherlands},
series = {EuroSys '14}
}

@inproceedings{2014-quasar-asplos,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Quasar: Resource-Efficient and QoS-Aware Cluster Management},
year = {2014},
month = mar,
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2541940.2541941},
abstract = {Cloud computing promises flexibility and high performance for users and high cost-efficiency for operators. Nevertheless, most cloud facilities operate at very low utilization, hurting both cost effectiveness and future scalability.We present Quasar, a cluster management system that increases resource utilization while providing consistently high application performance. Quasar employs three techniques. First, it does not rely on resource reservations, which lead to underutilization as users do not necessarily understand workload dynamics and physical resource requirements of complex codebases. Instead, users express performance constraints for each workload, letting Quasar determine the right amount of resources to meet these constraints at any point. Second, Quasar uses classification techniques to quickly and accurately determine the impact of the amount of resources (scale-out and scale-up), type of resources, and interference on performance for each workload and dataset. Third, it uses the classification results to jointly perform resource allocation and assignment, quickly exploring the large space of options for an efficient way to pack workloads on available resources. Quasar monitors workload performance and adjusts resource allocation and assignment when needed. We evaluate Quasar over a wide range of workload scenarios, including combinations of distributed analytics frameworks and low-latency, stateful services, both on a local cluster and a cluster of dedicated EC2 servers. At steady state, Quasar improves resource utilization by 47\% in the 200-server EC2 cluster, while meeting performance constraints for workloads of all types.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {127–144},
numpages = {18},
keywords = {resource efficiency, resource allocation and assignment, quality of service, cloud computing, datacenters, cluster management},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}


@INPROCEEDINGS{2013-ibench-iiswc,
author={Delimitrou, Christina and Kozyrakis, Christos},
booktitle={2013 IEEE International Symposium on Workload Characterization (IISWC)},
title={iBench: Quantifying interference for datacenter applications},
year={2013},
month = sep,
volume={},
number={},
pages={23-33},
abstract={Interference between co-scheduled applications is one of the major reasons that causes modern datacenters (DCs) to operate at low utilization. DC operators traditionally side-step interference either by disallowing colocation altogether and providing isolated server instances, or by requiring the users to express resource reservations, which are often exaggerated to counter-balance the unpredictability in the quality of allocated resources. Understanding, reducing and managing interference can significantly impact the manner in which these large-scale systems operate. We present iBench, a novel workload suite that helps quantify the pressure different applications put in various shared resources, and similarly the pressure they can tolerate in these resources. iBench consists of a set of carefully-crafted benchmarks that induce interference of increasing intensity in resources that span the CPU, cache hierarchy, memory, storage and networking subsystems. We first validate the effect that iBench workloads have on performance against a wide spectrum of DC applications. Then, we use iBench to demonstrate the importance of considering interference in a set of challenging problems that range from DC scheduling and server provisioning, to resource-efficient application development and scheduling for heterogeneous CMPs. In all cases quantifying interference with iBench results in significant performance and/or efficiency improvements. We plan to release iBench under a free software license.},
keywords={Interference;Benchmark testing;Bandwidth;Kernel;Servers;Sensitivity;Prefetching},
doi={10.1109/IISWC.2013.6704667},
ISSN={},
month=sep}

@article{2013-paragon-tocs,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {QoS-Aware Scheduling in Heterogeneous Datacenters with Paragon},
year = {2013},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {0734-2071},
doi = {10.1145/2556583},
abstract = {Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty of matching applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online, and do not scale beyond a few applications.We present Paragon, an online and scalable DC scheduler that is heterogeneity- and interference-aware. Paragon is derived from robust analytical methods, and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown incoming workload with respect to heterogeneity and interference in multiple shared resources. It does so by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily schedule applications in a manner that minimizes interference and maximizes server utilization. After the initial application placement, Paragon monitors application behavior and adjusts the scheduling decisions at runtime to avoid performance degradations. Additionally, we design ARQ, a multiclass admission control protocol that constrains application waiting time. ARQ queues applications in separate classes based on the type of resources they need and avoids long queueing delays for easy-to-satisfy workloads in highly-loaded scenarios. Paragon scales to tens of thousands of servers and applications with marginal scheduling overheads in terms of time or state.We evaluate Paragon with a wide range of workload scenarios, on both small and large-scale systems, including 1,000 servers on EC2. For a 2,500-workload scenario, Paragon enforces performance guarantees for 91\% of applications, while significantly improving utilization. In comparison, heterogeneity-oblivious, interference-oblivious, and least-loaded schedulers only provide similar guarantees for 14\%, 11\%, and 3\% of workloads. The differences are more striking in oversubscribed scenarios where resource efficiency is more critical.},
journal = {ACM Transactions Computer Systems},
month = dec,
articleno = {12},
numpages = {34},
keywords = {interference, cloud computing, QoS, Datacenter, resource-efficiency, scheduling, heterogeneity}
}

@inproceedings{2013-locality-spaa,
author = {Yoo, Richard M. and Hughes, Christopher J. and Kim, Changkyu and Chen, Yen-Kuang and Kozyrakis, Christos},
title = {Locality-Aware Task Management for Unstructured Parallelism: A Quantitative Limit Study},
year = {2013},
month = jul,
isbn = {9781450315722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2486159.2486175},
abstract = {As we increase the number of cores on a processor die, the on-chip cache hierarchies that support these cores are getting larger, deeper, and more complex. As a result, non-uniform memory access effects are now prevalent even on a single chip. To reduce execution time and energy consumption, data access locality should be exploited. This is especially important for task-based programming systems, where a scheduler decides when and where on the chip the code segments, i.e., tasks, should execute. Capturing locality for structured task parallelism has been done effectively, but the more difficult case, unstructured parallelism, remains largely unsolved - little quantitative analysis exists to demonstrate the potential of locality-aware scheduling, and to guide future scheduler implementations in the most fruitful direction.This paper quantifies the potential of locality-aware scheduling for unstructured parallelism on three different many-core processors. Our simulation results of 32-core systems show that locality-aware scheduling can bring up to 2.39x speedup over a randomized schedule, and 2.05x speedup over a state-of-the-art baseline scheduling scheme. At the same time, a locality-aware schedule reduces average energy consumption by 55\% and 47\%, relative to the random and the baseline schedule, respectively. In addition, our 1024-core simulation results project that these benefits will only increase: Compared to 32-core executions, we see up to 1.83x additional locality benefits. To capture such potentials in a practical setting, we also perform a detailed scheduler design space exploration to quantify the impact of different scheduling decisions. We also highlight the importance of locality-aware stealing, and demonstrate that a stealing scheme can exploit significant locality while performing load balancing. Over randomized stealing, our proposed scheme shows up to 2.0x speedup for stolen tasks.},
booktitle = {Proceedings of the 25th  ACM Symposium on Parallelism in Algorithms and Architectures (SPAA)},
pages = {315–325},
numpages = {11},
keywords = {task stealing, task scheduling, energy, performance, locality},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {SPAA '13}
}


@inproceedings {2013-qos-icac,
author = {Christina Delimitrou and Nick Bambos and Christos Kozyrakis},
title = {{QoS-Aware} Admission Control in Heterogeneous Datacenters},
booktitle = {10th International Conference on Autonomic Computing (ICAC)},
year = {2013},
month = jun,
isbn = {978-1-931971-02-7},
address = {San Jose, CA},
pages = {291--296},
url = {https://www.usenix.org/conference/icac13/technical-sessions/presentation/delimitrou},
publisher = {USENIX Association},
month = jun,
}

@Misc{2013-nack-transact,
  author = 	 {Woongki Baek, Richard Yoo, Christos Kozyrakis},
  title = 	 {Enhanced Concurrency Control with Transactional NACKs},
  howpublished = {Proceedings of the 8th ACM SIGPLAN Workshop on Transactional Computing (TRANSACT)},
  month = 	 Mar,
  year = 	 2013}

@article{2013-energy-scis,
  author    = {Maria A. Kazandjieva and
               Brandon Heller and
               Omprakash Gnawali and
               Philip Alexander Levis and
               Christos Kozyrakis},
  title     = {Measuring and analyzing the energy use of enterprise computing systems},
  journal   = {Sustainable Computing Informatics and Systems},
  volume    = {3},
  number    = {3},
  pages     = {218--229},
  year      = {2013},
  doi       = {10.1016/j.suscom.2013.01.009},
  timestamp = {Mon, 30 Aug 2021 16:42:58 +0200},
  biburl    = {https://dblp.org/rec/journals/suscom/KazandjievaHGLK13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2013-zsim-isca,
author = {Sanchez, Daniel and Kozyrakis, Christos},
title = {ZSim: Fast and Accurate Microarchitectural Simulation of Thousand-Core Systems},
year = {2013},
month = jun,
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2485922.2485963},
abstract = {Architectural simulation is time-consuming, and the trend towards hundreds of cores is making sequential simulation even slower. Existing parallel simulation techniques either scale poorly due to excessive synchronization, or sacrifice accuracy by allowing event reordering and using simplistic contention models. As a result, most researchers use sequential simulators and model small-scale systems with 16-32 cores. With 100-core chips already available, developing simulators that scale to thousands of cores is crucial.We present three novel techniques that, together, make thousand-core simulation practical. First, we speed up detailed core models (including OOO cores) with instruction-driven timing models that leverage dynamic binary translation. Second, we introduce bound-weave, a two-phase parallelization technique that scales parallel simulation on multicore hosts efficiently with minimal loss of accuracy. Third, we implement lightweight user-level virtualization to support complex workloads, including multiprogrammed, client-server, and managed-runtime applications, without the need for full-system simulation, sidestepping the lack of scalable OSs and ISAs that support thousands of cores.We use these techniques to build zsim, a fast, scalable, and accurate simulator. On a 16-core host, zsim models a 1024-core chip at speeds of up to 1,500 MIPS using simple cores and up to 300 MIPS using detailed OOO cores, 2-3 orders of magnitude faster than existing parallel simulators. Simulator performance scales well with both the number of modeled cores and the number of host cores. We validate zsim against a real Westmere system on a wide variety of workloads, and find performance and microarchitectural events to be within a narrow range of the real system.},
booktitle = {Proceedings of the 40th  International Symposium on Computer Architecture},
pages = {475–486},
numpages = {12},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}


@inproceedings{2013-convolution-isca,
author = {Qadeer, Wajahat and Hameed, Rehan and Shacham, Ofer and Venkatesan, Preethi and Kozyrakis, Christos and Horowitz, Mark A.},
title = {Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing},
year = {2013},
month = jun,
isbn = {9781450320795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2485922.2485925},
abstract = {This paper focuses on the trade-off between flexibility and efficiency in specialized computing. We observe that specialized units achieve most of their efficiency gains by tuning data storage and compute structures and their connectivity to the data-flow and data-locality patterns in the kernels. Hence, by identifying key data-flow patterns used in a domain, we can create efficient engines that can be programmed and reused across a wide range of applications.We present an example, the Convolution Engine (CE), specialized for the convolution-like data-flow that is common in computational photography, image processing, and video processing applications. CE achieves energy efficiency by capturing data reuse patterns, eliminating data transfer overheads, and enabling a large number of operations per memory access. We quantify the tradeoffs in efficiency and flexibility and demonstrate that CE is within a factor of 2-3x of the energy and area efficiency of custom units optimized for a single kernel. CE improves energy and area efficiency by 8-15x over a SIMD engine for most applications.},
booktitle = {Proceedings of the 40th  International Symposium on Computer Architecture (ISCA)},
pages = {24–35},
numpages = {12},
keywords = {specialized computing, H.264, tensilica, demosaic, energy efficiency, computational photography, convolution},
location = {Tel-Aviv, Israel},
series = {ISCA '13}
}

@inproceedings{2013-resource-date,
author = {Kozyrakis, Christos},
title = {Resource Efficient Computing for Warehouse-Scale Datacenters},
year = {2013},
month = mar,
isbn = {9781450321532},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {An increasing amount of information technology services and data are now hosted in the cloud, primarily due to the cost and scalability benefits for both the end-users and the operators of the warehouse-scale datacenters (DCs) that host cloud services. Hence, it is vital to continuously improve the capabilities and efficiency of these large-scale systems. Over the past ten years, capability has improved by increasing the number of servers in a DC and the bandwidth of the network that connects them. Cost and energy efficiency have improved by eliminating the high overheads of the power delivery and cooling infrastructure. To achieve further improvements, we must now examine how well we are utilizing the servers themselves, which are the primary determinant for DC performance, cost, and energy efficiency. This is particularly important since the semiconductor chips used in servers are now energy limited and their efficiency does not scale as fast as in the past. This paper motivates the need for resource efficient computing in large-scale datacenters and reviews the major challenges and research opportunities.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe (DATE)},
pages = {1351–1356},
numpages = {6},
location = {Grenoble, France},
series = {DATE '13}
}

@inproceedings{2013-paragon-asplos,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Paragon: QoS-Aware Scheduling for Heterogeneous Datacenters},
year = {2013},
month = Mar,
isbn = {9781450318709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2451116.2451125},
abstract = {Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty to match applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online and do not scale beyond few applications.We present Paragon, an online and scalable DC scheduler that is heterogeneity and interference-aware. Paragon is derived from robust analytical methods and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown, incoming workload with respect to heterogeneity and interference in multiple shared resources, by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily schedule applications in a manner that minimizes interference and maximizes server utilization. Paragon scales to tens of thousands of servers with marginal scheduling overheads in terms of time or state.We evaluate Paragon with a wide range of workload scenarios, on both small and large-scale systems, including 1,000 servers on EC2. For a 2,500-workload scenario, Paragon enforces performance guarantees for 91\% of applications, while significantly improving utilization. In comparison, heterogeneity-oblivious, interference-oblivious and least-loaded schedulers only provide similar guarantees for 14\%, 11\% and 3\% of workloads. The differences are more striking in oversubscribed scenarios where resource efficiency is more critical.},
booktitle = {Proceedings of the 18th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {77–88},
numpages = {12},
keywords = {interference, cloud computing, datacenter, scheduling, qos, heterogeneity},
location = {Houston, Texas, USA},
series = {ASPLOS '13}
}



@article{2013-hotchips-micro,
author = {Kozyrakis, Christos and Zahir, Rumi},
title = {Selected Research from Hot Chips 24},
year = {2013},
issue_date = {March 2013},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {33},
number = {2},
issn = {0272-1732},
doi = {10.1109/MM.2013.44},
abstract = {This introduction to the special issue introduces the articles selected for publication from Hot Chips 24.},
journal = {IEEE Micro},
month = mar,
pages = {6–7},
numpages = {2},
keywords = {low voltage, Program processors, Special issues and sections, Multicore processing, instructed set, energy-efficient scaling, processing engine, Multiprocessors, Hot Chips}
}


@article{2013-netflix-cal,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {The Netflix Challenge: Datacenter Edition},
year = {2013},
issue_date = {January 2013},
publisher = {IEEE Computer Society},
address = {USA},
volume = {12},
number = {1},
issn = {1556-6056},
doi = {10.1109/L-CA.2012.10},
abstract = {The hundreds of thousands of servers in modern warehouse-scale systems make performance and efficiency optimizations pressing design challenges. These systems are traditionally considered homogeneous. However, that is not typically the case. Multiple server generations compose a heterogeneous environment, whose performance opportunities have not been fully explored since techniques that account for platform heterogeneity typically do not scale to the tens of thousands of applications hosted in large-scale cloud providers. We present ADSM, a scalable and efficient recommendation system for application-to-server mapping in large-scale datacenters (DCs) that is QoS-aware. ADSM overcomes the drawbacks of previous techniques, by leveraging robust and computationally efficient analytical methods to scale to tens of thousands of applications with minimal overheads. It is also QoS-aware, mapping applications to platforms while enforcing strict QoS guarantees. ADSM is derived from validated analytical models, has low and bounded prediction errors, is simple to implement and scales to thousands of applications without significant changes to the system. Over 390 real DC workloads, ADSM improves performance by 16\% on average and up to 2.5x and efficiency by 22\% in a DC with 10 different server configurations.},
journal = {IEEE Computer Architecture Letters},
month = jan,
pages = {29–32},
numpages = {4},
keywords = {Computer System Implementation, Computer Systems Organization, Large and Medium (“Mainframe”), simulation of multiple-processor systems}
}

@inproceedings{2012-echo-iiswc,
author = {Delimitrou, Christina and Sankar, Sriram and Kansal, Aman and Kozyrakis, Christos},
title = {ECHO: Recreating Network Traffic Maps for Datacenters with Tens of Thousands of Servers},
year = {2012},
month = nov,
isbn = {9781467345316},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/IISWC.2012.6402896},
abstract = {Large-scale datacenters now host a large part of the world's data and computation, which makes their design a crucial architectural challenge. Datacenter (DC) applications, unlike traditional workloads, are dominated by user patterns that only emerge in the large-scale. This creates the need for concise, accurate and scalable analytical models that capture both their temporal and spatial features and can be used to create representative activity patterns. Unfortunately, previous work lacks the ability to track the complex patterns that are present in these applications, or scales poorly with the size of the system. In this work, we focus on the network aspect of datacenter workloads. We present ECHO, a scalable and accurate modeling scheme that uses hierarchical Markov Chains to capture the network activity of large-scale applications in time and space. ECHO can also use these models to re-create representative network traffic patterns. We validate the model against real DC-scale applications, such as Websearch and show marginal deviations between original and generated workloads. We verify that ECHO captures all the critical features of DC workloads, such as the locality of communication and burstiness and evaluate the granularity necessary for this. Finally we perform a detailed characterization of the network traffic for workloads in DCs of tens of thousands of servers over significant time frames.},
booktitle = {Proceedings of the 2012 IEEE International Symposium on Workload Characterization (IISWC)},
pages = {14–24},
numpages = {11},
series = {IISWC '12}
}

@inproceedings{2012-dune-osdi,
author = {Belay, Adam and Bittau, Andrea and Mashtizadeh, Ali and Terei, David and Mazi\`{e}res, David and Kozyrakis, Christos},
title = {Dune: Safe User-Level Access to Privileged CPU Features},
year = {2012},
month = oct,
isbn = {9781931971966},
publisher = {USENIX Association},
address = {USA},
abstract = {Dune is a system that provides applications with direct but safe access to hardware features such as ring protection, page tables, and tagged TLBs, while preserving the existing OS interfaces for processes. Dune uses the virtualization hardware in modern processors to provide a process, rather than a machine abstraction. It consists of a small kernel module that initializes virtualization hardware and mediates interactions with the kernel, and a user-level library that helps applications manage privileged hardware features. We present the implementation of Dune for 64- bit x86 Linux. We use Dune to implement three user-level applications that can benefit from access to privileged hardware: a sandbox for untrusted code, a privilege separation facility, and a garbage collector. The use of Dune greatly simplifies the implementation of these applications and provides significant performance advantages.},
booktitle = {Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementatio (OSDI)},
pages = {335–348},
numpages = {14},
location = {Hollywood, CA, USA},
series = {OSDI'12}
}


@inproceedings{2012-custom-codes,
author = {Hong, Sungpack and Oguntebi, Tayo and Casper, Jared and Bronson, Nathan and Kozyrakis, Christos and Olukotun, Kunle},
title = {A Case of System-Level Hardware/Software Co-Design and Co-Verification of a Commodity Multi-Processor System with Custom Hardware},
year = {2012},
month = oct,
isbn = {9781450314268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2380445.2380524},
abstract = {This paper presents an interesting system-level co-design and co-verification case study for a non-trivial design where multiple high-performing x86 processors and custom hardware were connected through a coherent interconnection fabric. In functional verification of such a system, we used a processor bus functional model (BFM) to combine native software execution with a cycle-accurate interconnect simulator and an HDL simulator. However, we found that significant extensions need to be made to the conventional BFM methodology in order to capture various data-race cases in simulation, which eventually happen in modern multi-processor systems. Especially essential were faithful implementations of the memory consistency model and cache coherence protocol, as well as timing randomization. We demonstrate how such a co-simulation environment can be constructed from existing tools and software. Lessons from our study can similarly be applied to design and verification of other tightly-coupled systems.},
booktitle = {Proceedings of the 8th IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)},
pages = {513–520},
numpages = {8},
keywords = {co-simulation, bus functional model, fpga prototyping, transactional memory, co-verification},
location = {Tampere, Finland},
series = {CODES+ISSS '12}
}

@article{2012-dcstorage-cal,
author = {Delimitrou, Christina and Sankar, Sriram and Vaid, Kushagra and Kozyrakis, Christos},
title = {Decoupling Datacenter Storage Studies from Access to Large-Scale Applications},
year = {2012},
issue_date = {July 2012},
publisher = {IEEE Computer Society},
address = {USA},
volume = {11},
number = {2},
issn = {1556-6056},
doi = {10.1109/L-CA.2011.37},
abstract = {Suboptimal storage design has significant cost and power impact in large-scale datacenters (DCs). Performance, power and cost-optimized systems require deep understanding of target workloads, and mechanisms to effectively model different storage design choices. Traditional benchmarking is invalid in cloud data-stores, representative storage profiles are hard to obtain, while replaying applications in different storage configurations is impractical both in cost and time. Despite these issues, current workload generators are not able to reproduce key aspects of real application patterns (e.g., spatial/temporal locality, I/O intensity). In this paper, we propose a modeling and generation framework for large-scale storage applications. As part of this framework we use a state diagram-based storage model, extend it to a hierarchical representation, and implement a tool that consistently recreates DC application I/O loads. We present the principal features of the framework that allow accurate modeling and generation of storage workloads, and the validation process performed against ten original DC application traces. Finally, we explore two practical applications of this methodology: SSD caching and defragmentation benefits on enterprise storage. Since knowledge of the workload's spatial and temporal locality is necessary to model these use cases, our framework was instrumental in quantifying their performance benefits. The proposed methodology provides detailed understanding of the storage activity of large-scale applications, and enables a wide spectrum of storage studies, without the requirement to access application code and full application deployment.},
journal = {IEEE Compututer Architecture Letters},
month = jul,
pages = {53–56},
numpages = {4},
keywords = {Very large scale integration, Modeling techniques, Modeling of computer architecture, Computational modeling, Generators, Storage area networks, Super (very large) computers, Electronic mail, Load modeling, Throughput, Mass storage}
}

@inproceedings{2012-dram-isca,
author = {Malladi, Krishna T. and Lee, Benjamin C. and Nothaft, Frank A. and Kozyrakis, Christos and Periyathambi, Karthika and Horowitz, Mark},
title = {Towards Energy-Proportional Datacenter Memory with Mobile DRAM},
year = {2012},
month = jun,
isbn = {9781450316422},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {To increase datacenter energy efficiency, we need memory systems that keep pace with processor efficiency gains. Currently, servers use DDR3 memory, which is designed for high bandwidth but not for energy proportionality. A system using 20\% of the peak DDR3 bandwidth consumes 2.3x the energy per bit compared to the energy consumed by a system with fully utilized memory bandwidth. Nevertheless, many datacenter applications stress memory capacity and latency but not memory bandwidth. In response, we architect server memory systems using mobile DRAM devices, trading peak bandwidth for lower energy consumption per bit and more efficient idle modes. We demonstrate 3-5x lower memory power, better proportionality, and negligible performance penalties for datacenter workloads.},
booktitle = {Proceedings of the 39th  International Symposium on Computer Architecture (ISCA)},
pages = {37–48},
numpages = {12},
location = {Portland, Oregon},
series = {ISCA '12}
}



@inproceedings{2012-green-igcc,
author = {Kazandjieva, Maria and Heller, Brandon and Gnawali, Omprakash and Levis, Philip and Kozyrakis, Christos},
title = {Green Enterprise Computing Data: Assumptions and Realities},
year = {2012},
month = jun,
isbn = {9781467321556},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/IGCC.2012.6322264},
abstract = {Until now, green computing research has largely relied on few, short-term power measurements to characterize the energy use of enterprise computing. This paper brings new and comprehensive power datasets through Powernet, a hybrid sensor network that monitors the power and utilization of the IT systems in a large academic building. Over more than two years, we have collected power data from 250+ individual computing devices and have monitored a subset of CPU and network loads. This dense, long-term monitoring allows us to extrapolate the data to a detailed breakdown of electricity use across the building's computing systems. Our datasets provide an opportunity to examine assumptions commonly made in green computing. We show that power variability both between similar devices and over time for a single device can lead to cost or savings estimates that are off by 15–20\%. Extending the coverage of measured devices and the duration (to at least one month) significantly reduces errors. Lastly, our experiences with collecting data and the subsequent analysis lead to a better understanding of how one should go about power characterization studies. We provide several methodology guidelines for future green computing research.},
booktitle = {Proceedings of the 2012 International Green Computing Conference (IGCC)},
pages = {1–10},
numpages = {10},
series = {IGCC '12}
}


@article{2012-vantage-toppicks,
author = {Sanchez, Daniel and Kozyrakis, Christos},
title = {Scalable and Efficient Fine-Grained Cache Partitioning with Vantage},
year = {2012},
issue_date = {May 2012},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {32},
number = {3},
issn = {0272-1732},
doi = {10.1109/MM.2012.19},
abstract = {The Vantage cache-partitioning technique enables configurability and quality-of-service guarantees in large-scale chip multiprocessors with shared caches. Caches can have hundreds of partitions with sizes specified at cache line granularity, while maintaining high associativity and strict isolation among partitions},
journal = {IEEE Micro},
month = may,
pages = {26–37},
numpages = {12},
keywords = {Vantage, QoS, cache memories, cache partitioning, design styles, hardware, memory hierarchy, microarchitecture implementation considerations, memory structures, computer systems organization, parallel architectures, processor architectures, CMP}
}

@article{2012-subsetting-taco,
author = {Ahn, Jung Ho and Jouppi, Norman P. and Kozyrakis, Christos and Leverich, Jacob and Schreiber, Robert S.},
title = {Improving System Energy Efficiency with Memory Rank Subsetting},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {1544-3566},
doi = {10.1145/2133382.2133386},
abstract = {VLSI process technology scaling has enabled dramatic improvements in the capacity and peak bandwidth of DRAM devices. However, current standard DDRx DIMM memory interfaces are not well tailored to achieve high energy efficiency and performance in modern chip-multiprocessor-based computer systems. Their suboptimal performance and energy inefficiency can have a significant impact on system-wide efficiency since much of the system power dissipation is due to memory power. New memory interfaces, better suited for future many-core systems, are needed. In response, there are recent proposals to enhance the energy efficiency of main-memory systems by dividing a memory rank into subsets, and making a subset rather than a whole rank serve a memory request.We holistically assess the effectiveness of rank subsetting from system-wide performance, energy-efficiency, and reliability perspectives. We identify the impact of rank subsetting on memory power and processor performance analytically, compare two promising rank-subsetting proposals, Multicore DIMM and mini-rank, and verify our analysis by simulating a chip-multiprocessor system using multithreaded and consolidated workloads. We extend the design of Multicore DIMM for high-reliability systems and show that compared with conventional chipkill approaches, rank subsetting can lead to much higher system-level energy efficiency and performance at the cost of additional DRAM devices. This holistic assessment shows that rank subsetting offers compelling alternatives to existing processor-memory interfaces for future DDR systems.},
journal = {ACM Transactions on Architecture and Code Optimization},
month = mar,
articleno = {4},
numpages = {28},
keywords = {overfetch, rank subsetting, Memory system, mini-rank, DRAM, multicore DIMM}
}

@inproceedings{2012-scd-hpca,
author = {Sanchez, Daniel and Kozyrakis, Christos},
title = {SCD: A Scalable Coherence Directory with Flexible Sharer Set Encoding},
year = {2012},
month = feb,
isbn = {9781467308274},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/HPCA.2012.6168950},
abstract = {Large-scale CMPs with hundreds of cores require a directory-based protocol to maintain cache coherence. However, previously proposed coherence directories are hard to scale beyond tens of cores, requiring either excessive area or energy, complex hierarchical protocols, or inexact representations of sharer sets that increase coherence traffic and degrade performance. We present SCD, a scalable coherence directory that relies on efficient highly-associative caches (such as zcaches) to implement a single-level directory that scales to thousands of cores, tracks sharer sets exactly, and incurs negligible directory-induced invalidations. SCD scales because, unlike conventional directories, it uses a variable number of directory tags to represent sharer sets: lines with one or few sharers use a single tag, while widely shared lines use additional tags, so tags remain small as the system scales up. We show that, thanks to the efficient highly-associative array it relies on, SCD can be fully characterized using analytical models, and can be sized to guarantee a negligible number of evictions independently of the workload. We evaluate SCD using simulations of a 1024-core CMP. For the same level of coverage, we find that SCD is 13 more area-efficient than full-map sparse directories, and 2 more area-efficient and faster than hierarchical directories, while requiring a simpler protocol. Furthermore, we show that SCD's analytical models are accurate in practice.},
booktitle = {Proceedings of the 2012 IEEE 18th International Symposium on High-Performance Computer Architecture (HPCA)},
pages = {1–12},
numpages = {12},
series = {HPCA '12}
}


@inproceedings{2011-decoupling-iiswc,
author = {Delimitrou, Christina and Sankar, Sriram and Vaid, Kushagra and Kozyrakis, Christos},
title = {Decoupling Datacenter Studies from Access to Large-Scale Applications: A Modeling Approach for Storage Workloads},
year = {2011},
month = nov,
isbn = {9781457720635},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/IISWC.2011.6114196},
abstract = {The cost and power impact of suboptimal storage configurations is significant in datacenters (DCs) as inefficiencies are aggregated over several thousand servers and represent considerable losses in capital and operating costs. Designing performance, power and cost-optimized systems requires a deep understanding of target workloads, and mechanisms to effectively model different storage design choices. Traditional benchmarking is invalid in cloud data-stores, representative storage profiles are hard to obtain, while replaying the entire application in all storage configurations is impractical both from a cost and time perspective. Despite these issues, current workload generators are not able to accurately reproduce key aspects of real application patterns. Some of these features include spatial and temporal locality, as well as tuning the intensity of the workload to emulate different storage system configurations. To address these limitations, we propose a modeling and characterization framework for large-scale storage applications. As part of this framework we use a state diagram-based storage model, extend it to a hierarchical representation and implement a tool that consistently recreates I/O loads of DC applications. We present the principal features of the framework that allow accurate modeling and generation of storage workloads and the validation process performed against ten original DC applications traces. Furthermore, using our framework, we perform an in-depth, per-thread characterization of these applications and provide insights on their behavior. Finally, we explore two practical applications of this methodology: SSD caching and defragmentation benefits on enterprise storage. In both cases we observe significant speedup for most of the examined applications. Since knowledge of the workload's spatial and temporal locality is necessary to model these use cases, our framework was instrumental in quantifying their performance benefits. The proposed methodology provides a detailed understanding on the storage activity of large-scale applications and enables a wide spectrum of storage studies without the requirement for access to real applications and full application deployment.},
booktitle = {Proceedings of the 2011 IEEE International Symposium on Workload Characterization (IISWC)},
pages = {51–60},
numpages = {10},
series = {IISWC '11}
}

@inproceedings{2011-mars-mobiheld,
author = {Cidon, Asaf and London, Tomer M. and Katti, Sachin and Kozyrakis, Christos and Rosenblum, Mendel},
title = {MARS: Adaptive Remote Execution for Multi-Threaded Mobile Devices},
year = {2011},
month = oct,
isbn = {9781450309806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2043106.2043107},
abstract = {Mobile devices face a growing demand to support computationally intensive applications like 3D graphics and computer vision. However, these devices are inherently limited by processor power density and device battery life. Dynamic remote execution addresses this problem, by enabling mobile devices to opportunistically offload computations to a remote server. We envision remote execution as a new type of cloud-based heterogeneous computing resource, or a "Cloud-on-Chip", which would be managed as a system resource as if it were a local CPU, with a highly variable wireless interconnect. To realize this vision, we introduce MARS, the first adaptive, online and lightweight RPC-based remote execution scheduler supporting multi-threaded and multi-core systems. MARS uses a novel efficient offloading decision algorithm that takes into account the inherent trade-offs between communication and computation delays and power consumption. Due to its lightweight design, MARS runs on the device itself, instantly adapts its decisions to changing wireless resources, and supports any number of threads and cores. We evaluated MARS using a trace-based simulator driven by real world measurements on augmented reality, face recognition and video game applications. MARS achieves an average speedup of 57\% and 33\% higher energy savings over the best static client-server partitions.},
booktitle = {Proceedings of the 3rd ACM SOSP Workshop on Networking, Systems, and Applications on Mobile Handhelds (MobiHeld)},
articleno = {1},
numpages = {6},
location = {Cascais, Portugal},
series = {MobiHeld '11}
}

@inproceedings{2011-dynamic-pact,
author = {Sanchez, Daniel and Lo, David and Yoo, Richard M. and Sugerman, Jeremy and Kozyrakis, Christos},
title = {Dynamic Fine-Grain Scheduling of Pipeline Parallelism},
year = {2011},
motn = sep,
isbn = {9780769545660},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/PACT.2011.9},
abstract = {Scheduling pipeline-parallel programs, defined as a graph of stages that communicate explicitly through queues, is challenging. When the application is regular and the underlying architecture can guarantee predictable execution times, several techniques exist to compute highly optimized static schedules. However, these schedules do not admit run-time load balancing, so variability introduced by the application or the underlying hardware causes load imbalance, hindering performance. On the other hand, existing schemes for dynamic fine-grain load balancing (such as task-stealing) do not work well on pipeline-parallel programs: they cannot guarantee memory footprint bounds, and do not adequately schedule complex graphs or graphs with ordered queues. We present a scheduler implementation for pipeline-parallel programs that performs fine-grain dynamic load balancing efficiently. Specifically, we implement the first real runtime for GRAMPS, a recently proposed programming model that focuses on supporting irregular pipeline and data-parallel applications (in contrast to classical stream programming models and schedulers, which require programs to be regular). Task-stealing with per-stage queues and queuing policies, coupled with a backpressure mechanism, allow us to maintain strict footprint bounds, and a buffer management scheme based on packet-stealing allows low-overhead and locality-aware dynamic allocation of queue data. We evaluate our runtime on a multi-core SMP and find that it provides low-overhead scheduling of irregular workloads while maintaining locality. We also show that the GRAMPS scheduler outperforms several other commonly used scheduling approaches. Specifically, while a typical task-stealing scheduler performs on par with GRAMPS on simple graphs, it does significantly worse on complex ones, a canonical GPGPU scheduler cannot exploit pipeline parallelism and suffers from large memory footprints, and a typical static, streaming scheduler achieves somewhat better locality, but suffers significant load imbalance on a general-purpose multi-core due to fine-grain architecture variability (e.g., cache misses and SMT).},
booktitle = {Proceedings of the 2011 International Conference on Parallel Architectures and Compilation Techniques (PACT)},
pages = {22–32},
numpages = {11},
series = {PACT '11}
}

@article{2011-gp-cacm,
author = {Hameed, Rehan and Qadeer, Wajahat and Wachs, Megan and Azizi, Omid and Solomatnikov, Alex and Lee, Benjamin C. and Richardson, Stephen and Kozyrakis, Christos and Horowitz, Mark},
title = {Understanding Sources of Inefficiency in General-Purpose Chips},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10},
issn = {0001-0782},
doi = {10.1145/2001269.2001291},
abstract = {Scaling the performance of a power limited processor requires decreasing the energy expended per instruction executed, since energy/op * op/second is power. To better understand what improvement in processor efficiency is possible, and what must be done to capture it, we quantify the sources of the performance and energy overheads of a 720p HD H.264 encoder running on a general-purpose four-processor CMP system. The initial overheads are large: the CMP was 500 x less energy efficient than an Application Specific Integrated Circuit (ASIC) doing the same job. We explore methods to eliminate these overheads by transforming the CPU into a specialized system for H.264 encoding. Broadly applicable optimizations like single instruction, multiple data (SIMD) units improve CMP performance by 14 x and energy by 10x, which is still 50x worse than an ASIC. The problem is that the basic operation costs in H.264 are so small that even with a SIMD unit doing over 10 ops per cycle, 90\% of the energy is still overhead. Achieving ASIC-like performance and effciency requires algorithm-specifc optimizations. For each subalgorithm of H.264, we create a large, specialized functional/storage unit capable of executing hundreds of operations per instruction. This improves energy effciency by 160x (instead of 10x), and the final customized CMP reaches the same performance and within 3x of an ASIC solution's energy in comparable area.},
journal = {Communications of the ACM},
month = oct,
pages = {85–93},
numpages = {9}
}


@inproceedings{2011-modeling-tpctc,
author = {Delimitrou, Christina and Sankar, Sriram and Khessib, Badriddine and Vaid, Kushagra and Kozyrakis, Christos},
title = {Time and Cost-Efficient Modeling and Generation of Large-Scale TPCC/TPCE/TPCH Workloads},
year = {2011},
month = aug,
isbn = {9783642326264},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
doi = {10.1007/978-3-642-32627-1_11},
abstract = {Large-scale TPC workloads are critical for the evaluation of datacenter-scale storage systems. However, these workloads have not been previously characterized, in-depth, and modeled in a DC environment. In this work, we categorize the TPC workloads into storage threads that have unique features and characterize the storage activity of TPCC, TPCE and TPCH based on I/O traces from real server installations. We also propose a framework for modeling and generation of large-scale TPC workloads, which allows us to conduct a wide spectrum of storage experiments without requiring knowledge on the structure of the application or the overhead of fully deploying it in different storage configurations. Using our framework, we eliminate the time for TPC setup and reduce the time for experiments by two orders of magnitude, due to the compression in storage activity enforced by the model. We demonstrate the accuracy of the model and the applicability of our method to significant datacenter storage challenges, including identification of early disk errors, and SSD caching.},
booktitle = {Proceedings of the 3rd TPC Technology Conference on Topics in Performance Evaluation, Measurement and Characterization (TPC)},
pages = {146–162},
numpages = {17},
keywords = {modeling, datacenter, storage configuration, storage traces, characterization, workload, TPC benchmarks},
location = {Seattle, WA},
series = {TPCTC'11}
}

@article{2011-ramcloud-cacm,
author = {Ousterhout, John and Agrawal, Parag and Erickson, David and Kozyrakis, Christos and Leverich, Jacob and Mazi\`{e}res, David and Mitra, Subhasish and Narayanan, Aravind and Ongaro, Diego and Parulkar, Guru and Rosenblum, Mendel and Rumble, Stephen M. and Stratmann, Eric and Stutsman, Ryan},
title = {The Case for RAMCloud},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {7},
issn = {0001-0782},
doi = {10.1145/1965724.1965751},
abstract = {With scalable high-performance storage entirely in DRAM, RAMCloud will enable a new breed of data-intensive applications.},
journal = {Communications of the ACM},
month = jul,
pages = {121–130},
numpages = {10}
}


@inproceedings{2011-modeling-icdcs,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Cross-Examination of Datacenter Workload Modeling Techniques},
year = {2011},
month = jun,
isbn = {9780769543864},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/ICDCSW.2011.45},
abstract = {Data center workload modeling has become a necessity in recent years due to the emergence of large-scale applications and cloud data-stores, whose implementation remains largely unknown. Detailed knowledge of target workloads is critical in order to correctly provision performance, power and cost-optimized systems. In this work we aggregate previous work on data center workload modeling and perform a qualitative comparison based on the representativeness, accuracy and completeness of these designs. We categorize modeling techniques in two main approaches, in-breadth and in-depth, based on the way they address the modeling of the workload. The former models the behavior of a workload in specific system parts, while the latter traces a user request throughout its execution. Furthermore, we propose the early concept of a new design, which bridges the gap between these two approaches by combining some features from each one. Some first results on the request features and performance metrics of the generated workload based on this design appear promising as far as the accuracy of the model is concerned.},
booktitle = {Proceedings of the  31st International Conference on Distributed Computing Systems Workshops (ICDCS)},
pages = {72–79},
numpages = {8},
keywords = {datacenter, workload, applications, modeling},
series = {ICDCSW '11}
}



@inproceedings{2011-phoenix-mapreduces,
author = {Talbot, Justin and Yoo, Richard M. and Kozyrakis, Christos},
title = {Phoenix++: Modular MapReduce for Shared-Memory Systems},
year = {2011},
month = jun,
isbn = {9781450307000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1996092.1996095},
abstract = {This paper describes our rewrite of Phoenix, a MapReduce framework for shared-memory CMPs and SMPs. Despite successfully demonstrating the applicability of a MapReduce-style pipeline to shared-memory machines, Phoenix has a number of limitations; its uniform intermediate storage of key-value pairs, inefficient combiner implementation, and poor task overhead amortization fail to efficiently support a wide range of MapReduce applications, encouraging users to manually circumvent the framework. We describe an alternative implementation, Phoenix++, that provides a modular, flexible pipeline that can be easily adapted by the user to the characteristics of a particular workload. Compared to Phoenix, this new approach achieves a 4.7-fold performance improvement and increased scalability, while allowing users to write simple, strict MapReduce code.},
booktitle = {Proceedings of the 2nd International Workshop on MapReduce and Its Applications (MapReduce)},
pages = {9–16},
numpages = {8},
keywords = {modularity, performance, shared-memory MapReduce},
location = {San Jose, California, USA},
series = {MapReduce '11}
}


@inproceedings{2011-vantage-isca,
author = {Sanchez, Daniel and Kozyrakis, Christos},
title = {Vantage: Scalable and Efficient Fine-Grain Cache Partitioning},
year = {2011},
month = jun,
isbn = {9781450304726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2000064.2000073},
abstract = {Cache partitioning has a wide range of uses in CMPs, from guaranteeing quality of service and controlled sharing to security-related techniques. However, existing cache partitioning schemes (such as way-partitioning) are limited to coarse-grain allocations, can only support few partitions, and reduce cache associativity, hurting performance. Hence, these techniques can only be applied to CMPs with 2-4 cores, but fail to scale to tens of cores.We present Vantage, a novel cache partitioning technique that overcomes the limitations of existing schemes: caches can have tens of partitions with sizes specified at cache line granularity, while maintaining high associativity and strong isolation among partitions. Vantage leverages cache arrays with good hashing and associativity, which enable soft-pinning a large portion of cache lines. It enforces capacity allocations by controlling the replacement process. Unlike prior schemes, Vantage provides strict isolation guarantees by partitioning most (e.g. 90\%) of the cache instead of all of it. Vantage is derived from analytical models, which allow us to provide strong guarantees and bounds on associativity and sizing independent of the number of partitions and their behaviors. It is simple to implement, requiring around 1.5\% state overhead and simple changes to the cache controller.We evaluate Vantage using extensive simulations. On a 32-core system, using 350 multiprogrammed workloads and one partition per core, partitioning the last-level cache with conventional techniques degrades throughput for 71\% of the workloads versus an unpartitioned cache (by 7\% average, 25\% maximum degradation), even when using 64-way caches. In contrast, Vantage improves throughput for 98\% of the workloads, by 8\% on average (up to 20\%), using a 4-way cache.},
booktitle = {Proceedings of the 38th  International Symposium on Computer Architecture (ISCA)},
pages = {57–68},
numpages = {12},
keywords = {qos, shared cache, multi-core, cache partitioning},
location = {San Jose, California, USA},
series = {ISCA '11}
}


@inproceedings{2011-gen-ispass,
author = {Delimitrou, Christina and Sankar, Sriram and Vaid, Kushagra and Kozyrakis, Christos},
title = {Storage I/O Generation and Replay for Datacenter Applications},
year = {2011},
month = apr,
isbn = {9781612843674},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the advent of social networking and cloud data-stores, user data is increasingly being stored in large capacity and high performance storage systems, which account for a significant portion of the total cost of ownership of a datacenter (DC) [3]. One of the main challenges when trying to evaluate storage system options is the difficulty in replaying the entire application in all possible system configurations. Furthermore, code and datasets of DC applications are rarely available to storage system designers. This makes the development of a representative model that captures key aspects of the workload's storage profile, even more appealing. Once such a model is available, the next step is to create a tool that convincingly reproduces the application's storage behavior via a synthetic I/O access pattern.},
booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
pages = {123–124},
numpages = {2},
series = {ISPASS '11}
}



@inproceedings{2011-tm-asplos,
author = {Casper, Jared and Oguntebi, Tayo and Hong, Sungpack and Bronson, Nathan G. and Kozyrakis, Christos and Olukotun, Kunle},
title = {Hardware Acceleration of Transactional Memory on Commodity Systems},
year = {2011},
month = mar,
isbn = {9781450302661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1950365.1950372},
abstract = {The adoption of transactional memory is hindered by the high overhead of software transactional memory and the intrusive design changes required by previously proposed TM hardware. We propose that hardware to accelerate software transactional memory (STM) can reside outside an unmodified commodity processor core, thereby substantially reducing implementation costs. This paper introduces Transactional Memory Acceleration using Commodity Cores (TMACC), a hardware-accelerated TM system that does not modify the processor, caches, or coherence protocol.We present a complete hardware implementation of TMACC using a rapid prototyping platform. Using this hardware, we implement two unique conflict detection schemes which are accelerated using Bloom filters on an FPGA. These schemes employ novel techniques for tolerating the latency of fine-grained asynchronous communication with an out-of-core accelerator. We then conduct experiments to explore the feasibility of accelerating TM without modifying existing system hardware. We show that, for all but short transactions, it is not necessary to modify the processor to obtain substantial improvement in TM performance. In these cases, TMACC outperforms an STM by an average of 69\% in applications using moderate-length transactions, showing maximum speedup within 8\% of an upper bound on TM acceleration. Overall, we demonstrate that hardware can substantially accelerate the performance of an STM on unmodified commodity processors.},
booktitle = {Proceedings of the 16th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {27–38},
numpages = {12},
keywords = {fpga, hardware acceleration, transactional memory},
location = {Newport Beach, California, USA},
series = {ASPLOS XVI}
}

@inproceedings{2010-zcache-toppicks,
author = {Sanchez, Daniel and Kozyrakis, Christos},
title = {The ZCache: Decoupling Ways and Associativity},
year = {2010},
month = oct,
isbn = {9780769542997},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/MICRO.2010.20},
abstract = {The ever-increasing importance of main memory latency and bandwidth is pushing CMPs towards caches with higher capacity and associativity. Associativity is typically improved by increasing the number of ways. This reduces conflict misses, but increases hit latency and energy, placing a stringent trade-off on cache design. We present the zcache, a cache design that allows much higher associativity than the number of physical ways (e.g. a 64-associative cache with 4 ways). The zcache draws on previous research on skew-associative caches and cuckoo hashing. Hits, the common case, require a single lookup, incurring the latency and energy costs of a cache with a very low number of ways. On a miss, additional tag lookups happen off the critical path, yielding an arbitrarily large number of replacement candidates for the incoming block. Unlike conventional designs, the zcache provides associativity by increasing the number of replacement candidates, but not the number of cache ways. To understand the implications of this approach, we develop a general analysis framework that allows to compare associativity across different cache designs (e.g. a set-associative cache and a zcache) by representing associativity as a probability distribution. We use this framework to show that for zcaches, associativity depends only on the number of replacement candidates, and is independent of other factors (such as the number of cache ways or the workload). We also show that, for the same number of replacement candidates, the associativity of a zcache is superior than that of a set-associative cache for most workloads. Finally, we perform detailed simulations of multithreaded and multiprogrammed workloads on a large-scale CMP with zcache as the last-level cache. We show that zcaches provide higher performance and better energy efficiency than conventional caches without incurring the overheads of designs with a large number of ways.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Symposium on Microarchitecture (MICRO)},
pages = {187–198},
numpages = {12},
keywords = {multi-core, performance, cache, energy efficiency, associativity},
series = {MICRO '43}
}

@inproceedings{2010-eigenbench-iiswc,
author = {Sungpack Hong and Oguntebi, Tayo and Casper, Jared and Bronson, Nathan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Eigenbench: A Simple Exploration Tool for Orthogonal TM Characteristics},
year = {2010},
month = sep,
isbn = {9781424492978},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/IISWC.2010.5648812},
abstract = {There are a significant number of Transactional Memory(TM) proposals, varying in almost all aspects of the design space. Although several transactional benchmarks have been suggested, a simple, yet thorough, evaluation framework is still needed to completely characterize a TM system and allow for comparison among the various proposals. Unfortunately, TM system evaluation is difficult because the application characteristics which affect performance are often difficult to isolate from each other. We propose a set of orthogonal application characteristics that form a basis for transactional behavior and are useful in fully understanding the performance of a TM system. In this paper, we present EigenBench, a lightweight yet powerful microbenchmark for fully evaluating a transactional memory system. We show that EigenBench is useful for thoroughly exploring the orthogonal space of TM application characteristics. Because of its flexibility, our microbenchmark is also capable of reproducing a representative set of TM performance pathologies. In this paper, we use Eigenbench to evaluate two well-known TM systems and provide significant insight about their strengths and weaknesses. We also demonstrate how EigenBench can be used to mimic the evaluation coverage of a popular TM benchmark suite called STAM.},
booktitle = {Proceedings of the IEEE International Symposium on Workload Characterization (IISWC)},
pages = {1–11},
numpages = {11},
series = {IISWC '10}
}

@article{2010-insights-micro,
author = {Kozyrakis, Christos and Kansal, Aman and Sankar, Sriram and Vaid, Kushagra},
title = {Server Engineering Insights for Large-Scale Online Services},
year = {2010},
issue_date = {July 2010},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {30},
number = {4},
issn = {0272-1732},
doi = {10.1109/MM.2010.73},
abstract = {The rapid growth of online services in the last decade has led to the development of large data centers to host these workloads. These large-scale online, user-facing services have unique engineering and capacity provisioning design requirements. The authors explore these requirements, focusing on system balancing, the impact of technology trends, and the challenges of online service workloads.},
journal = {IEEE Micro},
month = jul,
pages = {8–19},
numpages = {12},
keywords = {server balancing, data centers, hardware, online services, data centers, large-scale production services, server design, server balancing, scale-out design, total cost of ownership, hardware, scale-out design, server design, total cost of ownership, online services, large-scale production services}
}


@inproceedings{2010-gp-isca,
author = {Hameed, Rehan and Qadeer, Wajahat and Wachs, Megan and Azizi, Omid and Solomatnikov, Alex and Lee, Benjamin C. and Richardson, Stephen and Kozyrakis, Christos and Horowitz, Mark},
title = {Understanding Sources of Inefficiency in General-Purpose Chips},
year = {2010},
month = jun,
isbn = {9781450300537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1815961.1815968},
abstract = {Due to their high volume, general-purpose processors, and now chip multiprocessors (CMPs), are much more cost effective than ASICs, but lag significantly in terms of performance and energy efficiency. This paper explores the sources of these performance and energy overheads in general-purpose processing systems by quantifying the overheads of a 720p HD H.264 encoder running on a general-purpose CMP system. It then explores methods to eliminate these overheads by transforming the CPU into a specialized system for H.264 encoding. We evaluate the gains from customizations useful to broad classes of algorithms, such as SIMD units, as well as those specific to particular computation, such as customized storage and functional units.The ASIC is 500x more energy efficient than our original four-processor CMP. Broadly applicable optimizations improve performance by 10x and energy by 7x. However, the very low energy costs of actual core ops (100s fJ in 90nm) mean that over 90\% of the energy used in these solutions is still "overhead". Achieving ASIC-like performance and efficiency requires algorithm-specific optimizations. For each sub-algorithm of H.264, we create a large, specialized functional unit that is capable of executing 100s of operations per instruction. This improves performance and energy by an additional 25x and the final customized CMP matches an ASIC solution's performance within 3x of its energy and within comparable area.},
booktitle = {Proceedings of the 37th International Symposium on Computer Architecture (ISCA)},
pages = {37–47},
numpages = {11},
keywords = {chip multiprocessor, ASIC, tensilica, energy efficiency, high performance, customization, h.264},
location = {Saint-Malo, France},
series = {ISCA '10}
}


@inproceedings{2010-nested-spaa,
author = {Baek, Woongki and Bronson, Nathan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Implementing and Evaluating Nested Parallel Transactions in Software Transactional Memory},
year = {2010},
month = jun,
isbn = {9781450300797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1810479.1810528},
abstract = {Transactional Memory (TM) is a promising technique that simplifies parallel programming for shared-memory applications. To date, most TM systems have been designed to efficiently support single-level parallelism. To achieve widespread use and maximize performance gains, TM must support nested parallelism available in many applications and supported by several programming models.We present NesTM, a software TM (STM) system that supports closed-nested parallel transactions. NesTM is based on a high-performance, blocking STM that uses eager version management and word-granularity conflict detection. Its algorithm targets the state and runtime overheads of nested parallel transactions. We also describe several subtle correctness issues in supporting nested parallel transactions in NesTM and discuss their performance impact.Through our evaluation, we quantitatively analyze the performance of NesTM using STAMP applications and microbenchmarks based on concurrent data structures. First, we show that the performance overhead of NesTM is reasonable when single-level parallelism is used. Second, we quantify the incremental overhead of NesTM when the parallelism is exploited in deeper nesting levels and draw conclusions that can be useful in designing a nesting-aware TM runtime environment. Finally, we demonstrate a use-case where nested parallelism improves the performance of a transactional microbenchmark.},
booktitle = {Proceedings of the 22nd ACM Symposium on Parallelism in Algorithms and Architectures (SPAA)},
pages = {253–262},
numpages = {10},
keywords = {transactional memory, parallel programming, nested parallelism},
location = {Thira, Santorini, Greece},
series = {SPAA '10}
}

@inproceedings{2010-nested-ics,
author = {Baek, Woongki and Bronson, Nathan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Making Nested Parallel Transactions Practical Using Lightweight Hardware Support},
year = {2010},
month = jun,
isbn = {9781450300186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1810085.1810097},
abstract = {Transactional Memory (TM) simplifies parallel programming by supporting parallel tasks that execute in an atomic and isolated way. To achieve the best possible performance, TM must support the nested parallelism available in real-world applications and supported by popular programming models. A few recent papers have proposed support for nested parallelism in software TM (STM) and hardware TM (HTM). However, the proposed designs are still impractical, as they either introduce excessive runtime overheads or require complex hardware structures.This paper presents filter-accelerated, nested TM (FaNTM). We extend a hybrid TM based on hardware signatures to provide practical support for nested parallel transactions. In the FaNTM design, hardware filters provide continuous and nesting-aware conflict detection, which effectively eliminates the excessive overheads of software nested transactions. In contrast to a full HTM approach, FaNTM simplifies hardware by decoupling nested parallel transactions from caches using hardware filters. We also describe subtle correctness and liveness issues that do not exist in the non-nested baseline TM.We quantify the performance of FaNTM using STAMP applications and microbenchmarks that use concurrent data structures. First, we demonstrate that the runtime overhead of FaNTM is small (2.3\% on average) when applications use only single-level parallelism. Second, we show that the incremental performance overhead of FaNTM is reasonable when the available parallelism is used in deeper nesting levels. We also demonstrate that nested parallel transactions on FaNTM run significantly faster (e.g., 12.4x) than those on a nested STM. Finally, we show how nested parallelism is used to improve the overall performance of a transactional microbenchmark.},
booktitle = {Proceedings of the 24th ACM International Conference on Supercomputing (ICS)},
pages = {61–71},
numpages = {11},
keywords = {transactional memory, parallel programming, nested parallelism},
location = {Tsukuba, Ibaraki, Japan},
series = {ICS '10}
}

@article{2010-interconnect-taco,
author = {Sanchez, Daniel and Michelogiannakis, George and Kozyrakis, Christos},
title = {An Analysis of On-Chip Interconnection Networks for Large-Scale Chip Multiprocessors},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {1544-3566},
doi = {10.1145/1736065.1736069},
abstract = {With the number of cores of chip multiprocessors (CMPs) rapidly growing as technology scales down, connecting the different components of a CMP in a scalable and efficient way becomes increasingly challenging. In this article, we explore the architectural-level implications of interconnection network design for CMPs with up to 128 fine-grain multithreaded cores. We evaluate and compare different network topologies using accurate simulation of the full chip, including the memory hierarchy and interconnect, and using a diverse set of scientific and engineering workloads.We find that the interconnect has a large impact on performance, as it is responsible for 60\% to 75\% of the miss latency. Latency, and not bandwidth, is the primary performance constraint, since, even with many threads per core and workloads with high miss rates, networks with enough bandwidth can be efficiently implemented for the system scales we consider. From the topologies we study, the flattened butterfly consistently outperforms the mesh and fat tree on all workloads, leading to performance advantages of up to 22\%. We also show that considering interconnect and memory hierarchy together when designing large-scale CMPs is crucial, and neglecting either of the two can lead to incorrect conclusions. Finally, the effect of the interconnect on overall performance becomes more important as the number of cores increases, making interconnection choices especially critical when scaling up.},
journal = {ACM Transactions on Architure and Code Optimization},
month = may,
articleno = {4},
numpages = {28},
keywords = {Networks-on-chip, hierarchical networks, chip multiprocessors}
}

@inproceedings{2010-bufferless-nocs,
author = {Michelogiannakis, George and Sanchez, Daniel and Dally, William J. and Kozyrakis, Christos},
title = {Evaluating Bufferless Flow Control for On-Chip Networks},
year = {2010},
month = may,
isbn = {9780769540535},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/NOCS.2010.10},
abstract = {With the emergence of on-chip networks, the power consumed by router buffers has become a primary concern. Bufferless flow control addresses this issue by removing router buffers, and handles contention by dropping or deflecting flits. This work compares virtual-channel (buffered) and deflection (packet-switched bufferless) flow control. Our evaluation includes optimizations for both schemes: buffered networks use custom SRAM-based buffers and empty buffer bypassing for energy efficiency, while bufferless networks feature a novel routing scheme that reduces average latency by 5\%. Results show that unless process constraints lead to excessively costly buffers, the performance, cost and increased complexity of deflection flow control outweigh its potential gains: bufferless designs are only marginally (up to 1.5\%) more energy efficient at very light loads, and buffered networks provide lower latency and higher throughput per unit power under most conditions.},
booktitle = {Proceedings of the 4th ACM/IEEE International Symposium on Networks-on-Chip (NOCS)},
pages = {9–16},
numpages = {8},
keywords = {Networks, Multiprocessor interconnection, Flow control, Buffers},
series = {NOCS '10}
}

@inproceedings{2010-farm-fccm,
author = {Oguntebi, Tayo and Hong, Sungpack and Casper, Jared and Bronson, Nathan and Kozyrakis, Christos and Olukotun, Kunle},
title = {FARM: A Prototyping Environment for Tightly-Coupled, Heterogeneous Architectures},
year = {2010},
month = may,
isbn = {9780769540566},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/FCCM.2010.41},
abstract = {Computer architectures are increasingly turning to parallelism and heterogeneity as solutions for boosting performance in the face of power constraints. As this trend continues, the challenges of simulating and evaluating these architectures have grown. Hardware prototypes provide deeper insight into these systems when compared to simulators, but are traditionally more difficult and costly to build. We present the Flexible Architecture Research Machine (FARM), a hardware prototyping system based on an FPGA coherently connected to a multiprocessor system. FARM substantially reduces the difficulty and cost of building hardware prototypes by providing a ready-made framework for communicating with a custom design on the FPGA. FARM ensures efficient, low-latency communication with the FPGA via a variety of mechanisms, allowing a wide range of applications to effectively utilize the system. FARM’s coherent FPGA includes a cache and participates in coherence activities with the processors. This tight coupling allows for realistic, innovative architecture prototypes that would otherwise be extremely difficult to simulate. We evaluate FARM by providing the reader with a profile of the overheads introduced across the full range of communication mechanisms. This will guide the potential FARM user towards an optimal configuration when designing his prototype.},
booktitle = {Proceedings of the  18th IEEE International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
pages = {221–228},
numpages = {8},
keywords = {coprocessors, prototyping, FPGA communication, HyperTransport, accelerators, coherent FPGA},
series = {FCCM '10}
}


@article{2010-pointless-osr,
author = {Dalton, Michael and Kannan, Hari and Kozyrakis, Christos},
title = {Tainting is Not Pointless},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0163-5980},
doi = {10.1145/1773912.1773933},
abstract = {Pointer tainting is a form of Dynamic Information Flow Tracking used primarily to prevent software security attacks such as buffer overflows. Researchers have also applied pointer tainting to malware and virus analysis.A recent paper by Slowinska and Bos has criticized pointer tainting as a security mechanism, arguing that it is has serious, inherent false positive and false negative defects. We present a rebuttal that addresses the confusion due to the two uses of pointer tainting in security literature. We clarify that many of the arguments against pointer tainting apply only to its use as a malware and virus analysis platform, but do not apply to the application of pointer tainting to memory corruption protection. Hence, we argue that pointer tainting remains a useful and promising technique for robust protection against memory corruption attacks.},
journal = {SIGOPS Operating Systems Review (OSR)},
month = apr,
pages = {88–92},
numpages = {5},
keywords = {memory corruption, software security, virus detection, malware detection, dynamic information flow tracking, pointer tainting, buffer overflow}
}

@inproceedings{2010-checker-iceccs,
author = {Baek, Woongki and Bronson, Nathan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Implementing and Evaluating a Model Checker for Transactional Memory Systems},
year = {2010},
month = mar,
isbn = {9780769540153},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/ICECCS.2010.30},
abstract = {Transactional Memory (TM) is a promising technique that addresses the difficulty of parallel programming. Since TM takes responsibility for all concurrency control, TM systems are highly vulnerable to subtle correctness errors. Due to the difficulty of fully proving the correctness of TM systems, many of them are used without any formal correctness guarantees. This paper presents ChkTM, a flexible model checking environment to verify the correctness of various TM systems. ChkTM aims to model TM systems close to the implementation level to reveal as many potential bugs as possible. For example, ChkTM accurately models the version control mechanism in timestamp-based software TMs (STMs). In addition, ChkTM can flexibly model TM systems that use additional hardware components or support nested parallelism. Using ChkTM, we model several TM systems including a widely-used industrial STM (TL2), a hybrid TM (SigTM) that uses hardware signatures, and an STM (NesTM) that supports nested parallel transactions. We then demonstrate how ChkTM can be used to find a previously unreported correctness bug in the current implementation of eager-versioning TL2. We also verify the serializability of TL2 and SigTM and strong isolation guarantees of SigTM. Finally, we quantitatively analyze ChkTM to understand the practical issues and motivate further research in model checking TM systems.},
booktitle = {Proceedings of the 2010 15th IEEE International Conference on Engineering of Complex Computer Systems (ICECCS)},
pages = {117–126},
numpages = {10},
keywords = {Transactional Memory, Model Checking},
series = {ICECCS '10}
}

@inproceedings{2010-adm-asplos,
author = {Sanchez, Daniel and Yoo, Richard M. and Kozyrakis, Christos},
title = {Flexible Architectural Support for Fine-Grain Scheduling},
year = {2010},
month = mar,
isbn = {9781605588391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1736020.1736055},
abstract = {To make efficient use of CMPs with tens to hundreds of cores, it is often necessary to exploit fine-grain parallelism. However, managing tasks of a few thousand instructions is particularly challenging, as the runtime must ensure load balance without compromising locality and introducing small overheads. Software-only schedulers can implement various scheduling algorithms that match the characteristics of different applications and programming models, but suffer significant overheads as they synchronize and communicate task information over the deep cache hierarchy of a large-scale CMP. To reduce these costs, hardware-only schedulers like Carbon, which implement task queuing and scheduling in hardware, have been proposed. However, a hardware-only solution fixes the scheduling algorithm and leaves no room for other uses of the custom hardware.This paper presents a combined hardware-software approach to build fine-grain schedulers that retain the flexibility of software schedulers while being as fast and scalable as hardware ones. We propose asynchronous direct messages (ADM), a simple architectural extension that provides direct exchange of asynchronous, short messages between threads in the CMP without going through the memory hierarchy. ADM is sufficient to implement a family of novel, software-mostly schedulers that rely on low-overhead messaging to efficiently coordinate scheduling and transfer task information. These schedulers match and often exceed the performance and scalability of Carbon when using the same scheduling algorithm. When the ADM runtime tailors its scheduling algorithm to application characteristics, it outperforms Carbon by up to 70\%.},
booktitle = {Proceedings of the 15th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {311–322},
numpages = {12},
keywords = {many-core, scheduling, fine-grain scheduling, messaging, work-stealing, chip-multiprocessors},
location = {Pittsburgh, Pennsylvania, USA},
series = {ASPLOS XV}
}


@article{2010-gating-cal,
author = {Leverich, Jacob and Monchiero, Matteo and Talwar, Vanish and Ranganathan, Parthasarathy and Kozyrakis, Christos},
title = {Power Management of Datacenter Workloads Using Per-Core Power Gating},
year = {2009},
issue_date = {July 2009},
publisher = {IEEE Computer Society},
address = {USA},
volume = {8},
number = {2},
issn = {1556-6056},
doi = {10.1109/L-CA.2009.46},
abstract = {While modern processors offer a wide spectrum of software-controlled power modes, most datacenters only rely on Dynamic Voltage and Frequency Scaling (DVFS, a.k.a. P-states) to achieve energy efficiency. This paper argues that, in the case of datacenter workloads, DVFS is not the only option for processor power management. We make the case for per-core power gating (PCPG) as an additional power management knob for multi-core processors. PCPG is the ability to cut the voltage supply to selected cores, thus reducing to almost zero the leakage power for the gated cores. Using a testbed based on a commercial 4-core chip and a set of real-world application traces from enterprise environments, we have evaluated the potential of PCPG. We show that PCPG can significantly reduce a processor's energy consumption (up to 40\%) without significant performance overheads. When compared to DVFS, PCPG is highly effective saving up to 30\% more energy than DVFS. When DVFS and PCPG operate together they can save up to almost 60\%.},
journal = {IEEE Computer Architure Letters (CAL)},
month = jul,
pages = {48–51},
numpages = {4},
keywords = {System architectures, integration and modeling, Energy-aware systems}
}



@article{2010-ramcloud-osr,
author = {Ousterhout, John and Agrawal, Parag and Erickson, David and Kozyrakis, Christos and Leverich, Jacob and Mazi\`{e}res, David and Mitra, Subhasish and Narayanan, Aravind and Parulkar, Guru and Rosenblum, Mendel and Rumble, Stephen M. and Stratmann, Eric and Stutsman, Ryan},
title = {The Case for RAMClouds: Scalable High-Performance Storage Entirely in DRAM},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0163-5980},
doi = {10.1145/1713254.1713276},
abstract = {Disk-oriented approaches to online storage are becoming increasingly problematic: they do not scale gracefully to meet the needs of large-scale Web applications, and improvements in disk capacity have far outstripped improvements in access latency and bandwidth. This paper argues for a new approach to datacenter storage called RAMCloud, where information is kept entirely in DRAM and large-scale systems are created by aggregating the main memories of thousands of commodity servers. We believe that RAMClouds can provide durable and available storage with 100-1000x the throughput of disk-based systems and 100-1000x lower access latency. The combination of low latency and large scale will enable a new breed of dataintensive applications.},
journal = {SIGOPS Operating Systems Review (OSR)},
month = jan,
pages = {92–105},
numpages = {14}
}




@inproceedings{2009-dram-sc,
author = {Ahn, Jung Ho and Jouppi, Norman P. and Kozyrakis, Christos and Leverich, Jacob and Schreiber, Robert S.},
title = {Future Scaling of Processor-Memory Interfaces},
year = {2009},
month = Nov,
isbn = {9781605587448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1654059.1654102},
abstract = {Continuous evolution in process technology brings energy-efficiency and reliability challenges, which are harder for memory system designs since chip multiprocessors demand high bandwidth and capacity, global wires improve slowly, and more cells are susceptible to hard and soft errors. Recently, there are proposals aiming at better main-memory energy efficiency by dividing a memory rank into subsets.We holistically assess the effectiveness of rank subsetting in the context of system-wide performance, energy-efficiency, and reliability perspectives. We identify the impact of rank subsetting on memory power and processor performance analytically, then verify the analyses by simulating a chipmultiprocessor system using multithreaded and consolidated workloads. We extend the design of Multicore DIMM, one proposal embodying rank subsetting, for high-reliability systems and show that compared with conventional chipkill approaches, it can lead to much higher system-level energy efficiency and performance at the cost of additional DRAM devices.},
booktitle = {Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis (SC)},
articleno = {42},
numpages = {12},
location = {Portland, Oregon},
series = {SC '09}
}

@article{2010-hadoop-osr,
author = {Leverich, Jacob and Kozyrakis, Christos},
title = {On the Energy (in)Efficiency of Hadoop Clusters},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5980},
doi = {10.1145/1740390.1740405},
abstract = {Distributed processing frameworks, such as Yahoo!'s Hadoop and Google's MapReduce, have been successful at harnessing expansive datacenter resources for large-scale data analysis. However, their effect on datacenter energy efficiency has not been scrutinized. Moreover, the filesystem component of these frameworks effectively precludes scale-down of clusters deploying these frameworks (i.e. operating at reduced capacity). This paper presents our early work on modifying Hadoop to allow scale-down of operational clusters. We find that running Hadoop clusters in fractional configurations can save between 9\% and 50\% of energy consumption, and that there is a tradeoff between performance energy consumption. We also outline further research into the energy-efficiency of these frameworks.},
journal = {SIGOPS Operating Systems Review},
month = mar,
pages = {61–65},
numpages = {5}
}
@InProceedings{2009-ppl-hotchips,
  author = 	 {Christos Kozyrakis, Kunle Olukotun},
  title = 	 { The Stanford Pervasive Parallelism Lab},
  booktitle = {Technical Record of the 21st Hot Chips Symposium},
  year = 	 2009,
  month = 	 Aug}


@inproceedings{2009-phoenix-iiswc,
author = {Yoo, Richard M. and Romano, Anthony and Kozyrakis, Christos},
title = {Phoenix Rebirth: Scalable MapReduce on a Large-Scale Shared-Memory System},
year = {2009},
month = Oct,
isbn = {9781424451562},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/IISWC.2009.5306783},
abstract = {Dynamic runtimes can simplify parallel programming by automatically managing concurrency and locality without further burdening the programmer. Nevertheless, implementing such runtime systems for large-scale, shared-memory systems can be challenging. This work optimizes Phoenix, a MapReduce runtime for shared-memory multi-cores and multiprocessors, on a quad-chip, 32-core, 256-thread UltraSPARC T2+ system with NUMA characteristics. We show how a multi-layered approach that comprises optimizations on the algorithm, implementation, and OS interaction leads to significant speedup improvements with 256 threads (average of 2.5 higher speedup, maximum of 19 ). We also identify the roadblocks that limit the scalability of parallel runtimes on shared-memory systems, which are inherently tied to the OS scalability on large-scale systems.},
booktitle = {Proceedings of the 2009 IEEE International Symposium on Workload Characterization (IISWC)},
pages = {198–207},
numpages = {10},
series = {IISWC '09}
}



@inproceedings{2009-nemesis-usenixsc,
author = {Dalton, Michael and Kozyrakis, Christos and Zeldovich, Nickolai},
title = {Nemesis: Preventing Authentication &amp; Access Control Vulnerabilities in Web Applications},
year = {2009},
month = aug,
publisher = {USENIX Association},
address = {USA},
abstract = {This paper presents Nemesis, a novel methodology for mitigating authentication bypass and access control vulnerabilities in existing web applications. Authentication attacks occur when a web application authenticates users unsafely, granting access to web clients that lack the appropriate credentials. Access control attacks occur when an access control check in the web application is incorrect or missing, allowing users unauthorized access to privileged resources such as databases and files. Such attacks are becoming increasingly common, and have occurred in many high-profile applications, such as IIS [10] and WordPress [31], as well as 14\% of surveyed web sites [30]. Nevertheless, none of the currently available tools can fully mitigate these attacks.Nemesis automatically determines when an application safely and correctly authenticates users, by using Dynamic Information Flow Tracking (DIFT) techniques to track the flow of user credentials through the application's language runtime. Nemesis combines authentication information with programmer-supplied access control rules on files and database entries to automatically ensure that only properly authenticated users are granted access to any privileged resources or data. A study of seven popular web applications demonstrates that a prototype of Nemesis is effective at mitigating attacks, requires little programmer effort, and imposes minimal runtime overhead. Finally, we show that Nemesis can also improve the precision of existing security tools, such as DIFT analyses for SQL injection prevention, by providing runtime information about user authentication.},
booktitle = {Proceedings of the 18th USENIX Security Symposium (UsenixSec)},
pages = {267–282},
numpages = {16},
location = {Montreal, Canada},
series = {SSYM'09}
}


@INPROCEEDINGS{2009-dift-dsn,
author={Kannan, Hari and Dalton, Michael and Kozyrakis, Christos},
booktitle={Proceedings of the IEEE/IFIP International Conference on Dependable Systems Networks (DSN)}, 
title={Decoupling Dynamic Information Flow Tracking with a dedicated coprocessor},
year={2009},
month = jun,
volume={},
number={},
pages={105-114},
abstract={Dynamic information flow tracking (DIFT) is a promising security technique. With hardware support, DIFT prevents a wide range of attacks on vulnerable software with minimal performance impact. DIFT architectures, however, require significant changes in the processor pipeline that increase design and verification complexity and may affect clock frequency. These complications deter hardware vendors from supporting DIFT. This paper makes hardware support for DIFT cost effective by decoupling DIFT functionality onto a simple, separate coprocessor. Decoupling is possible because DIFT operations and regular computation need only synchronize on system calls. The coprocessor is a small hardware engine that performs logical operations and caches 4-bit tags. It introduces no changes to the design or layout of the main processor's logic, pipeline, or caches, and can be combined with various processors. Using a full-system hardware prototype and realistic Linux workloads, we show that the DIFT coprocessor provides the same security guarantees as current DIFT architectures with low runtime overheads.},
keywords={Coprocessors;Hardware;Pipelines;Information security;Software performance;Computer architecture;Clocks;Cost function;Frequency synchronization;Engines;Software security;Semantic Vulnerabilities;Dynamic information flow tracking;Processor architecture;Coprocessors},
doi={10.1109/DSN.2009.5270347},
ISSN={2158-3927},
month=Jun}



@inproceedings{2009-smart-isca,
author = {Firoozshahian, Amin and Solomatnikov, Alex and Shacham, Ofer and Asgar, Zain and Richardson, Stephen and Kozyrakis, Christos and Horowitz, Mark},
title = {A Memory System Design Framework: Creating Smart Memories},
year = {2009},
month = jun,
isbn = {9781605585260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1555754.1555805},
abstract = {As CPU cores become building blocks, we see a great expansion in the types of on-chip memory systems proposed for CMPs. Unfortunately, designing the cache and protocol controllers to support these memory systems is complex, and their concurrency and latency characteristics significantly affect the performance of any CMP. To address this problem, this paper presents a microarchitecture framework for cache and protocol controllers, which can aid in generating the RTL for new memory systems. The framework consists of three pipelined engines' request-tracking, state-manipulation, and data movement' which are programmed to implement a higher-level memory model. This approach simplifies the design and verification of CMP systems by decomposing the memory model into sequences of state and data manipulations. Moreover, implementing the framework itself produces a polymorphic memory system.To validate the approach, we implemented a scalable, flexible CMP in silicon. The memory system was then programmed to support three disparate memory models' cache coherent shared memory, streams and transactional memory. Measured overheads of this approach seem promising. Our system generates controllers with performance overheads of less than 20\% compared to an ideal controller with zero internal latency. Even the overhead of directly implementing a fully programmable controller was modest. While it did double the controller's area, the amortized effective area in the system grew by roughly 7\%.},
booktitle = {Proceedings of the 36th International Symposium on Computer Architecture (ISCA)},
pages = {406–417},
numpages = {12},
keywords = {memory systems, reconfigurable architecture, stream programming, cache coherence, multi-core processors, transactional memory, protocol controller, memory access protocol},
location = {Austin, TX, USA},
series = {ISCA '09}
}
@inproceedings{2009-snapshot-ics,
author = {Chung, Jaewoong and Baek, Woongki and Kozyrakis, Christos},
title = {Fast Memory Snapshot for Concurrent Programmingwithout Synchronization},
year = {2009},
month = jun,
isbn = {9781605584980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1542275.1542297},
abstract = {The industry-wide turn toward chip-multiprocessors (CMPs) provides an increasing amount of parallel resources for commodity systems. However, it is still difficult to harness the available parallelism in user applications and system software code.We propose MShot, a hardware-assisted memory snapshot for concurrent programming without synchronization code. It supports atomic multi-word read operations on a large dataset. Since modern processors support atomic access only to a single word, programmers should add synchronization code to process a multiword dataset concurrently in multithreading environment. With snapshot, programmers read the dataset atomically and process the snapshot image without synchronization code. We implement MShot using hardware resources for transactional memory and reduce the storage overhead from 2.98\% to 0.07\%. To demonstrate the usefulness of fast snapshot, we use MShot to implement concurrent versions of garbage collection and call-path profiling. Without the need for synchronization code, MShot allows such system services to run in parallel with user applications on spare cores in CMP systems. As a result, the overhead of these services},
booktitle = {Proceedings of the 23rd International Conference on Supercomputing (ICS)},
pages = {117–125},
numpages = {9},
keywords = {snapshot, transactional memory},
location = {Yorktown Heights, NY, USA},
series = {ICS '09}
}


@inproceedings{2009-fdbopt-popl,
author = {Bronson, Nathan G. and Kozyrakis, Christos and Olukotun, Kunle},
title = {Feedback-Directed Barrier Optimization in a Strongly Isolated STM},
year = {2009},
month = jan,
isbn = {9781605583792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1480881.1480909},
abstract = {Speed improvements in today's processors have largely been delivered in the form of multiple cores, increasing the importance of abstractions that ease parallel programming. Software transactional memory (STM) addresses many of the complications of concurrency by providing a simple and composable model for safe access to shared data structures. Software transactions extend a language with an atomic primitive that declares that the effects of a block of code should not be interleaved with actions executing concurrently on other threads. Adding barriers to shared memory accesses provides atomicity, consistency and isolation.Strongly isolated STMs preserve the safety properties of transactions for all memory operations in a program, not just those inside an atomic block. Isolation barriers are added to non-transactional loads and stores in such a system to prevent those accesses from observing or corrupting a partially completed transaction. Strong isolation is especially important when integrating transactions into an existing language and memory model. Isolation barriers have a prohibitive performance overhead, however, so most STM proposals have chosen not to provide strong isolation.In this paper we reduce the costs of strong isolation by customizing isolation barriers for their observed usage. The customized barriers provide accelerated execution by blocking threads whose accesses do not follow the expected pattern. We use hot swap to tighten or loosen the hypothesized pattern, while preserving strong isolation. We introduce a family of optimization hypotheses that balance verification cost against generality.We demonstrate the feasibility of dynamic barrier optimization by implementing it in a bytecode-rewriting Java STM. Feedback-directed customization reduces the overhead of strong isolation from 505\% to 38\% across 11 non-transactional benchmarks; persistent feedback data further reduces the overhead to 16\%. Dynamic optimization accelerates a multi-threaded transactional benchmark by 31\% for weakly-isolated execution and 34\% for strongly-isolated execution.},
booktitle = {Proceedings of the 36th ACM Symposium on Principles of Programming Languages (POPL)},
pages = {213–225},
numpages = {13},
keywords = {hot swap, strong isolation, bytecode rewriting, transactional memory, deoptimization, weak isolation},
location = {Savannah, GA, USA},
series = {POPL '09}
}

@inproceedings{2008-loki-osdi,
author = {Zeldovich, Nickolai and Kannan, Hari and Dalton, Michael and Kozyrakis, Christos},
title = {Hardware Enforcement of Application Security Policies Using Tagged Memory},
year = {2008},
month = dec,
publisher = {USENIX Association},
address = {USA},
abstract = {Computers are notoriously insecure, in part because application security policies do not map well onto traditional protection mechanisms such as Unix user accounts or hardware page tables. Recent work has shown that application policies can be expressed in terms of information flow restrictions and enforced in an OS kernel, providing a strong assurance of security. This paper shows that enforcement of these policies can be pushed largely into the processor itself, by using tagged memory support, which can provide stronger security guarantees by enforcing application security even if the OS kernel is compromised.We present the Loki tagged memory architecture, along with a novel operating system structure that takes advantage of tagged memory to enforce application security policies in hardware. We built a full-system prototype of Loki by modifying a synthesizable SPARC core, mapping it to an FPGA board, and porting HiStar, a Unix-like operating system, to run on it. One result is that Loki allows HiStar, an OS already designed to have a small trusted kernel, to further reduce the amount of trusted code by a factor of two, and to enforce security despite kernel compromises. Using various workloads, we also demonstrate that HiStar running on Loki incurs a low performance overhead.},
booktitle = {Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation (OSDI)},
pages = {225–240},
numpages = {16},
location = {San Diego, California},
series = {OSDI'08}
}

@inproceedings{2008-powermodel-hotpower,
author = {Rivoire, Suzanne and Ranganathan, Parthasarathy and Kozyrakis, Christos},
title = {A Comparison of High-Level Full-System Power Models},
year = {2008},
month = dec,
publisher = {USENIX Association},
address = {USA},
abstract = {Dynamic power management in enterprise environments requires an understanding of the relationship between resource utilization and system-level power consumption. Power models based on resource utilization have been proposed in the context of enabling specific energy-efficiency optimizations on specific machines, but the accuracy and portability of different approaches to modeling have not been systematically compared. In this work, we use a common infrastructure to fit a family of high-level full-system power models, and we compare these models over a wide variation of workloads and machines, from a laptop to a server. This analysis shows that a model based on OS utilization metrics and CPU performance counters is generally most accurate across the machines and workloads tested. It is particularly useful for machines whose dynamic power consumption is not dominated by the CPU, as well as machines with aggressively power-managed CPUs, two classes of systems that are increasingly prevalent.},
booktitle = {Proceedings of the 2008 Conference on Power Aware Computing and Systems (HotPower)},
pages = {3},
numpages = {1},
location = {San Diego, California},
series = {HotPower'08}
}


@article{2008-memory-taco,
author = {Leverich, Jacob and Arakida, Hideho and Solomatnikov, Alex and Firoozshahian, Amin and Horowitz, Mark and Kozyrakis, Christos},
title = {Comparative Evaluation of Memory Models for Chip Multiprocessors},
year = {2008},
issue_date = {November 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {1544-3566},
doi = {10.1145/1455650.1455651},
abstract = {There are two competing models for the on-chip memory in Chip Multiprocessor (CMP) systems: hardware-managed coherent caches and software-managed streaming memory. This paper performs a direct comparison of the two models under the same set of assumptions about technology, area, and computational capabilities. The goal is to quantify how and when they differ in terms of performance, energy consumption, bandwidth requirements, and latency tolerance for general-purpose CMPs. We demonstrate that for data-parallel applications on systems with up to 16 cores, the cache-based and streaming models perform and scale equally well. For certain applications with little data reuse, streaming scales better due to better bandwidth use and macroscopic software prefetching. However, the introduction of techniques such as hardware prefetching and nonallocating stores to the cache-based model eliminates the streaming advantage. Overall, our results indicate that there is not sufficient advantage in building streaming memory systems where all on-chip memory structures are explicitly managed. On the other hand, we show that streaming at the programming model level is particularly beneficial, even with the cache-based model, as it enhances locality and creates opportunities for bandwidth optimizations. Moreover, we observe that stream programming is actually easier with the cache-based model because the hardware guarantees correct, best-effort execution even when the programmer cannot fully regularize an application's code.},
journal = {ACM Transactions on Architure and Code Optimization},
month = dec,
articleno = {12},
numpages = {30},
keywords = {locality optimizations, parallel programming, cache coherence, streaming memory, Chip multiprocessors}
}


@article{2008-tm-cacm,
author = {Larus, James and Kozyrakis, Christos},
title = {Transactional Memory},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {7},
issn = {0001-0782},
doi = {10.1145/1364782.1364800},
abstract = {Is TM the answer for improving parallel programming?},
journal = {Communications of the ACM},
month = jul,
pages = {80–88},
numpages = {9}
}



@INPROCEEDINGS{2008-stamp-iiswc,
author={Chi Cao Minh and JaeWoong Chung and Kozyrakis, Christos and Olukotun, Kunle},
booktitle={Proceedings of the IEEE International Symposium on Workload Characterization (IISWC)},
title={STAMP: Stanford Transactional Applications for Multi-Processing},
year={2008},
volume={},
number={},
pages={35-46},
abstract={Transactional Memory (TM) is emerging as a promising technology to simplify parallel programming. While several TM systems have been proposed in the research literature, we are still missing the tools and workloads necessary to analyze and compare the proposals. Most TM systems have been evaluated using microbenchmarks, which may not be representative of any real-world behavior, or individual applications, which do not stress a wide range of execution scenarios. We introduce the Stanford Transactional Application for Multi-Processing (STAMP), a comprehensive benchmark suite for evaluating TM systems. STAMP includes eight applications and thirty variants of input parameters and data sets in order to represent several application domains and cover a wide range of transactional execution cases (frequent or rare use of transactions, large or small transactions, high or low contention, etc.). Moreover, STAMP is portable across many types of TM systems, including hardware, software, and hybrid systems. In this paper, we provide descriptions and a detailed characterization of the applications in STAMP.We also use the suite to evaluate six different TM systems, identify their shortcomings, and motivate further research on their performance characteristics.},
keywords={Benchmark testing;Data structures;Software;Bioinformatics;Hardware;Synchronization;Genomics},
doi={10.1109/IISWC.2008.4636089},
ISSN={},
month=sep,}

@inproceedings{2008-buffer-usenixsec,
author = {Dalton, Michael and Kannan, Hari and Kozyrakis, Christos},
title = {Real-World Buffer Overflow Protection for Userspace &amp; Kernelspace},
year = {2008},
month = jul,
publisher = {USENIX Association},
address = {USA},
abstract = {Despite having been around for more than 25 years, buffer overflow attacks are still a major security threat for deployed software. Existing techniques for buffer overflow detection provide partial protection at best as they detect limited cases, suffer from many false positives, require source code access, or introduce large performance overheads. Moreover, none of these techniques are easily applicable to the operating system kernel.This paper presents a practical security environment for buffer overflow detection in userspace and kernelspace code. Our techniques build upon dynamic information flow tracking (DIFT) and prevent the attacker from overwriting pointers in the application or operating system. Unlike previous work, our technique does not have false positives on unmodified binaries, protects both data and control pointers, and allows for practical hardware support. Moreover, it is applicable to the kernel and provides robust detection of buffer overflows and user/kernel pointer dereferences. Using a full system prototype of a Linux workstation (hardware and software), we demonstrate our security approach in practice and discuss the major challenges for robust buffer overflow protection in real-world software.},
booktitle = {Proceedings of the 17th USENIX Security Symposium (UsenixSec)},
pages = {395–410},
numpages = {16},
location = {San Jose, CA},
series = {SS'08}
}

@inproceedings{2008-snapshot-spaa,
author = {Chung, JaeWoong and Seo, Jiwon and Baek, Woongki and CaoMinh, Chi and McDonald, Austen and Kozyrakis, Christos and Olukotun, Kunle},
title = {Improving Software Concurrency with Hardware-Assisted Memory Snapshot},
year = {2008},
month = jun,
isbn = {9781595939739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1378533.1378596},
abstract = {We propose a hardware-assisted memory snapshot to improve software concurrency. It is built on top of the hardware resources for transactional memory and allows for easy development of system software modules such as concurrent garbage collector and dynamic profiler.},
booktitle = {Proceedings of the 12th Symposium on Parallelism in Algorithms and Architectures (SPAA)},
pages = {363},
numpages = {1},
keywords = {memory snapshot, transactional memory},
location = {Munich, Germany},
series = {SPAA '08}
}

@inproceedings{2008-ased-spaa,
author = {Chung, JaeWoong and Baek, Woongki and Bronson, Nathan Grasso and Seo, Jiwon and Kozyrakis, Christos and Olukotun, Kunle},
title = {Ased: Availability, Security, and Debugging Support Usingtransactional Memory},
year = {2008},
month = jun,
isbn = {9781595939739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1378533.1378599},
abstract = {We propose ASeD that uses the hardware resources of transactional memory systems for non transactional memory purpose. We show that the hardware components for register checkpointing, data versioning, and conflict detection can be reused as basic building blocks for reliability, security, and debugging support.},
booktitle = {Proceedings of the 12th Symposium on Parallelism in Algorithms and Architectures (SPAA)},
pages = {366},
numpages = {1},
keywords = {security, reliability, transactional memory, debugging},
location = {Munich, Germany},
series = {SPAA '08}
}


@INPROCEEDINGS{2008-dbt-hpca,
author={JaeWoong Chung and Dalton, Michael and Kannan, Hari and Kozyrakis, Christos},
booktitle={Proceedings of the IEEE 14th International Symposium on High Performance Computer Architecture (HPCA)},
title={Thread-safe dynamic binary translation using transactional memory},
year={2008},
month = feb,
volume={},
number={},
pages={279-289},
abstract={Dynamic binary translation (DBT) is a runtime instrumentation technique commonly used to support profiling, optimization, secure execution, and bug detection tools for application binaries. However, DBT frameworks may incorrectly handle multithreaded programs due to races involving updates to the application data and the corresponding metadata maintained by the DBT. Existing DBT frameworks handle this issue by serializing threads, disallowing multithreaded programs, or requiring explicit use of locks. This paper presents a practical solution for correct execution of multithreaded programs within DBT frameworks. To eliminate races involving metadata, we propose the use of transactional memory (TM). The DBT uses memory transactions to encapsulate the data and metadata accesses in a trace, within one atomic block. This approach guarantees correct execution of concurrent threads of the translated program, as TM mechanisms detect and correct races. To demonstrate this approach, we implemented a DBT-based tool for secure execution of x86 binaries using dynamic information flow tracking. This is the first such framework that correctly handles multithreaded binaries without serialization. We show that the use of software transactions in the DBT leads to a runtime overhead of 40\%. We also show that software optimizations in the DBT and hardware support for transactions can reduce the runtime overhead to 6\%.},
keywords={Optimization;Security;Hardware;Instruments;Runtime;Software;Buffer overflow},
doi={10.1109/HPCA.2008.4658646},
ISSN={2378-203X},
month=Feb,}

@inproceedings{2007-tmsync-ppopp,
author = {Adl-Tabatabai, Ali-Reza and Dice, David and Herlihy, Maurice and Shavit, Nir and Kozyrakis, Christos and von Praun, Christoph and Scott, Michael},
title = {Potential Show-Stoppers for Transactional Synchronization},
year = {2007},
month = mar,
isbn = {9781595936028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1229428.1229439},
booktitle = {Proceedings of the 12th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)},
pages = {55},
numpages = {1},
location = {San Jose, California, USA},
series = {PPoPP '07}
}

@inproceedings{2007-tmprog-ppopp,
author = {Adl-Tabatabai, Ali-Reza and Kozyrakis, Christos and Saha, Bratin},
title = {Transactional Programming in a Multi-Core Environment},
year = {2007},
month = mar,
isbn = {9781595936028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1229428.1229484},
abstract = {With single thread performance starting to plateau, HW architects have turned to chip level multiprocessing (CMP) to increase processing power. All major microprocessor companies are aggressively shipping multi-core products in the mainstream computing market. Moore's law will largely be used to increase HW thread-level parallelism through higher core counts in a CMP environment. CMPs bring new challenges into the design of the software system stack.In this tutorial, we talk about the shift to multi-core processors and the programming implications. In particular, we focus on transactional programming. Transactions have emerged as a promising alternative to lock-based synchronization that eliminates many of the problems associated with lock-based synchronization. We discuss the design of both hardware and software transactional memory and quantify the tradeoffs between the different design points. We show how to extend the Java and C languages with transactional constructs, and how to integrate transactions with compiler optimizations and the language runtime (e.g., memory manager and garbage collection).},
booktitle = {Proceedings of the 12th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)},
pages = {272},
numpages = {1},
keywords = {transactional memory, atomicity, parallel programming, hardware architecture},
location = {San Jose, California, USA},
series = {PPoPP '07}
}

@article{2007-energy-computer,
author = {Rivoire, Suzanne and Shah, Mehul A. and Ranganathan, Parthasarathy and Kozyrakis, Christos and Meza, Justin},
title = {Models and Metrics to Enable Energy-Efficiency Optimizations},
year = {2007},
issue_date = {December 2007},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {40},
number = {12},
issn = {0018-9162},
doi = {10.1109/MC.2007.436},
abstract = {Power consumption and energy efficiency are important factors in the initial design and day-to-day management of computer systems. Researchers and system designers need benchmarks that characterize energy efficiency to evaluate systems and identify promising new technologies. To predict the effects of new designs and configurations, they also need accurate methods of modeling power consumption.},
journal = {IEEE Computer},
month = dec,
pages = {39–48},
numpages = {10},
keywords = {power models, JouleSort benchmark, green computing, energy-efficiency optimizations}
}

@inproceedings{2007-bliss-cases,
author = {Zmily, Ahmad and Kozyrakis, Christos},
title = {A Low Power Front-End for Embedded Processors Using a Block-Aware Instruction Set},
year = {2007},
month = sep,
isbn = {9781595938268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1289881.1289926},
abstract = {Energy, power, and area efficiency are critical design concerns for embedded processors. Much of the energy of a typical embedded processor is consumed in the front-end since instruction fetching happens on nearly every cycle and involves accesses to large memory arrays such as instruction and branch target caches. The use of small front-end arrays leads to significant power and area savings, but typically results in significant performance degradation. This paper evaluates and compares optimizations that improve the performance of embedded processors with small front-end caches. We examine both software techniques, such as instruction re-ordering and selective caching, and hardware techniques, such as instruction prefetching, tagless instruction cache, and unified caches for instruction and branch targets. We demonstrate that, building on top of a block-aware instruction set, these optimizations can eliminate the performance degradation due to small front-end caches. Moreover, selective combinations of these optimizations lead to an embedded processor that performs significantly better than the large cache design while maintaining the area and energy efficiency of the small cache design.},
booktitle = {Proceedings of the 2007 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems (CASES)},
pages = {267–276},
numpages = {10},
keywords = {low power front-end, instruction re-ordering, unified instruction cache and BTB, tagless instruction cache, software hints, instruction prefetching},
location = {Salzburg, Austria},
series = {CASES '07}
}


@inproceedings{2007-opentm-pact,
author = {Baek, Woongki and Minh, Chi Cao and Trautmann, Martin and Kozyrakis, Christos and Olukotun, Kunle},
title = {The OpenTM Transactional Application Programming Interface},
year = {2007},
month = sep,
isbn = {0769529445},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Transactional Memory (TM) simplifies parallel programming by supporting atomic and isolated execution of user-identified tasks. To date, TM programming has required the use of libraries that make it difficult to achieve scalable performance with code that is easy to develop and maintain. For TM programming to become practical, it is important to integrate TM into familiar, high-level environments for parallel programming. This paper presents OpenTM, an application programming interface (API) for parallel programming with transactions. OpenTM extends OpenMP, a widely used API for shared-memory parallel programming, with a set of compiler directives to express non-blocking synchronization and speculative parallelization based on memory transactions. We also present a portable OpenTM implementation that produces code for hardware, software, and hybrid TM systems. The implementation builds upon the OpenMP support in the GCC compiler and includes a runtime for the C programming language. We evaluate the performance and programmability features of OpenTM. We show that it delivers the performance of fine-grain locks at the programming simplicity of coarsegrain locks. Compared to transactional programming with lower-level interfaces, it removes the burden of manual annotations for accesses to shared variables and enables easy changes of the scheduling and contention management policies. Overall, OpenTM provides a practical and efficient TM programming environment within the familiar scope of OpenMP.},
booktitle = {Proceedings of the 16th International Conference on Parallel Architecture and Compilation Techniques (PACT)},
pages = {376–387},
numpages = {12},
series = {PACT '07}
}


@inproceedings{2007-joulesort-sigmod,
author = {Rivoire, Suzanne and Shah, Mehul A. and Ranganathan, Parthasarathy and Kozyrakis, Christos},
title = {JouleSort: A Balanced Energy-Efficiency Benchmark},
year = {2007},
month = jun,
isbn = {9781595936868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1247480.1247522},
abstract = {The energy efficiency of computer systems is an important concern in a variety of contexts. In data centers, reducing energy use improves operating cost, scalability, reliability, and other factors. For mobile devices, energy consumption directly affects functionality and usability. We propose and motivate JouleSort, an external sort benchmark, for evaluating the energy efficiency of a wide range of computer systems from clusters to handhelds. We list the criteria, challenges, and pitfalls from our experience in creating a fair energy-efficiency benchmark. Using a commercial sort, we demonstrate a JouleSort system that is over 3.5x as energy-efficient as last year's estimated winner. This system is quite different from those currently used in data centers. It consists of a commodity mobile CPU and 13 laptop drives connected by server-style I/O interfaces.},
booktitle = {Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data (SIGMOD)},
pages = {365–376},
numpages = {12},
keywords = {sort, servers, power, energy-efficiency, benchmark},
location = {Beijing, China},
series = {SIGMOD '07}
}


@inproceedings{2007-raksha-isca,
author = {Dalton, Michael and Kannan, Hari and Kozyrakis, Christos},
title = {Raksha: A Flexible Information Flow Architecture for Software Security},
year = {2007},
month = jun,
isbn = {9781595937063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1250662.1250722},
abstract = {High-level semantic vulnerabilities such as SQL injection and crosssite scripting have surpassed buffer overflows as the most prevalent security exploits. The breadth and diversity of software vulnerabilities demand new security solutions that combine the speed and practicality of hardware approaches with the flexibility and robustness of software systems.This paper proposes Raksha, an architecture for software security based on dynamic information flow tracking (DIFT). Raksha provides three novel features that allow for a flexible hardware/software approach to security. First, it supports flexible and programmable security policies that enable software to direct hardware analysis towards a wide range of high-level and low-level attacks. Second, it supports multiple active security policies that can protect the system against concurrent attacks. Third, it supports low-overhead security handlers that allow software to correct, complement, or extend the hardware-based analysis without the overhead associated with operating system traps.We present an FPGA prototype for Raksha that provides a full featured Linux workstation for security analysis. Using unmodified binaries for real-world applications, we demonstrate that Raksha can detect high-level attacks such as directory traversal, command injection, SQL injection, and cross-site scripting as well as low-level attacks such as buffer overflows. We also show that low overhead exception handling is critical for analyses such as memory corruption protection in order to address false positives that occur due to the diverse code patterns in frequently used software.},
booktitle = {Proceedings of the 34th International Symposium on Computer Architecture (ISCA)},
pages = {482–493},
numpages = {12},
keywords = {semantic vulnerabilities, software security, dynamic},
location = {San Diego, California, USA},
series = {ISCA '07}
}

@inproceedings{2007-memory-isca,
author = {Leverich, Jacob and Arakida, Hideho and Solomatnikov, Alex and Firoozshahian, Amin and Horowitz, Mark and Kozyrakis, Christos},
title = {Comparing Memory Systems for Chip Multiprocessors},
year = {2007},
month = jun,
isbn = {9781595937063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1250662.1250707},
abstract = {There are two basic models for the on-chip memory in CMP systems:hardware-managed coherent caches and software-managed streaming memory. This paper performs a direct comparison of the two modelsunder the same set of assumptions about technology, area, and computational capabilities. The goal is to quantify how and when they differ in terms of performance, energy consumption, bandwidth requirements, and latency tolerance for general-purpose CMPs. We demonstrate that for data-parallel applications, the cache-based and streaming models perform and scale equally well. For certain applications with little data reuse, streaming scales better due to better bandwidth use and macroscopic software prefetching. However, the introduction of techniques such as hardware prefetching and non-allocating stores to the cache-based model eliminates the streaming advantage. Overall, our results indicate that there is not sufficient advantage in building streaming memory systems where all on-chip memory structures are explicitly managed. On the other hand, we show that streaming at the programming model level is particularly beneficial, even with the cache-based model, as it enhances locality and creates opportunities for bandwidth optimizations. Moreover, we observe that stream programming is actually easier with the cache-based model because the hardware guarantees correct, best-effort execution even when the programmer cannot fully regularize an application's code.},
booktitle = {Proceedings of the 34th International Symposium on Computer Architecture (ISCA)},
pages = {358–368},
numpages = {11},
keywords = {streaming memory, chip multiprocessors, coherent caches, parallel programming, locality optimizations},
location = {San Diego, California, USA},
series = {ISCA '07}
}


@inproceedings{2007-htm-isca,
author = {Minh, Chi Cao and Trautmann, Martin and Chung, JaeWoong and McDonald, Austen and Bronson, Nathan and Casper, Jared and Kozyrakis, Christos and Olukotun, Kunle},
title = {An Effective Hybrid Transactional Memory System with Strong Isolation Guarantees},
year = {2007},
month = jun,
isbn = {9781595937063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1250662.1250673},
abstract = {We propose signature-accelerated transactional memory (SigTM), ahybrid TM system that reduces the overhead of software transactions. SigTM uses hardware signatures to track the read-set and write-set forpending transactions and perform conflict detection between concurrent threads. All other transactional functionality, including dataversioning, is implemented in software. Unlike previously proposed hybrid TM systems, SigTM requires no modifications to the hardware caches, which reduces hardware cost and simplifies support for nested transactions and multithreaded processor cores. SigTM is also the first hybrid TM system to provide strong isolation guarantees between transactional blocks and non-transactional accesses without additional read and write barriers in non-transactional code.Using a set of parallel programs that make frequent use of coarse-grain transactions, we show that SigTM accelerates software transactions by 30\% to 280\%. For certain workloads, SigTM can match the performance of a full-featured hardware TM system, while for workloads with large read-sets it can be up to two times slower. Overall, we show that SigTM combines the performance characteristics and strong isolation guarantees of hardware TM implementations with the low cost and flexibility of software TM systems.},
booktitle = {Proceedings of the 34th International Symposium on Computer Architecture (ISCA)},
pages = {69–80},
numpages = {12},
keywords = {strong isolation, transactional memory, multi-core architectures, parallel programming},
location = {San Diego, California, USA},
series = {ISCA '07}
}


@inproceedings{2006-pca-spaa,
author = {Baek, Woongki and Chung, JaeWoong and Minh, Chi Cao and Kozyrakis, Christos and Olukotun, Kunle},
title = {Towards Soft Optimization Techniques for Parallel Cognitive Applications},
year = {2007},
month = jun,
isbn = {9781595936677},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1248377.1248389},
booktitle = {Proceedings of the 19th ACM Symposium on Parallel Algorithms and Architectures (SPAA)},
pages = {59–60},
numpages = {2},
keywords = {parallel programming, parallel algorithms, optimization, cognitive applications},
location = {San Diego, California, USA},
series = {SPAA '07}
}


@article{2007-ramp-micro,
author = {Wawrzynek, John and Patterson, David and Oskin, Mark and Lu, Shih-Lien and Kozyrakis, Christos and Hoe, James C. and Chiou, Derek and Asanovic, Krste},
title = {RAMP: Research Accelerator for Multiple Processors},
year = {2007},
issue_date = {March 2007},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {27},
number = {2},
issn = {0272-1732},
doi = {10.1109/MM.2007.39},
abstract = {The RAMP project's goal is to enable the intensive, multidisciplinary innovation that the computing industry will need to tackle the problems of parallel processing. RAMP itself is an open-source, community-developed, FPGA-based emulator of parallel architectures. Its design framework lets a large, collaborative community develop and contribute reusable, composable design modules. Three complete designs--for transactional memory, distributed systems, and distributed-shared memory--demonstrate the platform's potential.},
journal = {IEEE Micro},
month = mar,
pages = {46–57},
numpages = {12},
keywords = {emulation, parallel architectures, hardware-software codesign, distributed-shared memory, modeling of computer architecture, field-programmable gate arrays, distributed systems, transactional memory, integration}
}


@inproceedings{2007-rpa-date,
author = {Park, JongSoo and Park, Sung-Boem and Balfour, James D. and Black-Schaffer, David and Kozyrakis, Christos and Dally, William J.},
title = {Register Pointer Architecture for Efficient Embedded Processors},
year = {2007},
month = jan,
isbn = {9783981080124},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Conventional register file architectures cannot optimally exploit temporal locality in data references due to their limited capacity and static encoding of register addresses in instructions. In conventional embedded architectures, the register file capacity cannot be increased without resorting to longer instruction words. Similarly, loop unrolling is often required to exploit locality in the register file accesses across iterations because naming registers statically is inflexible. Both optimizations lead to significant code size increases, which is undesirable in embedded systems.In this paper, we introduce the Register Pointer Architecture (RPA), which allows registers to be accessed indirectly through register pointers. Indirection allows a larger register file to be used without increasing the length of instruction words. Additional register file capacity allows many loads and stores, such as those introduced by spill code, to be eliminated, which improves performance and reduces energy consumption. Moreover, indirection affords additional flexibility in naming registers, which reduces the need to apply loop unrolling in order to maximize reuse of register allocated variables.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe (DATE)},
pages = {600–605},
numpages = {6},
location = {Nice, France},
series = {DATE '07}
}

@inproceedings{2007-atlas-date,
author = {Njoroge, Njuguna and Casper, Jared and Wee, Sewook and Teslyar, Yuriy and Ge, Daxia and Kozyrakis, Christos and Olukotun, Kunle},
title = {ATLAS: A Chip-Multiprocessor with Transactional Memory Support},
year = {2007},
month = jan,
isbn = {9783981080124},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Chip-multiprocessors are quickly becoming popular in embedded systems. However, the practical success of CMPs strongly depends on addressing the difficulty of multithreaded application development for such systems. Transactional Memory (TM) promises to simplify concurrency management in multithreaded applications by allowing programmers to specify coarse-grain parallel tasks, while achieving performance comparable to fine-grain lock-based applications.This paper presents ATLAS, the first prototype of a CMP with hardware support for transactional memory. ATLAS includes 8 embedded PowerPC cores that access coherent shared memory in a transactional manner. The data cache for each core is modified to support the speculative buffering and conflict detection necessary for transactional execution. We have mapped ATLAS to the BEE2 multi-FPGA board to create a full-system prototype that operates at 100MHz, boots Linux, and provides significant performance and ease-of-use benefits for a range of parallel applications. Overall, the ATLAS prototype provides an excellent framework for further research on the software and hardware techniques necessary to deliver on the potential of transactional memory.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe (DATE)},
pages = {3–8},
numpages = {6},
location = {Nice, France},
series = {DATE '07}
}


@inproceedings{2007-tcc-ppopp,
author = {Carlstrom, Brian D. and McDonald, Austen and Carbin, Michael and Kozyrakis, Christos and Olukotun, Kunle},
title = {Transactional Collection Classes},
year = {2007},
month = mar,
isbn = {9781595936028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1229428.1229441},
abstract = {While parallel programmers find it easier to reason about large atomic regions, the conventional mutual exclusion-based primitives for synchronization force them to interleave many small operations to achieve performance. Transactional memory promises that programmer scan use large atomic regions while achieving similar performance. However, these large transactions can conflict when operating on shared data structures, even for logically independent operations. Transactional collection classes address this problem by allowing long-running transactions to operate on shared data while eliminating unnecessary conflicts. Transactional collection classes wrap existing data structures, without the need for custom implementations or knowledge of data structure internals.Without transactional collection classes, access to shared datafrom within long-running transactions can suffer from data dependency conflicts that are logically unnecessary, but are artifacts of the data structure implementation such as hash table collisions or tree-balancing rotations. Our transactional collection classes use the concept of semantic concurrency control to eliminate these unnecessary data dependencies, replacing them with conflict detection based on the operations of the abstract data type.The design and behavior of these transactional collection classes is discussed with reference to the related work from the database community such as multi-level transactions and semantic concurrency control, as well as other concurrent data structures such as java.util.concurrent. The required transactional semantics needed for implementing transactional collection are enumerated, including open-nested transactions and commit and abort handlers. We also discuss how isolation can be reduced for greater concurrency. Finally, we provide guidelines on the construction of classes that preserve isolation and serializability.The performance of these classes is evaluated with a number of benchmarks including targeted micro-benchmarks and a version of SPECjbb2000 with increased contention. The results show that easier-to-use long transactions can still allow programs to deliver scalable performance by simply wrapping existing data structures with transactional collection classes.},
booktitle = {Proceedings of the 12th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)},
pages = {56–67},
numpages = {12},
keywords = {collection classes, multiprocessor architecture, Java, transactional memory},
location = {San Jose, California, USA},
series = {PPoPP '07}
}

@article{2007-tm-hotpicks,
author = {McDonald, Austen and Carlstrom, Brian D. and Chung, JaeWoong and Minh, Chi Cao and Chafi, Hassan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Transactional Memory: The Hardware-Software Interface},
year = {2007},
issue_date = {January 2007},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {27},
number = {1},
issn = {0272-1732},
doi = {10.1109/MM.2007.26},
abstract = {This comprehensive architecture supports nested transactions, transaction handling, and two-phase commit. The result is a seamless integration of transactional memory with modern programming languages and runtime environments.},
journal = {IEEE Micro},
month = jan,
pages = {67–76},
numpages = {10},
keywords = {parallel architectures, hardware/software interfaces, transactional memory}
}


@inproceedings{2007-atlas-fpgs
author = {Wee, Sewook and Casper, Jared and Njoroge, Njuguna and Tesylar, Yuriy and Ge, Daxia and Kozyrakis, Christos and Olukotun, Kunle},
title = {A Practical FPGA-Based Framework for Novel CMP Research},
year = {2007},
month = feb,
isbn = {9781595936004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1216919.1216936},
abstract = {Chip-multiprocessors are quickly gaining momentum in all segments of computing. However, the practical success of CMPs strongly depends on addressing the difficulty of multithreaded application development. To address this challenge, it is necessary to co-develop new CMP architecture with novel programming models. Currently, architecture research relies on software simulators which are too slow to facilitate interesting experiments with CMP software without using small datasets or significantly reducing the level of detail in the simulated models. An alternative to simulation is to exploit the rich capabilities of modern FPGAs to create FPGA-based platforms for novel CMP research. This paper presents ATLAS, the first prototype for CMPs with hardware support for Transactional Memory (TM), a technology aiming to simplify parallel programming. ATLAS uses the BEE2 multi-FPGA board to provide a system with 8 PowerPC cores that run at 100MHz and runs Linux. ATLAS provides significant benefits for CMP research such as 100x performance improvement over a software simulator and good visibility that helps with software tuning and architectural improvements. In addition to presenting and evaluating ATLAS, we share our observations about building a FPGA-based framework for CMP research. Specifically, we address issues such as overall performance, challenges of mapping ASIC-style CMP RTL on to FPGAs, software support, the selection criteria for the base processor, and the challenges of using pre-designed IP libraries.},
booktitle = {Proceedings of the ACM/SIGDA 15th International Symposium on Field Programmable Gate Arrays (FPGA)},
pages = {116–125},
numpages = {10},
keywords = {transactional memory, FPGA-based emulation, chip multi-processor},
location = {Monterey, California, USA},
series = {FPGA '07}
}


@inproceedings{2007-tcc-hpca,
author = {Chafi, Hassan and Casper, Jared and Carlstrom, Brian D. and McDonald, Austen and Minh, Chi Cao and Baek, Woongki and Kozyrakis, Christos and Olukotun, Kunle},
title = {A Scalable, Non-Blocking Approach to Transactional Memory},
year = {2007},
month = feb,
isbn = {1424408040},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/HPCA.2007.346189},
abstract = {Transactional Memory (TM) provides mechanisms that promise to simplify parallel programming by eliminating the need for locks and their associated problems (dead-lock, livelock, priority inversion, convoying). For TM to be adopted in the long term, not only does it need to deliver on these promises, but it needs to scale to a high number of processors. To date, proposals for scalable TM have relegated livelock issues to user-level contention managers. This paper presents the first scalable TM implementation for directory-based distributed shared memory systems that is livelock free without the need for user-level intervention. The design is a scalable implementation of optimistic concurrency control that supports parallel commits with a two-phase commit protocol, uses write-back caches, and filters coherence messages. The scalable design is based on Transactional Coherence and Consistency (TCC), which supports continuous transactions and fault isolation. A performance evaluation of the design using both scientific and enterprise benchmarks demonstrates that the directory-based TCC design scales efficiently for NUMA systems up to 64 processors.},
booktitle = {Proceedings of the IEEE 13th International Symposium on High Performance Computer Architecture (HPCA)},
pages = {97–108},
numpages = {12},
series = {HPCA '07}
}

@inproceedings{2007-mr-hpca,
author = {Ranger, Colby and Raghuraman, Ramanan and Penmetsa, Arun and Bradski, Gary and Kozyrakis, Christos},
title = {Evaluating MapReduce for Multi-Core and Multiprocessor Systems},
year = {2007},
month = feb,
isbn = {1424408040},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/HPCA.2007.346181},
abstract = {This paper evaluates the suitability of the MapReduce model for multi-core and multi-processor systems. MapReduce was created by Google for application development on data-centers with thousands of servers. It allows programmers to write functional-style code that is automaticatlly parallelized and scheduled in a distributed system. We describe Phoenix, an implementation of MapReduce for shared-memory systems that includes a programming API and an efficient runtime system. The Phoenix run-time automatically manages thread creation, dynamic task scheduling, data partitioning, and fault tolerance across processor nodes. We study Phoenix with multi-core and symmetric multiprocessor systems and evaluate its performance potential and error recovery features. We also compare MapReduce code to code written in lower-level APIs such as P-threads. Overall, we establish that, given a careful implementation, MapReduce is a promising model for scalable performance on shared-memory systems with simple parallel code.},
booktitle = {Proceedings of the IEEE 13th International Symposium on High Performance Computer Architecture (HPCA)},
pages = {13–24},
numpages = {12},
series = {HPCA '07}
}


@article{2007-tm-queue,
author = {Adl-Tabatabai, Ali-Reza and Kozyrakis, Christos and Saha, Bratin},
title = {Unlocking Concurrency: Multicore Programming with Transactional Memory},
year = {2006},
issue_date = {December-January 2006-2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {10},
issn = {1542-7730},
doi = {10.1145/1189276.1189288},
abstract = {Multicore architectures are an inflection point in mainstream software development because they force developers to write parallel programs. In a previous article in Queue, Herb Sutter and James Larus pointed out, “The concurrency revolution is primarily a software revolution. The difficult problem is not building multicore hardware, but programming it in a way that lets mainstream applications benefit from the continued exponential growth in CPU performance.” In this new multicore world, developers must write explicitly parallel applications that can take advantage of the increasing number of cores that each successive multicore generation will provide.},
journal = {ACM Queue},
month = Dec,
pages = {24–33},
numpages = {10}
}


@article{2007-qos-cal,
author = {Guo, Fei and Kannan, Hari and Zhao, Li and Illikkal, Ramesh and Iyer, Ravi and Newell, Don and Solihin, Yan and Kozyrakis, Christos},
title = {From Chaos to QoS: Case Studies in CMP Resource Management},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0163-5964},
doi = {10.1145/1241601.1241608},
abstract = {As more and more cores are enabled on the die of future CMP platforms, we expect that several diverse workloads will run simultaneously on the platform. A key example of this trend is the growth of virtualization usage models. When multiple virtual machines or applications or threads run simultaneously, the quality of service (QoS) that the platform provides to each individual thread is non-deterministic today. This occurs because the simultaneously running threads place very different demands on the shared resources (cache space, memory bandwidth, etc) in the platform and in most cases contend with each other. In this paper, we first present case studies that show how this results in non-deterministic performance. Unlike the compute resources managed through scheduling, platform resource allocation to individual threads cannot be controlled today. In order to provide better determinism and QoS, we then examine resource management mechanisms and present QoS-aware architectures and execution environments. The main contribution of this paper is the architecture feasibility analysis through prototypes that allow experimentation with QoS-Aware execution environments and architectural resources. We describe these QoS prototypes and then present preliminary case studies of multi-tasking and virtualization usage models sharing one critical CMP resource (last-level cache). We then demonstrate how proper management of the cache resource can provide service differentiation and deterministic performance behavior when running disparate workloads in future CMP platforms.},
journal = {SIGARCH Computer Architure News},
month = Mar,
pages = {21–30},
numpages = {10}
}


@Misc{2006-qos-dascmp,
  author = 	 {Hari Kannan, Fei Guo, Li Zhao, Ramesh Illikkal, Ravi Iyer, Don Newell, Yan Solihin, Christos Kozyrakis
},
  title = 	 {From Chaos to QoS: Case Studies in CMP Resource Management},
  howpublished = {The 2nd Workshop on Design, Architecture, and Simulation of Chip-Multiprocessors (dasCMP)},
  month = 	 Dec,
  year = 	 2006
}


@article{2006-jtm-scp,
author = {Carlstrom, Brian D. and Chung, JaeWoong and Chafi, Hassan and McDonald, Austen and Minh, Chi Cao and Hammond, Lance and Kozyrakis, Christos and Olukotun, Kunle},
title = {Executing Java Programs with Transactional Memory},
year = {2006},
issue_date = {1 December 2006},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {63},
number = {2},
issn = {0167-6423},
doi = {10.1016/j.scico.2006.05.006},
abstract = {Parallel programming is difficult due to the complexity of dealing with conventional lock-based synchronization. To simplify parallel programming, there have been a number of proposals to support transactions directly in hardware and eliminate locks completely. Although hardware support for transactions has the potential to completely change the way parallel programs are written, initially transactions will be used to execute existing parallel programs. In this paper we investigate the implications of using transactions to execute existing parallel Java programs. Our results show that transactions can be used to support all aspects of Java multithreaded programs, and once Java virtual machine issues have been addressed, the conversion of a lock-based application into transactions is largely straightforward. The performance that these converted applications achieve is equal to or sometimes better than the original lock-based implementation.},
journal = {Science of Computer Programming},
month = dec,
pages = {111–129},
numpages = {19},
keywords = {transactions, multiprocessor architecture, feedback optimization}
}


@inproceedings{2006-virttm-asplos,
author = {Chung, JaeWoong and Minh, Chi Cao and McDonald, Austen and Skare, Travis and Chafi, Hassan and Carlstrom, Brian D. and Kozyrakis, Christos and Olukotun, Kunle},
title = {Tradeoffs in Transactional Memory Virtualization},
year = {2006},
month = oct,
isbn = {1595934510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1168857.1168903},
abstract = {For transactional memory (TM) to achieve widespread acceptance, transactions should not be limited to the physical resources of any specific hardware implementation. TM systems should guarantee correct execution even when transactions exceed scheduling quanta, overflow the capacity of hardware caches and physical memory, or include more independent nesting levels than what is supported in hardware. Existing proposals for TM virtualization are either incomplete or rely on complex hardware implementations, which are an overkill if virtualization is invoked infrequently in the common case.We present eXtended Transactional Memory (XTM), the first TM virtualization system that virtualizes all aspects of transactional execution (time, space, and nesting depth). XTM is implemented in software using virtual memory support. It operates at page granularity, using private copies of overflowed pages to buffer memory updates until the transaction commits and snapshots of pages to detect interference between transactions. We also describe two enhancements to XTM that use limited hardware support to address key performance bottlenecks.We compare XTM to hardwarebased virtualization using both real applications and synthetic microbenchmarks. We show that despite being software-based, XTM and its enhancements are competitive with hardware-based alternatives. Overall, we demonstrate that XTM provides a complete, flexible, and low-cost mechanism for practical TM virtualization.},
booktitle = {Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {371–381},
numpages = {11},
keywords = {OS support, transactional memory, chip multi-processor, virtualization},
location = {San Jose, California, USA},
series = {ASPLOS XII}
}


@inproceedings{2006-testtm-pact,
author = {Manovit, Chaiyasit and Hangal, Sudheendra and Chafi, Hassan and McDonald, Austen and Kozyrakis, Christos and Olukotun, Kunle},
title = {Testing Implementations of Transactional Memory},
year = {2006},
month = sep,
isbn = {159593264X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1152154.1152177},
abstract = {Transactional memory is an attractive design concept for scalable multiprocessors because it offers efficient lock-free synchronization and greatly simplifies parallel software. Given the subtle issues involved with concurrency and atomicity, however, it is important that transactional memory systems be carefully designed and aggressively tested to ensure their correctness. In this paper, we propose an axiomatic framework to model the formal specification of a realistic transactional memory system which may contain a mix of transactional and non-transactional operations. Using this framework and extensions to analysis algorithms originally developed for checking traditional memory consistency, we show that the widely practiced pseudo-random testing methodology can be effectively applied to transactional memory systems. Our testing methodology was successful in finding previously unknown bugs in the implementation of TCC, a transactional memory system. We study two flavors of the underlying analysis algorithm, one incomplete and the other complete, and show that the complete algorithm while being theoretically intractable is very efficient in practice.},
booktitle = {Proceedings of the 15th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
pages = {134–143},
numpages = {10},
keywords = {transactional memory, testing, verification, specification},
location = {Seattle, Washington, USA},
series = {PACT '06}
}

@article{2006-bliss-taco,
author = {Zmily, Ahmad and Kozyrakis, Christos},
title = {Block-Aware Instruction Set Architecture},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1544-3566},
doi = {10.1145/1162690.1162694},
abstract = {Instruction delivery is a critical component for wide-issue, high-frequency processors since its bandwidth and accuracy place an upper limit on performance. The processor front-end accuracy and bandwidth are limited by instruction-cache misses, multicycle instruction-cache accesses, and target or direction mispredictions for control-flow operations. This paper presents a block-aware instruction set (BLISS) that allows software to assist with front-end challenges. BLISS defines basic block descriptors that are stored separately from the actual instructions in a program. We show that BLISS allows for a decoupled front-end that tolerates instruction-cache latency, facilitates instruction prefetching, and leads to higher prediction accuracy.},
journal = {ACM Transactions on Architure and Code Optimization (TACO)},
month = sep,
pages = {327–357},
numpages = {31},
keywords = {basic block, instruction fetch, Instruction set architecture, decoupled architecture, software hints, branch prediction}
}




@Misc{2006_cearch-hpec,
  author = 	 {S. Crago, J. McMahon,C. Archer, K.Asanovic, R. Chaung, K. Goolsbey, M. Hall, C. Kozyrakis, K. Olukotun U. O’Reilly, R. Pancoast, V. Prasanna, R. Rabbah, S. Ward, D.},
  title = 	 {CEARCH: Cognition Enabled Architecture},
  howpublished = {Proceedings of the 10th Workshop on High Performance Embedded Computing (HPEC)},
  month = 	 Sep,
  year = 	 2006
}


@InProceedings{2006-ramp-hotchips,
  author = 	 {Krste Asanovic, Derek Chiou, James Hoe, Christos Kozyrakis, Shih-Lien Lu, Mark Oskin, Jab Rabaey, John Wawrzynek},
  title = 	 {RAMP: Research Accelerator for Multiple Processors},
  booktitle = {Technical Record of the 18th Hot Chips Symposium},
  year = 	 2006,
  month = 	 Aug}

@INPROCEEDINGS{2006-vlt-icpp,
author={Rivoire, Suzanne and Schultz, Rebecca and Okuda, Tomofumi and Kozyrakis, Christos},
booktitle={Proceedings of the International Conference on Parallel Processing (ICPP)},
title={Vector Lane Threading},
year={2006},
pages={55-64},
abstract={Multi-lane vector processors achieve excellent computational throughput for programs with high data-level parallelism (DLP). However, application phases without significant DLP are unable to fully utilize the datapaths in the vector lanes. In this paper, we propose vector lane threading (VLT), an architectural enhancement that allows idle vector lanes to run short-vector or scalar threads. VLT-enhanced vector hardware can exploit both data-level and thread-level parallelism to achieve higher performance. We investigate implementation alternatives for VLT, focusing mostly on the instruction issue bandwidth requirements. We demonstrate that VLT's area overhead is small. For applications with short vectors, VLT leads to additional speedup of IA to 23 over the base vector design. For scalar threads, VLT outperforms a 2-way CMP design by a factor of two. Overall, VLT allows vector processors to reach high computational throughput for a wider range of parallel programs and become a competitive alternative to CMP systems},
keywords={Vector processors;Parallel processing;Yarn;Concurrent computing;Throughput;Hardware;Oceans;Multithreading;Bandwidth;Telecommunication computing},
doi={10.1109/ICPP.2006.74},
ISSN={2332-5690},
month=aug,}


@inproceedings{2006-tm-isca,
author = {McDonald, Austen and Chung, JaeWoong and Carlstrom, Brian D. and Minh, Chi Cao and Chafi, Hassan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Architectural Semantics for Practical Transactional Memory},
year = {2006},
month = jun,
isbn = {076952608X},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/ISCA.2006.9},
abstract = {Transactional Memory (TM) simplifies parallel programming by allowing for parallel execution of atomic tasks. Thus far, TM systems have focused on implementing transactional state buffering and conflict resolution. Missing is a robust hardware/software interface, not limited to simplistic instructions defining transaction boundaries. Without rich semantics, current TM systems cannot support basic features of modern programming languages and operating systems such as transparent library calls, conditional synchronization, system calls, I/O, and runtime exceptions. This paper presents a comprehensive instruction set architecture (ISA) for TM systems. Our proposal introduces three key mechanisms: two-phase commit; support for software handlers on commit, violation, and abort; and full support for open- and closed-nested transactions with independent rollback. These mechanisms provide a flexible interface to implement programming language and operating system functionality. We also show that these mechanisms are practical to implement at the ISA and microarchitecture level for various TM systems. Using an execution-driven simulation, we demonstrate both the functionality (e.g., I/O and conditional scheduling within transactions) and performance potential (2.2\texttimes{} improvement for SPECjbb2000) of the proposed mechanisms. Overall, this paper establishes a rich and efficient interface to foster both hardware and software research on transactional memory.},
booktitle = {Proceedings of the 33rd International Symposium on Computer Architecture (ISCA)},
pages = {53–65},
numpages = {13},
series = {ISCA '06}
}

@Misc{2006-sec-wddd,
  author = 	 {Michael Dalton, Hari Kannan, Christos Kozyrakis},
  title = 	 {Deconstructing Hardware Architectures for Security},
  howpublished = {The 5th Workshop on Duplicating, Deconstructing, and Debunking (WDDD) at ISCA},
  month = 	 Jun,
  year = 	 2006
}

@Misc{2006-power-mobs,
  author = 	 {Dimitris Economou, Suzanne Rivoire, Christos Kozyrakis, Partha Ranganathan
},
  title = 	 {Full-system Power Analysis and Modeling for Server Environments},
  howpublished = {Workshop on Modeling Benchmarking and Simulation (MOBS) at ISCA},
  month = 	 Jun,
  year = 	 2006
}


@inproceedings{2006-atomos-pldi,
author = {Carlstrom, Brian D. and McDonald, Austen and Chafi, Hassan and Chung, JaeWoong and Minh, Chi Cao and Kozyrakis, Christos and Olukotun, Kunle},
title = {The Atomos Transactional Programming Language},
year = {2006},
month = jun,
isbn = {1595933204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1133981.1133983},
abstract = {Atomos is the first programming language with implicit transactions, strong atomicity, and a scalable multiprocessor implementation. Atomos is derived from Java, but replaces its synchronization and conditional waiting constructs with simpler transactional alternatives.The Atomos watch statement allows programmers to specify fine-grained watch sets used with the Atomos retry conditional waiting statement for efficient transactional conflict-driven wakeup even in transactional memory systems with a limited number of transactional contexts. Atomos supports open-nested transactions, which are necessary for building both scalable application programs and virtual machine implementations.The implementation of the Atomos scheduler demonstrates the use of open nesting within the virtual machine and introduces the concept of transactional memory violation handlers that allow programs to recover from data dependency violations without rolling back.Atomos programming examples are given to demonstrate the usefulness of transactional programming primitives. Atomos and Java are compared through the use of several benchmarks. The results demonstrate both the improvements in parallel programming ease and parallel program performance provided by Atomos.},
booktitle = {Proceedings of the 27th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
pages = {1–13},
numpages = {13},
keywords = {java, transactional memory, conditional synchronization, multiprocessor architecture},
location = {Ottawa, Ontario, Canada},
series = {PLDI '06}
}

@Misc{2006-early-wtw,
  author = 	 {Travis Skare and Christos Kozyrakis},
  title = 	 {Early Release: Friend or Foe},
  howpublished = {The Workshop on Transactional Memory Workloads (WTW) at PLDI},
  month = 	 Jun,
  year = 	 2006
}

@Misc{2006-specjbb_wtw,
  author = 	 {JaeWoong Chung, Chi Cao Minh, Brian D. Carlstrom, Christos Kozyrakis},
  title = 	 { Parallelizing SPECjbb2000 with Transactional Memory},
  howpublished = {The Workshop on Transactional Memory Workloads (WTW) at PLDI},
  month = 	 Jun,
  year = 	 2006
}


@Misc{2006-tm-stmcs,
  author = 	 {Brian D. Calrstrom, JaeWoong Chung, Christos Kozyrakis, Kunle Olukotun},
  title = 	 {The Software Stack for Transactional Memory: Challenges and Opportunities},
  howpublished = {The 1st Workshop on Software Tools for Multicore Systems (STMCS)},
  month = 	 Mar,
  year = 	 2006
}


@inproceedings{2006-bliss-date,
author = {Zmily, Ahmad and Kozyrakis, Christos},
title = {Simultaneously Improving Code Size, Performance, and Energy in Embedded Processors},
year = {2006},
month = mar,
isbn = {3981080106},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Code size and energy consumption are critical design concerns for embedded processors as they determine the cost of the overall system. Techniques such as reduced length instruction sets lead to significant code size savings but also introduce performance and energy consumption impediments such as additional dynamic instructions or decompression latency. In this paper, we show that a block-aware instruction set (BLISS) which stores basic block descriptors in addition to and separately from the actual instructions in the program allows embedded processors to achieve significant improvements in all three metrics: reduced code size and improved performance and lower energy consumption.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe (DATE)},
pages = {224–229},
numpages = {6},
location = {Munich, Germany},
series = {DATE '06}
}



@Unpublished{2006_libprefetch,
  author = 	 {Varun Malhotra and Christos Kozyrakis},
  title = 	 {Library-based Prefetching for Pointer Intensive Applications},
  note = 	 {Online Technical Manuscript},
  month = 	 Feb,
  year = 	 2006}

@INPROCEEDINGS{2006-tm-hpca,
author={Chung, J.W. and Chafi, H. and Minh, C.C. and McDonald, A. and Carlstrom, B. and Kozyrakis, C. and Olukotun, K.},
booktitle={Proceedings of the 12th International Symposium on High-Performance Computer Architecture (HPCA)},
title={The common case transactional behavior of multithreaded programs},
year={2006},
volume={},
number={},
pages={266-277},
abstract={Transactional memory (TM) provides an easy-to-use and high-performance parallel programming model for the upcoming chip-multiprocessor systems. Several researchers have proposed alternative hardware and software TM implementations. However, the lack of transaction-based programs makes it difficult to understand the merits of each proposal and to tune future TM implementations to the common case behavior of real application. This work addresses this problem by analyzing the common case transactional behavior for 35 multithreaded programs from a wide range of application domains. We identify transactions within the source code by mapping existing primitives for parallelism and synchronization management to transaction boundaries. The analysis covers basic characteristics such as transaction length, distribution of read-set and write-set size, and the frequency of nesting and I/O operations. The measured characteristics provide key insights into the design of efficient TM systems for both non-blocking synchronization and speculative parallelization.},
keywords={Computer aided software engineering;Frequency synchronization;Application software;Concurrent computing;Parallel programming;Hardware;Proposals;Programming profession;System recovery;Robustness},
doi={10.1109/HPCA.2006.1598135},
ISSN={2378-203X},
month=Feb}

@Misc{2006-atlas-warfp,
  author = 	 {Njuguna Njoroge, Sewook Wee, Jared Casper, Justin Burdick, Yuriy Teslyar, Christos Kozyrakis, Kunle Olukotun},
  title = 	 {Building and Using the ATLAS Transactional Memory System},
  howpublished = {The 2nd Workshop on Architecture Research using FPGA Platforms (WARFP) at HPCA-12},
  month = 	 Feb,
  year = 	 2006}

@INPROCEEDINGS{2005-power-globecom,
author={Mastroleon, L. and Bambos, N. and Kozyrakis, C. and Economou, D.},
booktitle={Proceedings of the IEEE Global Telecommunications Conference (GLOBECOM)},
title={Automatic power management schemes for Internet servers and data centers},
year={2005},
month = nov, 
volume={2},
number={},
pages={5 pp.-},
abstract={We investigate autonomic power control policies for Internet servers and data centers. In particular, by monitoring the system load and thermal status, we decide how to vary the utilized processing resources to achieve acceptable delay and power performance. We formulate the problem using a dynamic programming approach that captures the power-performance tradeoff. We study the structural properties of the optimal solution and develop low-complexity justified heuristics, which achieve significant performance gains over standard benchmarks. The performance gains are higher when the load exhibits stronger temporal variations. We also demonstrate that the heuristics are very efficient, in the sense that they perform very close to the optimal solution obtained via dynamic programming.},
keywords={Energy management;Internet;Web server;Dynamic programming;Performance gain;Power system management;Power control;Monitoring;Thermal loading;Delay},
doi={10.1109/GLOCOM.2005.1577776},
ISSN={1930-529X},
month=Nov}



@Misc{2005-jtm-scool,
  author = 	 {Brian D. Carlstrom, JaeWoong Chung, Hassan Chafi, Austen McDonald, Chi Cao MinhLance Hammond, Christos Kozyrakis, Kunle Olukotun},
  title = {Transactional Execution of Java Programs},
  howpublished = {Workshop on Synchronization and Concurrency in Object-Oriented Languages (SCOOL) at OOPSLA},
  month = 	 Oct,
  year = 	 2005}

@inproceedings{2005-tcc-pact,
author = {McDonald, Austen and Chung, JaeWoong and Chafi, Hassan and Minh, Chi Cao and Carlstrom, Brian D. and Hammond, Lance and Kozyrakis, Christos and Olukotun, Kunle},
title = {Characterization of TCC on Chip-Multiprocessors},
year = {2005},
month = Sep,
isbn = {076952429X},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/PACT.2005.11},
abstract = {Transactional Coherence and Consistency (TCC) is a novel coherence scheme for shared memory multiprocessors that uses programmer-defined transactions as the fundamental unit of parallel work, synchronization, coherence, and consistency. TCC has the potential to simplify parallel program development and optimization by providing a smooth transition from sequential to parallel programs. In this paper, we study the implementation of TCC on chip-multiprocessors (CMPs). We explore design alternatives such as the granularity of state tracking, doublebuffering, and write-update and write-invalidate protocols. Furthermore, we characterize the performance of TCC in comparison to conventional snoopy cache coherence (SCC) using parallel applications optimized for each scheme. We conclude that the two coherence schemes perform similarly, with each scheme having a slight advantage for some applications. The bandwidth requirements of TCC are slightly higher but well within the capabilities of CMP systems. Also, we find that overflow of speculative state can be effectively handled by a simple victim cache. Our results suggest TCC can provide its programming advantages without compromising the performance expected from well-tuned parallel applications.},
booktitle = {Proceedings of the 14th International Conference on Parallel Architectures and Compilation Techniques (PACT) },
pages = {63–74},
numpages = {12},
series = {PACT '05}
}


@inproceedings{2005-bliss-europar,
author = {Zmily, Ahmad and Killian, Earl and Kozyrakis, Christos},
title = {Improving Instruction Delivery with a Block-Aware ISA},
year = {2005},
month = Aug,
isbn = {3540287000},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
doi = {10.1007/11549468_60},
abstract = {Instruction delivery is a critical component for wide-issue processors since its bandwidth and accuracy place an upper limit on performance. The processor front-end accuracy and bandwidth are limited by instruction cache misses, multi-cycle instruction cache accesses, and target or direction mispredictions for control-flow operations. This paper introduces a block-aware ISA (BLISS) that helps accurate instruction delivery by defining basic block descriptors in addition to and separate from the actual instructions in a program. We show that BLISS allows for a decoupled front-end that tolerates cache latency and allows for higher speculation accuracy. This translates to a 20\% IPC and 14\% energy improvements over conventional front-ends. We also demonstrate that a BLISS-based front-end outperforms by 13\% decoupled front-ends that detect fetched blocks dynamically in hardware, without any information from the ISA.},
booktitle = {Proceedings of the 11th International Euro-Par Conference on Parallel Processing (Euro-Par)},
pages = {530–539},
numpages = {10},
location = {Lisbon, Portugal},
series = {Euro-Par'05}
}


@TechReport{2005-ramp-tr,
  author = 	 {Arvind, Krste Asanović, Derek Chiou, James C. Hoe, Christos Kozyrakis, Shih-Lien Lu, Mark Oskin, David Patterson, Jan Rabaey, John Wawrzynek},
  title = 	 {RAMP: Research Accelerator for Multiple Processors - A Community Vision for a Shared Experimental Parallel HW/SW Platform },
  institution =  {UC Berkeley},
  year = 	 2005,
  number = 	 {UCB/CSD-05-1412},
  month = 	Sep}

@inproceedings{2005-bliss-ispled,
author = {Zmily, Ahmad and Kozyrakis, Christos},
title = {Energy-Efficient and High-Performance Instruction Fetch Using a Block-Aware ISA},
year = {2005},
month = Aug,
isbn = {1595931376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1077603.1077614},
abstract = {The front-end in superscalar processors must deliver high application performance in an energy-effective manner. Impediments such as multi-cycle instruction accesses, instruction-cache misses, and mispredictions reduce performance by 48\% and increase energy consumption by 21\%. This paper presents a block-aware instruction set architecture (BLISS) that defines basic block descriptors in addition to the actual instructions in a program. BLISS allows for a decoupled front-end that reduces the time and energy spent on misspeculated instructions. It also allows for accurate instruction prefetching and energy efficient instruction access. A BLISS-based front-end leads to 14\% IPC, 16\% total energy, and 83\% energy-delay-squared product improvements for wide-issue processors},
booktitle = {Proceedings of the International Symposium on Low Power Electronics and Design (ISLPED)},
pages = {36–41},
numpages = {6},
keywords = {energy efficiency, basic blocks, instruction delivery, instruction set architecture, decoupled architecture},
location = {San Diego, CA, USA},
series = {ISLPED '05}
}




@inproceedings{2005-tape-ics,
author = {Chafi, Hassan and Minh, Chi Cao and McDonald, Austen and Carlstrom, Brian D. and Chung, JaeWoong and Hammond, Lance and Kozyrakis, Christos and Olukotun, Kunle},
title = {TAPE: A Transactional Application Profiling Environment},
year = {2005},
month = jun,
isbn = {1595931678},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1088149.1088176},
abstract = {Transactional Coherence and Consistency (TCC) provides a new parallel programming model that uses transactions as the basic unit of parallel work and communication. TCC simplifies the development of correct parallel code because hardware provides transaction atomicity and ordering. Nevertheless, the programmer or a dynamic compiler must still optimize the parallel code for performance.This paper presents TAPE, a hardware and software infrastructure for profiling in TCC systems. TAPE extends the hardware for transactional execution to identify performance impediments such as dependence violations, buffer overflows, and work imbalance. It filters infrequent events to reduce resource requirements and allows the programmer to focus on the most important bottlenecks. We demonstrate that TAPE introduces minimal die area and performance overhead and can be used continuously, even for production runs. Moreover, we demonstrate how to leverage the profiling information to guide optimization for a set of parallel applications. TAPE accurately identifies the source code location and type of the most important bottlenecks, allowing a programmer to achieve maximum parallel speedup with a few profiling steps.},
booktitle = {Proceedings of the 19th International Conference on Supercomputing (ICS)},
pages = {199–208},
numpages = {10},
location = {Cambridge, Massachusetts},
series = {ICS '05}
}

@inproceedings{2005-spec-icpp,
author = {Whaley, John and Kozyrakis, Christos},
title = {Heuristics for Profile-Driven Method-Level Speculative Parallelization},
year = {2005},
month = jun,
isbn = {0769523803},
publisher = {IEEE Computer Society},
address = {USA},
doi = {10.1109/ICPP.2005.44},
abstract = {Thread level speculation (TLS) is an effective technique for extracting parallelism from sequential code. Method calls provide good templates for the boundaries of speculative threads as they often describe independent tasks. However, selecting the most profitable methods to speculate on is difficult as it involves complicated trade-offs between speculation violations, thread overheads, and resource utilization. This paper presents a first analysis of heuristics for automatic selection of speculative threads across method boundaries using a dynamic or profile-driven compiler. We study the potential of three classes of heuristics that involve increasing amounts of profiling information and runtime complexity. Several of the heuristics allow for speculation to start at internal method points, nested speculation, and speculative thread preemption. Using a set of Java benchmarks, we demonstrate that careful thread selection at method boundaries leads to speedups of 1.4 to 1.8 on practical TLS hardware. Single-pass heuristics that filter out less profitable methods using simple speedup estimates lead to the best average performance by consistently providing a good balance between over- and under-speculation. On the other hand, multi-pass heuristics that perform additional filtering by taking into account interactions between nested method calls often lead to significant under-speculation and perform poorly.},
booktitle = {Proceedings of the  International Conference on Parallel Processing (ICPP)},
pages = {147–156},
numpages = {10},
series = {ICPP '05}
}


@Misc{2005-atlas-warfp,
  author = 	 {Christos Kozyrakis, Kunle Olukotun
},
  title = 	 {ATLAS: A Scalable Emulator for Transactional Parallel Systems },
  howpublished = {Workshop on Architecture Research using FPGA Platforms (WARFP'05) at HPCA},
  month = 	 Feb,
  year = 	 2005}

@Misc{2004-svn-lar,
  author = 	 {Peter. Mattson, Richard Lethin, V. Litvinov, Francois Labonte, Ian Buck, Christos Kozyrakis, Mark Horowitz},
  title = 	 {Stream Virtual Machine and Two-Level Compilation Model for Streaming Architectures and Languages},
  howpublished = {Proceedings of the 3rd Intl. Workshop on Languages and Runtimes (LaR) at OOPSLA},
  month = 	 Oct,
  year = 	 2004,
  abstract = {The stream computing paradigm separates the application's
computational kernels from communication streams, matching the
structure and performance constraints of modern multiprocessors
and data intensive applications with regular communication
patterns. The multitude of stream architectures and languages
create interoperability problems and cause duplication of compiler
and tool development efforts. To address this, the Morphware
Forum [8] is developing a two-level complication process,
whereby a language-specific high-level compiler interfaces with
an architecture-specific low-level compiler using an architecture
model and Streaming Virtual Machine [7] code. We describe this
interface and highlight the challenges of this approach. }}

@article{2004-tcc-toppicks,
author = {Hammond, Lance and Carlstrom, Brian D. and Wong, Vicky and Chen, Michael and Kozyrakis, Christos and Olukotun, Kunle},
title = {Transactional Coherence and Consistency: Simplifying Parallel Hardware and Software},
year = {2004},
issue_date = {November 2004},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {24},
number = {6},
issn = {0272-1732},
doi = {10.1109/MM.2004.91},
abstract = {TCC simplifies parallel hardware and software design by eliminating the need for conventional cache coherence and consistency models and letting programmers parallelize a wide range of applications with a simple, lock-free transactional model.},
journal = {IEEE Micro},
month = nov,
pages = {92–103},
numpages = {12}
}


@inproceedings{2004-tcc-asplos,
author = {Hammond, Lance and Carlstrom, Brian D. and Wong, Vicky and Hertzberg, Ben and Chen, Mike and Kozyrakis, Christos and Olukotun, Kunle},
title = {Programming with Transactional Coherence and Consistency (TCC)},
year = {2004},
month = oct,
isbn = {1581138040},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1024393.1024395},
abstract = {Transactional Coherence and Consistency (TCC) offers a way to simplify parallel programming by executing all code within transactions. In TCC systems, transactions serve as the fundamental unit of parallel work, communication and coherence. As each transaction completes, it writes all of its newly produced state to shared memory atomically, while restarting other processors that have speculatively read stale data. With this mechanism, a TCC-based system automatically handles data synchronization correctly, without programmer intervention. To gain the benefits of TCC, programs must be decomposed into transactions. We describe two basic programming language constructs for decomposing programs into transactions, a loop conversion syntax and a general transaction-forking mechanism. With these constructs, writing correct parallel programs requires only small, incremental changes to correct sequential programs. The performance of these programs may then easily be optimized, based on feedback from real program execution, using a few simple techniques.},
booktitle = {Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
pages = {1–13},
numpages = {13},
keywords = {feedback optimization, transactions, multiprocessor architecture},
location = {Boston, MA, USA},
series = {ASPLOS XI}
}

@inproceedings{2004-svm-pact,
author = {Labonte, Francois and Mattson, Peter and Thies, William and Buck, Ian and Kozyrakis, Christos and Horowitz, Mark},
title = {The Stream Virtual Machine},
year = {2004},
month = sep,
isbn = {0769522297},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Stream programming is currently being pushed as a way to expose concurrency and separate communication from computation. Since there are many stream languages and potential stream execution engines, this paper proposes an abstract machine model that captures the essential characteristics of stream architectures, the Stream Virtual Machine (SVM). The goal of the SVM is to improve interoperability, allow developpment of common compilation tools and reason about stream program performance. The SVM contains control processors, slave kernel processors, and slave DMA units. Is is presented along with the compilation process that takes a stream program down to the SVM and finally down to machine binary. To extract the parameters for our SVM model, we use micro-kernels to characterize two graphics processors and a stream engine, Imagine. The results are encouraging; the model estimates the performance of the target machines with high accuracy.},
booktitle = {Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
pages = {267–277},
numpages = {11},
series = {PACT '04}
}


@inproceedings{2004-tcc-isca,
author = {Hammond, Lance and Wong, Vicky and Chen, Mike and Carlstrom, Brian D. and Davis, John D. and Hertzberg, Ben and Prabhu, Manohar K. and Wijaya, Honggo and Kozyrakis, Christos and Olukotun, Kunle},
title = {Transactional Memory Coherence and Consistency},
year = {2004},
month = jun,
isbn = {0769521436},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propos a new shared memory model: Transactionalmemory Coherence and Consistency (TCC).TCC providesa model in which atomic transactions are always the basicunit of parallel work, communication, memory coherence, andmemory reference consistency.TCC greatly simplifies parallelsoftware by eliminating the need for synchronization using conventionallocks and semaphores, along with their complexities.TCC hardware must combine all writes from each transaction regionin a program into a single packet and broadcast this packetto the permanent shared memory state atomically as a large block.This simplifies the coherence hardware because it reduces theneed for small, low-latency messages and completely eliminatesthe need for conventional snoopy cache coherence protocols, asmultiple speculatively written versions of a cache line may safelycoexist within the system.Meanwhile, automatic, hardware-controlledrollback of speculative transactions resolves any correctnessviolations that may occur when several processors attemptto read and write the same data simultaneously.The cost of thissimplified scheme is higher interprocessor bandwidth.To explore the costs and benefits of TCC, we study the characterisitcsof an optimal transaction-based memory system, and examinehow different design parameters could affect the performanceof real systems.Across a spectrum of applications, the TCC modelitself did not limit available parallelism.Most applications areeasily divided into transactions requiring only small write buffers,on the order of 4-8 KB.The broadcast requirements of TCCare high, but are well within the capabilities of CMPs and small-scaleSMPs with high-speed interconnects.},
booktitle = {Proceedings of the 31st International Symposium on Computer Architecture (ISCA)},
pages = {102},
location = {M\"{u}nchen, Germany},
series = {ISCA '04}
}

@ARTICLE{2003-code-toppicks,
author={Kozyrakis, Christos and Patterson, David},
journal={IEEE Micro},
title={Scalable vector processors for embedded systems},
year={2003},
volume={23},
number={6},
pages={36-45},
abstract={For embedded applications with data-level parallelism, a vector processor offers high performance at low power consumption and low design complexity. Unlike superscalar and VLIW designs, a vector processor is scalable and can optimally match specific application requirements.To demonstrate that vector architectures meet the requirements of embedded media processing, we evaluate the Vector IRAM, or VIRAM (pronounced "V-IRAM"), architecture developed at UC Berkeley, using benchmarks from the Embedded Microprocessor Benchmark Consortium (EEMBC). Our evaluation covers all three components of the VIRAM architecture: the instruction set, the vectorizing compiler, and the processor microarchitecture. We show that a compiler can vectorize embedded tasks automatically without compromising code density. We also describe a prototype vector processor that outperforms high-end superscalar and VLIW designs by 1.5x to 100x for media tasks, without compromising power consumption. Finally, we demonstrate that clustering and modular design techniques let a vector processor scale to tens of arithmetic data paths before wide instruction-issue capabilities become necessary.},
keywords={Vector processors;Embedded system;Registers;VLIW;Computer architecture;Energy consumption;Embedded computing;Parallel processing;Process design;High performance computing},
doi={10.1109/MM.2003.1261385},
ISSN={1937-4143},
month=Nov}


@INPROCEEDINGS{2003-code-iram,
author={Kozyrakis, Christos and Patterson, David},
booktitle={Proceedings of the 30th International Symposium on Computer Architecture (ISCA)},
title={Overcoming the limitations of conventional vector processors},
year={2003},
volume={},
number={},
pages={399-409},
abstract={Despite their superior performance for multimedia applications, vector processors have three limitations that hinder their widespread acceptance. First, the complexity and size of the centralized vector register file limits the number of functional units. Second, precise exceptions for vector instructions are difficult to implement. Third, vector processors require an expensive on-chip memory system that supports high bandwidth at low access latency. We introduce CODE, a scalable vector microarchitecture that addresses these three shortcomings. It is designed around a clustered vector register file and uses a separate network for operand transfers across functional units. With extensive use of decoupling, it can hide the latency of communication across functional units and provides 26\% performance improvement over a centralized organization. CODE scales efficiently to 8 functional units without requiring wide instruction issue capabilities. A renaming table makes the clustered register file transparent at the instruction set level. Renaming also enables precise exceptions for vector instructions at a performance loss of less than 5\%. Finally, decoupling allows CODE to tolerate large increases in memory latency at sublinear performance degradation without using on-chip caches. Thus, CODE can use economical, off-chip, memory systems.},
keywords={Vector processors;Registers;Delay;System-on-a-chip;Microarchitecture;Performance loss;Computer architecture;Bandwidth;CMOS technology;Computer science},
doi={10.1109/ISCA.2003.1207017},
ISSN={1063-6897},
month=Jun}

@INPROCEEDINGS{2002-vector-micro,
author={Kozyrakis, Christos and Patterson, David},
booktitle={Proceedings of the 35th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
title={Vector vs. superscalar and VLIW architectures for embedded multimedia benchmarks},
year={2002},
volume={},
number={},
pages={283-293},
abstract={Multimedia processing on embedded devices requires an architecture that leads to high performance, low power consumption, reduced design complexity, and small code size. In this paper, we use EEMBC, an industrial benchmark suite, to compare the VIRAM vector architecture to superscalar and VLIW processors for embedded multimedia applications. The comparison covers the VIRAM instruction set, vectorizing compiler and the prototype chip that integrates a vector processor with DRAM main memory. We demonstrate that executable code for VIRAM is up to 10 times smaller than VLIW code and comparable to /spl times/86 CISC code. The simple, cache-less VIRAM chip is 2 times faster than a 4-way superscalar RISC processor that uses a 5 times faster clock frequency and consumes 10 times more power VIRAM is also 10 times faster than cache-based VLIW processors. Even after manual optimization of the VLIW code and insertion of SIMD and DSP instructions, the single-issue VIRAM processor is 60\% faster than 5-way to 8-way VLIW designs.},
keywords={VLIW;Energy consumption;Prototypes;Vector processors;Reduced instruction set computing;Clocks;Frequency;Manuals;Design optimization;Digital signal processing chips},
doi={10.1109/MICRO.2002.1176257},
ISSN={1072-4451},
month=Nov}

@phdthesis{2002-phdthesis-kozyrakis,
  author  = "Christos Kozyrakis",
  title   = "Scalable Vector Media Processors for Embedded Systems ",
  school  = "University of California at Berkeley",
  year    = 2002,
  month   = May,
  abstarct = {Over the past twenty years, processor designers have concentrated on superscalar and VLIW
architectures that exploit the instruction-level parallelism (ILP) available in engineering applications
for workstation systems. Recently, however, the focus in computing has shifted from engineering
to multimedia applications and from workstations to embedded systems. In this new computing
environment, the performance, energy consumption, and development cost of ILP processors renders
them ineffective despite their theoretical generality.
This thesis focuses on the development of efficient architectures for embedded multimedia
systems. We argue that it is possible to design processors that deliver high performance, have
low energy consumption, and are simple to implement. The basis for the argument is the ability
of vector architectures to exploit efficiently the data-level parallelism in multimedia applications.
Furthermore, the increasing density of CMOS chips enables the design of cost-effective, on-chip,
memory systems that can support the high bandwidth necessary for a vector processor.
To test our hypothesis, we present VIRAM, a vector architecture for multimedia processing. We demonstrate that the vector instructions in VIRAM can capture the data-level parallelism
in multimedia tasks and lead to smaller code size than RISC, CISC, and VLIW architectures. We
also describe two scalable microarchitectures for vector media-processors: VIRAM-1 and CODE.
VIRAM-1 integrates a simple, yet highly parallel, vector processor with an embedded DRAM memory system in a prototype chip with 120 million transistors. CODE uses a composite and decoupled
organization for the vector processor in order to simplify the vector register file design, tolerate high
memory latency, and allow for precise exceptions support. Both microarchitectures provide up to 10
times higher performance than alternative approaches without using out-of-order or wide instruction
issue techniques that exacerbate energy consumption and design complexity},
  keywords = {intelligent memory, vector processors, multimedia, energy efficiency},
   number = {UCB-CSD-02-1183}
}


@ARTICLE{2001-codesign-ieee,
author={Kozyrakis, C. and Judd, D. and Gebis, J. and Williams, S. and Patterson, D. and Yelick, K.},
journal={Proceedings of the IEEE},
title={Hardware/compiler codevelopment for an embedded media processor},
year={2001},
volume={89},
number={11},
pages={1694-1709},
abstract={Embedded and portable systems running multimedia applications create a new challenge for hardware architects. A microprocessor for such applications needs to be easy to program like a general-purpose processor and have the performance and power efficiency of a digital signal processor. This paper presents the codevelopment of the instruction set, the hardware, and the compiler for the Vector IRAM media processor. A vector architecture is used to exploit the data parallelism of multimedia programs, which allows the use of highly modular hardware and enables implementations that combine high performance, low power consumption, and reduced design complexity. It also leads to a compiler model that is efficient both in terms of performance and executable code size. The memory system for the vector processor is implemented using embedded DRAM technology, which provides high bandwidth in an integrated, cost-effective manner. The hardware and the compiler for this architecture make complementary contributions to the efficiency of the overall system. This paper explores the interactions and tradeoffs between them, as well as the enhancements to a vector architecture necessary for multimedia processing. We also describe how the architecture, design, and compiler features come together in a prototype system-on-a-chip, able to execute 3.2 billion operations per second per watt.},
keywords={Hardware;Multimedia systems;Microprocessors;Digital signal processors;Energy consumption;Power system modeling;Vector processors;Random access memory;Bandwidth;Prototypes},
doi={10.1109/5.964446},
ISSN={1558-2256},
month=Nov}



@InProceedings{2000-iram-hotchips,
  author = 	 {Christos Kozyrakis, Joseph Gebis, David Martin, Samuel Williams, Iakovos Mavroidis, Steven Pope, Darren Jones, David Patterson, Katherine Yelick},
  title = 	 {Vector IRAM: A Media-oriented Vector Processor with Embedded DRAM },
  booktitle = {Technical Record of the 12th Hot Chips Conference},
  year = 	2000,
  month = 	 Aug}

@Misc{2000-compiler-ims,
  author = 	 {David Judd, Katherine Yelick, Christos Kozyrakis, David Martin, David Patterson},
  title = 	 { Exploiting On-chip Memory Bandwidth in the VIRAM Compiler},
  howpublished = {Proceedings of the 2nd Workshop on Intelligent Memory Systems (IMS)},
  month = 	 Nov,
  abstract = {Many architectural ideas that appear to be useful from a
hardware standpoint fail to achieve wide acceptance due to lack of compiler support. In this paper we explore the design of the VIRAM architecture from the perspective of compiler writers, describing some of
the code generation problems that arise in VIRAM and their solutions
in the VIRAM compiler. VIRAM is a single chip system designed primarily for multimedia. It combines vector processing with mixed logic
and DRAM to achieve high performance with relatively low energy, area,
and design complexity. The paper focuses on two aspects of the VIRAM
compiler and architecture. The first problem is to take advantage of the
on-chip bandwidth for memory-intensive applications, including those
with non-contiguous or unpredictable memory access patterns. The second problem is to support that kinds of narrow data types that arise in
media processing, including processing of 8 and 16-bit data.},
  year = 	 2000}

@Book{2001-proceedings-ims,
  editor = 	 {Frederik Chong, Christos Kozyrakis, Mark Oskin},
  title = 	 {Proceedings of the 2nd Workshop on Intelligent Memory Systems},
  publisher = 	 {Springer Verlag},
  year = 	 2001,
  number = 	 2107,
  series = 	 {Lecture Notes in Computer Science},
  month = 	 Sep,
  isbn = {3-540-42328-1}
}

@INPROCEEDINGS{2000-bottleneck-date,
author={Catthoor, F. and Dutt, N.D. and Kozyrakis, C.E.},
booktitle={Proceedings of the Design, Automation and Test in Europe Conference (DATE)},
title={How to solve the current memory access and data transfer bottlenecks: at the processor architecture or at the compiler level?},
year={2000},
volume={},
number={},
pages={426-433},
abstract={Current processor architectures, both in the programmable and custom case, become more and more dominated by the data access bottlenecks in the cache, system bus and main memory subsystems. In order to provide sufficiently high data throughput in the emerging era of highly parallel processors where many arithmetic resources can work concurrently, novel solutions for the memory access and data transfer will have to be introduced. The crucial question we want to address is where one can expect these novel solutions to reside: will they be mainly innovative processor architecture ideas, or novel approaches in the application compiler/synthesis technology, or a mix.},
keywords={Hardware;Parallel processing;Multithreading;Microprocessors;Runtime;Throughput;Arithmetic;Electronic switching systems;Microarchitecture;Memory management},
doi={10.1109/DATE.2000.840306},
ISSN={},
month=Mar}

@mastersthesis{1999-msthesis-tr,
  author = 	 {Kozyrakis, Christos},
  title = 	 {A Media-Enhanced Vector Architecture for Embedded Memory Systems},
  school =  {University of California at Berkeley},
  year = 	 1999,
  number = 	 {UCB-CSD-99-1059},
  month = 	 Jul,
  keywords = {intelligent memory, energy efficiency, DRAM vector processor, SIMD, multimedia},
  abstract = {Next generation portable devices will require processors with both low energy consumption and high performance for media functions. At the same time, modern CMOS technology
creates the need for highly scalable VLSI architectures. Conventional processor architectures fail to meet these requirements. This paper presents the architecture of Vector IRAM
(VIRAM), a processor that combines vector processing with embedded DRAM technology.
Vector processing achieves high multimedia performance with simple hardware, while embedded DRAM provides high memory bandwidth at low energy consumption. VIRAM provides flexible support for media data types, short vectors, and DSP features. The vector
pipeline is enhanced to hide DRAM latency without using caches. The peak performance is
3.2 GFLOPS (single precision) and maximum memory bandwidth is 25.6 GBytes/s. With a
target power consumption of 2 Watts for the vector pipeline and the memory system, VIRAM
supports 1.6 GFLOPS/Watt. For a set of representative media kernels, VIRAM sustains on
average 88\% of its peak performance, outperforming conventional SIMD media extensions
and DSP processors by factors of 4.5 to 17. Using a clustered implementation approach,
the modular design can be scaled without complicating control logic. We demonstrate that
scaling the architecture leads to near linear application speedup. We also evaluate the effect of scaling the capacity and parallelism of the on-chip memory system to die area and
sustained performance.}}

@ARTICLE{1998-direction-computer,
author={Kozyrakis, C. and Patterson, D.A.},
journal={IEEE Computer},
title={A new direction for computer architecture research},
year={1998},
volume={31},
number={11},
pages={24-32},
abstract={In the past few years, two important trends have evolved that could change the shape of computing: multimedia applications and portable electronics. Together, these trends will lead to a personal mobile-computing environment, a small device carried all the time that incorporates the functions of the pager, cellular phone, laptop computer, PDA, digital camera, and video game. The microprocessor needed for these devices is actually a merged general-purpose processor and digital-signal processor, with the power budget of the latter. Yet for almost two decades, architecture research has focused on desktop or server machines. We are designing processors of the future with a heavy bias toward the past. To design successful processor architectures for the future, we first need to explore future applications and match their requirements in a scalable, cost-effective way. The authors describe Vector IRAM, an initial approach in this direction, and challenge others in the very successful computer architecture community to investigate architectures with a heavy bias for the future.},
keywords={Computer architecture;Portable computers;Application software;Personal digital assistants;Process design;Shape;Computer applications;Multimedia computing;Cellular phones;Digital cameras},
doi={10.1109/2.730733},
ISSN={1558-0814},
month=Nov}

@INPROCEEDINGS{1997-iram-iccd,
author={Patterson, D. and Asanovic, K. and Brown, A. and Fromm, R. and Golbus, J. and Gribstad, B. and Keeton, K. and Kozyrakis, C. and Martin, D. and Perissakis, S. and Thomas, R. and Treuhaft, N. and Yelick, K.},
booktitle={Proceedings of the International Conference on Computer Design (ICCD)},
title={Intelligent RAM (IRAM): the industrial setting, applications, and architectures},
year={1997},
volume={},
number={},
pages={2-7},
abstract={The goal of intelligent RAM (IRAM) is to design a cost-effective computer by designing a processor in a memory fabrication process, instead of in a conventional logic fabrication process, and include memory on-chip. To design a processor in a DRAM process one must learn about the business and culture of the DRAMs, which is quite different from microprocessors. The authors describe some of those differences and their current vision of IRAM applications, architectures, and implementations.},
keywords={Random access memory;Costs;Logic testing;Application software;Fabrication;Microprocessors;Read-write memory;Logic design;Process design;Plastic packaging},
doi={10.1109/ICCD.1997.628842},
ISSN={1063-6404},
month=Oct,
keywords = {Intelligent memory, memory wall, DRAM, processor, processor in memory, energy efficiency}
}

@ARTICLE{1997-iram-computer,
author={Kozyrakis, C. and Perissakis, S. and Patterson, D. and Anderson, T. and Asanovic, K. and Cardwell, N. and Fromm, R. and Golbus, J. and Gribstad, B. and Keeton, K. and Thomas, R. and Treuhaft, N. and Yelick, K.},
journal={IEEE Computer},
title={Scalable processors in the billion-transistor era: IRAM},
year={1997},
volume={30},
number={9},
pages={75-78},
abstract={Members of the University of California, Berkeley, argue that the memory system will be the greatest inhibitor of performance gains in future architectures. Thus, they propose the intelligent RAM or IRAM. This approach greatly increases the on-chip memory capacity by using DRAM technology instead of much less dense SRAM memory cells. The resultant on-chip memory capacity coupled with the high bandwidths available on chip should allow cost-effective vector processors to reach performance levels much higher than those of traditional architectures. Although vector processors require explicit compilation, the authors claim that vector compilation technology is mature (having been used for decades in supercomputers), and furthermore, that future workloads will contain more heavily vectorizable components.},
keywords={Random access memory;Delay;Bandwidth;Read-write memory;Out of order;VLIW;Computer architecture;Fabrication;Bridges;Microprocessor chips},
doi={10.1109/2.612252},
ISSN={1558-0814},
month=Sep}

@INPROCEEDINGS{1997-atlas-arvlsi,
author={Kornaros, G. and Kozyrakis, C. and Vatsolaki, P. and Katevenis, M.},
booktitle={Proceedings of the 17th Conference on Advanced Research in VLSI (ARVLSI)},
title={Pipelined multi-queue management in a VLSI ATM switch chip with credit-based flow-control},
year={1997},
volume={},
number={},
pages={127-144},
abstract={We describe the queue management block of ATLAS I, a single-chip ATM switch (roster) with optional credit-based (backpressure) flow control. ATLAS I is a 4-million-transistor 0.35-micron CMOS chip, currently under development, offering 20 Gbit/s aggregate I/O throughput, sub-microsecond cut-through latency, 256-cell shared buffer containing multiple logical output queues, priorities, multicasting, and load monitoring. The queue management block of ATLAS I is a dual parallel pipeline that manages the multiple queues of ready cells, the per-flow-group credits, and the cells that are waiting for credits. All cells, in all queues, share one, common buffer space. These 3- and Q-stage pipelines handle events at the rate of one cell arrival or departure per clock cycle, and one credit arrival per clock cycle. The queue management block consists of two compiled SRAMs, pipeline bypass logic, and multi-port CAM and SRAM blocks that are laid out in full-custom and support special access operations. The full-custom part of queue management contains approximately 65 thousand transistors in logic and 14 Kbits in various special memories, it occupies 2.3 mm/sup 2/, it consumes 270 mW (worst case), and it operates at 80 MHz (worst case) versus 50 MHz which is the required clock frequency to support the 622 Mb/s switch link rate.},
keywords={Switches;Very large scale integration;Asynchronous transfer mode;Pipelines;Clocks;Logic;Aggregates;Throughput;Delay;Monitoring},
doi={10.1109/ARVLSI.1997.634851},
ISSN={},
month=Sep}


@INPROCEEDINGS{1997-energy-isca,
author={Fromm, R. and Perissakis, S. and Cardwell, N. and Kozyrakis, C. and McGaughy, B. and Patterson, D. and Anderson, T. and Yelick, K.},
booktitle={Proceedings of the 24th International Symposium on Computer Architecture (ISCA)},
title={The Energy Efficiency Of Iram Architectures},
year={1997},
volume={},
number={},
pages={327-337},
abstract={},
keywords={Energy efficiency;Random access memory;Portable computers;Personal digital assistants;Computer architecture;Energy consumption;Permission;Power system modeling;Power system simulation;Central Processing Unit},
doi={10.1145/264107.264214},
ISSN={1063-6897},
month=Jun,
keywords = {energy efficiency, memory wall, processor, DRAM, intelligent memory}
}



@Misc{1997-existing-wmld,
  author = 	 {Ngeci Bowman, Neal Cardwell, Christos Kozyrakis, Cynthia Romer, Helen Wang
},
  title = 	 {Evaluation of Existing Architectures in IRAM Systems},
  howpublished = {Proceeding of the Workshop on Mixing Logic and DRAM},
  month = 	 Jun,
  year = 	 1997,
  keywords = 	 {processor - DRAM integration, DRAM, intelligent memory}
}

@ARTICLE{1997-iram-micro,
author={Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
journal={IEEE Micro},
title={A case for intelligent RAM},
year={1997},
volume={17},
number={2},
pages={34-44},
abstract={Two trends call into question the current practice of fabricating microprocessors and DRAMs as different chips on different fabrication lines. The gap between processor and DRAM speed is growing at 50\% per year; and the size and organization of memory on a single DRAM chip is becoming awkward to use, yet size is growing at 60\% per year. Intelligent RAM, or IRAM, merges processing and memory into a single chip to lower memory latency, increase memory bandwidth, and improve energy efficiency. It also allows more flexible selection of memory size and organization, and promises savings in board area. This article reviews the state of microprocessors and DRAMs today, explores some of the opportunities and challenges for IRAMs, and finally estimates performance and energy efficiency of three IRAM designs.},
keywords={Computer aided software engineering;Random access memory;Delay;Clocks;Bandwidth;Databases;Read-write memory;Sparse matrices;Pins;Out of order},
doi={10.1109/40.592312},
ISSN={1937-4143},
month=Mar}


@INPROCEEDINGS{1997-iram-isscc,
author={Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
booktitle={The Digest of Technical Papers of the IEEE International Solids-State Circuits Conference (ISSCC)},
title={Intelligent RAM (IRAM): chips that remember and compute},
year={1997},
volume={},
number={},
pages={224-225},
abstract={It is time to reconsider unifying logic and memory. Since most of the transistors on this merged chip will be devoted to memory, it is called 'intelligent RAM'. IRAM is attractive because the gigabit DRAM chip has enough transistors for both a powerful processor and a memory big enough to contain whole programs and data sets. It contains 1024 memory blocks each 1kb wide. It needs more metal layers to accelerate the long lines of 600mm/sup 2/ chips. It may require faster transistors for the high-speed interface of synchronous DRAM. Potential advantages of IRAM include lower memory latency, higher memory bandwidth, lower system power, adjustable memory width and size, and less board space. Challenges for IRAM include high chip yield given processors have not been repairable via redundancy, high memory retention rates given processors usually need higher power than DRAMs, and a fast processor given logic is slower in a DRAM process.},
keywords={Random access memory;Microprocessors;Delay;Bandwidth;Switches;Logic;Vector processors;Read-write memory;Computer science;Electronics industry},
doi={10.1109/ISSCC.1997.585348},
ISSN={0193-6530},
month=Feb}


@TechReport{1996-qmanage-tr,
  author = 	 {Christos Kozyrakis},
  title = 	 {The Architecture, Operation, and Design of the Queue Management Block in the ATLAS I ATM Switch},
  institution =  {Institute of Computer Science (ICS), Foundation for Research and Technology (FORTH)},
  year = 	 1996,
  number = 	 {TR-172},
  address = 	 {Heraklion, Greece},
  month = 	 Jul,
  abstract = {Among the various switch buffer architectures, output queueing implemented in a completely shared buffer is the one that achieves the highest possible utilization of both output
bandwidth and buffer space. The high link throughput, small cell size and additional features of ATM switching, such as multiple classes of service, multicasting and flow control,
enforce further extensions to the above scheme and demand pure hardware implementations. In this work we present the hardware block maintaining output queues per priority
class in the ATLAS I single chip ATM switch. It also provides support for multicasting
and multi-lane credit-based flow control. Techniques such as pipelined and superscalar
processing, usually employed in processors’ design, are used in order to accommodate for
the amount and high speed of operation required. This also modifies the approach to the
timing of operations, the control design and the calculation of the hardware complexity.
The block was extensively simulated to ensure the correctness of its operation. Although
the hardware implementation is currently in progress, the circuits already laid out are presented, while the VLSI design of the remaining blocks is analyzed. In addition, the Priority
Enforcer circuit and its full-custom layout is thoroughly described.},
  keywords = {VLSI switches, ATM switches, ATLAS I switch, shared buffer, credit-based
flow-control, multiple output queues, queue management, pipelining, priority enforcer}
}

