@inproceedings{2019-tangram-asplos,
 abstract = {The use of increasingly larger and more complex neural networks (NNs) makes it critical to scale the capabilities and efficiency of NN accelerators. Tiled architectures provide an intuitive scaling solution that supports both coarse-grained parallelism in NNs: intra-layer parallelism, where all tiles process a single layer, and inter-layer pipelining, where multiple layers execute across tiles in a pipelined manner. This work proposes dataflow optimizations to address the shortcomings of existing parallel dataflow techniques for tiled NN accelerators. For intra-layer parallelism, we develop buffer sharing dataflow that turns the distributed buffers into an idealized shared buffer, eliminating excessive data duplication and the memory access overheads. For inter-layer pipelining, we develop alternate layer loop ordering that forwards the intermediate data in a more fine-grained and timely manner, reducing the buffer requirements and pipeline delays. We also make inter-layer pipelining applicable to NNs with complex DAG structures. These optimizations improve the performance of tiled NN accelerators by 2x and reduce their energy consumption by 45% across a wide range of NNs. The effectiveness of our optimizations also increases with the NN size and complexity.},
 address = {New York, NY, USA},
 author = {Gao, Mingyu and Yang, Xuan and Pu, Jing and Horowitz, Mark and Kozyrakis, Christos},
 booktitle = {Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
 doi = {10.1145/3297858.3304014},
 isbn = {9781450362405},
 keywords = {dataflow, parallelism, neural networks},
 location = {Providence, RI, USA},
 month = {April},
 numpages = {14},
 pages = {807â€“820},
 publisher = {Association for Computing Machinery},
 series = {ASPLOS '19},
 title = {TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators},
 year = {2019}
}

